<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>Unified Latents: 扩散模型的框架下，令生成与编码相互配合</title>
    <link href="/2026/02/26/Unified-Latents-%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B%E4%B8%8B%E7%9A%84%E7%94%9F%E6%88%90%E4%B8%8E%E7%BC%96%E7%A0%81%E9%85%8D%E5%90%88/"/>
    <url>/2026/02/26/Unified-Latents-%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B%E4%B8%8B%E7%9A%84%E7%94%9F%E6%88%90%E4%B8%8E%E7%BC%96%E7%A0%81%E9%85%8D%E5%90%88/</url>
    
    <content type="html"><![CDATA[<p>本文介绍 google deepmind 团队提出的 Unified Latents架构，在扩散模型的架构下统一了生成和表示压缩，并且给出了优雅的信息界。</p><h2 id="生成与编码的权衡">生成与编码的权衡</h2><p>在生成模型设定中（详细见<ahref="https://notdesigned.github.io/2025/11/03/Reading-notes-of-The-Principle-of-Diffusion-Models/">此文</a>） ，首先我们有一个在高维空间中的分布</p><p><span class="math display">\[x\sim \mu,\ p(x):=\mathrm{Law}(\mu) : \mathbb R^n \to [0,\infty)\]</span></p><p>历史上， 变分自编码器（Variational Autoencoder, VAE）引入了一个潜变量<span class="math inline">\(z\)</span>，并且得出了一个关于 <spanclass="math inline">\(p(x)\)</span> 的下界。然后考虑真实的条件概率 <spanclass="math inline">\(p(z|x)\)</span> 与其变分近似 <spanclass="math inline">\(q(z|x)\)</span>: <span class="math display">\[\begin{aligned}\log p(x) &amp;= \mathbb E_{q(z|x)}\left[\log\frac{p(x,z)}{p(z|x)}\right] \\&amp;\geq \mathbb E_{q(z|x)}\left[\log \frac{p(x,z)}{q(z|x)}\right] \\&amp;= \underbrace{\mathbb E_{q(z|x)}\left[\logp(x|z)\right]}_{\text{Reconstruction Term}} - \underbrace{\mathrm{KL}(q(z|x) || p(z))}_{\text{Latent Encode Cost}}\\\end{aligned}\]</span></p><p>即：先从原始数据中压缩出潜变量 <spanclass="math inline">\(z\)</span>，然后从潜变量 <spanclass="math inline">\(z\)</span> 中还原出 <spanclass="math inline">\(x\)</span>。并且让编码器 <spanclass="math inline">\(q(z|x)\)</span> 靠近标准正态分布 <spanclass="math inline">\(p(z)=\mathcal{N}(0,I)\)</span>以得到优良的潜空间性质。</p><p>在这里，编码器 <span class="math inline">\(q(z|x)\)</span>从原始数据中提取出潜变量 <spanclass="math inline">\(z\)</span>，而生成器 <spanclass="math inline">\(p(x|z)\)</span> 从潜变量 <spanclass="math inline">\(z\)</span> 中还原出原始数据 <spanclass="math inline">\(x\)</span>。这个由信息瓶颈（InformationBottleneck）引入的潜变量 <span class="math inline">\(z\)</span>连接了生成和编码两个过程。</p><h3 id="信息瓶颈">信息瓶颈</h3><p>信息瓶颈描述了在编码过程中，如何在保留有用信息的同时压缩数据。具体来说，信息瓶颈试图找到一个潜变量<spanclass="math inline">\(z\)</span>，使得它能够最大程度地保留关于输入数据<span class="math inline">\(x\)</span> 的有用信息，同时尽可能地压缩<span class="math inline">\(z\)</span> 的表示。</p><p>数学上，信息瓶颈可以通过以下优化问题来描述： <spanclass="math display">\[\min_{p(z|x)} I(X;Z) - \beta I(Z;Y)\]</span> 其中，<span class="math inline">\(I(X;Z) = \mathbbE_{p(x,z)}[\log\frac{p(x,z)}{p(x)p(z)}]\)</span> 是输入数据 <spanclass="math inline">\(X\)</span> 和潜变量 <spanclass="math inline">\(Z\)</span> 之间的互信息，<spanclass="math inline">\(I(Z;Y)\)</span> 是潜变量 <spanclass="math inline">\(Z\)</span> 和目标变量 <spanclass="math inline">\(Y\)</span> 之间的互信息，<spanclass="math inline">\(\beta\)</span> 是一个权衡参数。</p><p>较大的 <span class="math inline">\(\beta\)</span>会使得模型更倾向于最小化 <spanclass="math inline">\(I(Z;Y)\)</span>，从而得到一个更保留输入数据 <spanclass="math inline">\(X\)</span> 的潜变量 <spanclass="math inline">\(Z\)</span>，而较小的 <spanclass="math inline">\(\beta\)</span> 则会使得模型更倾向于最小化 <spanclass="math inline">\(I(X;Z)\)</span>，从而得到一个更压缩的潜变量 <spanclass="math inline">\(Z\)</span>。</p><p>变分自编码器通过调控 KL 散度项 <spanclass="math inline">\(\mathrm{KL}(q(z|x) || p(z))\)</span>来实现信息瓶颈的效果。较大的 KL 散度会使得潜变量 <spanclass="math inline">\(z\)</span> 更接近于标准正态分布，从而令 <spanclass="math inline">\(I(X;Z)\)</span> 较小，得到一个更压缩的潜变量 <spanclass="math inline">\(z\)</span>；反之亦然。</p><p>问题在于，变分自编码器中，我们无法显式地控制 <spanclass="math inline">\(I(Z;X)\)</span>，或者说潜变量 <spanclass="math inline">\(z\)</span> 中保留了多少关于输入数据 <spanclass="math inline">\(x\)</span> 的信息，只能通过调节损失函数里面的 KL散度权重来间接控制。这就导致了复杂的调试过程，而且因模型而异。</p><h2 id="unified-latents-架构">Unified Latents 架构</h2><p>首先，Unified Latents 架构使用了一个确定性的编码器 <spanclass="math inline">\(E: \mathbb R^n \to \mathbbR^d\)</span>，将输入数据 <span class="math inline">\(x\)</span>映射到一个潜空间中的表示 <span class="math inline">\(z_{\text{clean}}=E(x)\)</span>。</p><p>然后，对这个确定性的编码加噪得到带噪编码 <spanclass="math inline">\(z_t = \alpha_z(t) z_{\text{clean}} + \sigma_z^2\epsilon\)</span>，其中 <span class="math inline">\(\epsilon \sim\mathcal{N}(0, I)\)</span> 是标准高斯噪声。</p><p>接下来，使用两个独立的扩散模型，一个先验扩散模型（diffusionprior）负责从加噪编码 <span class="math inline">\(z_t\)</span>生成原始编码。另一个从一个设定好的固定程度加噪（<spanclass="math inline">\(\lambda(0)=5\)</span>，见下文）编码 <spanclass="math inline">\(z_t\)</span> 作为条件生成数据 <spanclass="math inline">\(x\)</span>。</p><figure><img src="Arch.png" alt="Unified Latents 架构" /><figcaption aria-hidden="true">Unified Latents 架构</figcaption></figure><p><span class="math display">\[z_t = \alpha_z(t) \cdot z_{\text {clean}} + \sigma_z(t) \cdot \epsilon,\quad \epsilon \sim \mathcal{N}(0, I)\]</span> 信噪比为 <span class="math display">\[\mathrm{SNR}(t) = \frac{\alpha_z(t)^2}{\sigma_z(t)^2}\]</span> 同时定义 <span class="math inline">\(\lambda(t) = \log\mathrm{SNR}(t)\)</span> ，<span class="math inline">\(\lambda\)</span>大意味着信号强、噪声弱（数据清晰）；<spanclass="math inline">\(\lambda\)</span> 小意味着噪声主导。</p><p>好处是可以方便地表达信号和噪声的比例： <span class="math display">\[\alpha^2 = \text{sigmoid}(\lambda), \sigma^2 = \text{sigmoid}(-\lambda).\]</span></p><blockquote><p>注：<span class="math inline">\(\text{sigmoid}(t) = \frac{1}{1 +e^{-t}}\)</span></p></blockquote><figure><img src="pic.jpg" alt="手画的演示" /><figcaption aria-hidden="true">手画的演示</figcaption></figure><h3 id="损失函数推导-与-elbo-重加权">损失函数推导 与 ELBO 重加权</h3><p>对于扩散模型，文中使用了以下的损失： <span class="math display">\[\mathbb E_{t\in\text{Uniform}(0,1)}\left[-\frac{d\lambda(t)}{dt}\frac{\exp\lambda(t)}{2}\omega(\lambda(t))\|x-\hatx(x_t,\theta)\|^2\right] + \mathrm{KL}\left(p(x_1|x) | p(x_1)\right)\]</span></p><p>逻辑是，首先最小化负对数似然展开成 ELBO。 <spanclass="math display">\[-\log p_\theta(x) \leq \mathbb E_{q(z|x)}[-\log p(x|z)] +\mathrm{KL}(q(z|x) || p(z))\]</span></p><p>在扩散模型的设定里，<span class="math inline">\(z\)</span>是标准高斯噪声 <spanclass="math inline">\(\mathcal{N}(0,I)\)</span>。</p><p>但是不同于 VAE，扩散模型中的 <span class="math inline">\(z\)</span>代表的是一整个时间步的噪声过程，也即是路径空间 <spanclass="math inline">\(\Omega \times [0,1]\)</span>上的作为随机变量。然后这个路径由前向和后向过程 Itô SDE 定义。</p><p>根据 Girsanov 定理，路径空间上的 KL 散度可以被重写为一个时间积分：<span class="math display">\[\text{KL}[\mathbb{Q}^{\leftarrow} \| \mathbb{P}_\theta] = \frac{1}{2}\int_0^1 g(t)^2 \, \mathbb{E}_{q(x_t)} \left[\left\| \nabla_{x_t} \logq_t(x_t) - s_\theta(x_t, t) \right\|^2 \right] dt\]</span> 其中 <span class="math inline">\(s_\theta(x_t, t)\)</span>是模型的得分函数，<span class="math inline">\(\nabla_{x_t} \logq_t(x_t)\)</span> 是真实的得分函数。</p><p>而 <span class="math inline">\(x\)</span> 和 <spanclass="math inline">\(s\)</span> 的关系由扩散模型的定义给出： <spanclass="math display">\[s_\theta(x_t, t) = \frac{1}{\sigma(t)} (x_t - \alpha(t) \hat{x}(x_t, t))\]</span></p><p>然后由 SDE 的对应关系给出，<span class="math inline">\(g(t)^2 =-\dot{\lambda} \sigma_t^2\)</span>。</p><p>带入化简一下就可以得到文中使用的损失函数。</p><p>而这里只降噪到 <span class="math inline">\(\lambda(0)=5\)</span>之类的 <span class="math inline">\(z_0\)</span>，所以实际上 timeintegral 的下界不是 0。然后还需要加上 <spanclass="math inline">\(z_1\)</span> 端点的 KL 散度： <spanclass="math display">\[\mathrm{KL}\left(p(z_1|x) | \mathcal{N}(0,I)\right)\]</span></p><p>而 reweight 就是在路径空间上对不同时间步的损失进行加权，根据 Jensen’sinequality 可以证明，重加权后的损失函数仍然是 ELBO 的上界。</p><blockquote><p>TODO: 有时间写一个详细的推导过程。</p></blockquote><h2 id="双阶段训练">双阶段训练</h2><p>TODO.</p>]]></content>
    
    
    <categories>
      
      <category>Machine Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Diffusion Models</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>nanochat 学习笔记</title>
    <link href="/2026/01/10/nanochat-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    <url>/2026/01/10/nanochat-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/</url>
    
    <content type="html"><![CDATA[<p>本文档记录解读学习 Andrej Karpathy 开源项目 <ahref="https://github.com/karpathy/nanochat">nanochat</a> 的笔记。</p><p>学习的 git 代码版本是: <code>f5a0ea4</code>。</p><h2 id="项目简介">项目简介</h2><p>nanochat 是一个用（几乎，有一点 Rust 用来训练分词器）纯 Python实现的全栈 GPT库，包括 pretrain、midtrain、finetune、inference等。它的目标是最简化帮助理解大型语言模型（LLM）的工作原理。</p><p>计划从以下几个方面来学习这个项目：</p><ol type="1"><li><strong>代码结构</strong>：了解项目的整体架构和各个模块的功能。</li><li><strong>模型实现</strong>：理解 Transformer 模型的实现细节。</li><li><strong>训练流程</strong>：学习模型的训练过程，包括数据预处理、训练循环等。</li></ol><p>Wiki 参考： - <ahref="https://deepwiki.com/karpathy/nanochat">nanochat Wiki</a></p><h2 id="项目安装">项目安装</h2><p>我使用的机器与环境如下： - windows上的wsl2+Ubuntu 22.04 - CUDA 13.1 -RAM: 48GB - GPU: RTX 5080 16GB</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash">git <span class="hljs-built_in">clone</span> https://github.com/karpathy/nanochat.git<br><span class="hljs-built_in">cd</span> nanochat<br>uv venv<br>uv <span class="hljs-built_in">sync</span> --extra gpu<br><span class="hljs-built_in">source</span> .venv/bin/activate<br></code></pre></td></tr></table></figure><p>试试跑起来 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash">python -m nanochat.report reset<br>python -m nanochat.dataset -n 240<br>python -m scripts.tok_train --max_chars=2000000000 --vocab_size=65536<br>python -m scripts.tok_eval<br>python -m scripts.base_train --depth=4 --max_seq_len=512 --device_batch_size=1 --eval_tokens=512 --core_metric_every=-1 --total_batch_size=512 --num_iterations=20<br></code></pre></td></tr></table></figure></p><h2 id="代码结构">代码结构</h2><ul><li><code>nanochat/</code>：核心代码目录，包含模型实现、数据处理等模块。<ul><li><code>adamw.py</code>：AdamW 优化器实现。</li><li><code>checkpoint_manager.py</code>：检查点管理器，用于保存和加载模型,优化器状态等。</li><li><code>common.py</code>：常用工具函数。有打印，logging，检测torchdevice等。</li><li><code>core_eval.py</code>：评价模型性能的核心函数。</li><li><code>dataloader.py</code>：数据加载器实现。</li><li><code>dataset.py</code>：数据集处理模块。</li><li><code>engine.py</code>：用于推理对话的引擎，发送和接收token</li><li><code>execution.py</code>：一个沙盒执行环境</li><li><code>gpt.py</code>：GPT 模型的核心实现。</li><li><code>loss_eval.py</code>：损失评估模块。</li><li><code>muon.py</code>：Muon 优化器实现。</li><li><code>report.py</code>：报告生成模块。</li><li><code>tokenizer.py</code>：分词器实现。</li></ul></li></ul><h2 id="代码实现">代码实现</h2><p>从前到后，按照模型本身的逻辑顺序来学习代码实现，外加训练，评估等辅助功能。</p><h3 id="数据集与分词器">数据集与分词器</h3><p>数据集相关代码在 <code>dataset.py</code> 和<code>dataloader.py</code> 中，分词器在 <code>tokenizer.py</code>中。</p><h4 id="分词器实现">分词器实现</h4><p>首先，<code>tokenizer.py</code>实现了两个分词器类：<code>HuggingFaceTokenizer</code> 和<code>RustBPETokenizer</code>。按照描述，两者应该都是基于 BPE（Byte PairEncoding）算法的分词器。只是 <code>HuggingFaceTokenizer</code> 使用了Hugging Face 的 <code>tokenizers</code> 库，可能为了完整的兼容。而<code>RustBPETokenizer</code> 是基于 <code>tiktoken</code> 库并用RustBPE 训练。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python">SPECIAL_TOKENS = [<br>    <span class="hljs-comment"># every document begins with the Beginning of Sequence (BOS) token that delimits documents</span><br>    <span class="hljs-string">&quot;&lt;|bos|&gt;&quot;</span>,<br>    <span class="hljs-comment"># tokens below are only used during finetuning to render Conversations into token ids</span><br>    <span class="hljs-string">&quot;&lt;|user_start|&gt;&quot;</span>, <span class="hljs-comment"># user messages</span><br>    <span class="hljs-string">&quot;&lt;|user_end|&gt;&quot;</span>,<br>    <span class="hljs-string">&quot;&lt;|assistant_start|&gt;&quot;</span>, <span class="hljs-comment"># assistant messages</span><br>    <span class="hljs-string">&quot;&lt;|assistant_end|&gt;&quot;</span>,<br>    <span class="hljs-string">&quot;&lt;|python_start|&gt;&quot;</span>, <span class="hljs-comment"># assistant invokes python REPL tool</span><br>    <span class="hljs-string">&quot;&lt;|python_end|&gt;&quot;</span>,<br>    <span class="hljs-string">&quot;&lt;|output_start|&gt;&quot;</span>, <span class="hljs-comment"># python REPL outputs back to assistant</span><br>    <span class="hljs-string">&quot;&lt;|output_end|&gt;&quot;</span>,<br>]<br><br><span class="hljs-comment"># <span class="hljs-doctag">NOTE:</span> this split pattern deviates from GPT-4 in that we use \p&#123;N&#125;&#123;1,2&#125; instead of \p&#123;N&#125;&#123;1,3&#125;</span><br><span class="hljs-comment"># I did this because I didn&#x27;t want to &quot;waste&quot; too many tokens on numbers for smaller vocab sizes.</span><br><span class="hljs-comment"># I haven&#x27;t validated that this is actually a good idea, TODO.</span><br>SPLIT_PATTERN = <span class="hljs-string">r&quot;&quot;&quot;&#x27;(?i:[sdmt]|ll|ve|re)|[^\r\n\p&#123;L&#125;\p&#123;N&#125;]?+\p&#123;L&#125;+|\p&#123;N&#125;&#123;1,2&#125;| ?[^\s\p&#123;L&#125;\p&#123;N&#125;]++[\r\n]*|\s*[\r\n]|\s+(?!\S)|\s+&quot;&quot;&quot;</span><br></code></pre></td></tr></table></figure><p>这标注了特殊的 token 列表<code>SPECIAL_TOKENS</code>，以及用于预分词的正则表达式<code>SPLIT_PATTERN</code>。</p><p>这里简单分析一下它的正则表达式逻辑： -<code>'(?i:[sdmt]|ll|ve|re)</code>：匹配常见的缩写形式，如 ’s, ’d, ’m,’t, ll, ve, re（不区分大小写）。 -<code>[^\r\n\p{L}\p{N}]?+\p{L}+</code>：匹配以字母开头的单词，前面可以有一个非字母数字字符。- <code>\p{N}{1,2}</code>：匹配1到2位的数字序列，标准的 GPT-4使用的是1到3位数字。这里做了修改以节省 token 空间。 -<code>?[^\s\p{L}\p{N}]++[\r\n]*</code>：匹配非空白、非字母数字的字符，前面可以有一个空格，后面可以跟换行符。- <code>\s*[\r\n]</code>：匹配换行符，前面可以有任意数量的空白字符。 -<code>\s+(?!\S)</code>：匹配空白字符，后面不跟非空白字符（即匹配行尾的空白）。- <code>\s+</code>：匹配任意数量的空白字符。</p><p>先来看 <code>HuggingFaceTokenizer</code> 类：</p><p><ahref="https://huggingface.co/docs/tokenizers/v0.20.3/">文档参考</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> tokenizers <span class="hljs-keyword">import</span> Tokenizer <span class="hljs-keyword">as</span> HFTokenizer<br><span class="hljs-keyword">from</span> tokenizers <span class="hljs-keyword">import</span> pre_tokenizers, decoders, Regex<br><span class="hljs-keyword">from</span> tokenizers.models <span class="hljs-keyword">import</span> BPE<br><span class="hljs-keyword">from</span> tokenizers.trainers <span class="hljs-keyword">import</span> BpeTrainer<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">HuggingFaceTokenizer</span>:<br>    <span class="hljs-string">&quot;&quot;&quot;Light wrapper around HuggingFace Tokenizer for some utilities&quot;&quot;&quot;</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, tokenizer</span>):<br>        <span class="hljs-variable language_">self</span>.tokenizer = tokenizer<br>    <span class="hljs-comment"># ...</span><br><span class="hljs-meta">    @classmethod</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">train_from_iterator</span>(<span class="hljs-params">cls, text_iterator, vocab_size</span>):<br>        <span class="hljs-comment"># 从一个文本迭代器，加上一个词汇表大小，来训练一个 Hugging Face BPE 分词器</span><br>        tokenizer = HFTokenizer(BPE(<br>            byte_fallback=<span class="hljs-literal">True</span>, <span class="hljs-comment"># needed!</span><br>            unk_token=<span class="hljs-literal">None</span>,<br>            fuse_unk=<span class="hljs-literal">False</span>,<br>        ))<br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">当 byte_fallback 设为 True 时，分词器的处理逻辑会发生根本变化：</span><br><span class="hljs-string"></span><br><span class="hljs-string">首选尝试：按照正常的词表（如 BPE 或 Unigram 算法生成的子词）进行分词。</span><br><span class="hljs-string"></span><br><span class="hljs-string">触发降级（Fallback）：如果遇到一个完全不在词表里的字符，它不会直接输出 [UNK]。</span><br><span class="hljs-string"></span><br><span class="hljs-string">字节化处理：分词器会将该未知字符转化为其 UTF-8 编码的字节（Bytes）。</span><br><span class="hljs-string"></span><br><span class="hljs-string">字节 Token 映射：词表中预先保留了 256 个基础 Token，分别对应字节 0 到 255。分词器会将这个未知字符拆解为一系列字节 Token。</span><br><span class="hljs-string"></span><br><span class="hljs-string">unk_token (str, optional) — 指定未知字符的替代 Token，这里 None，表示不使用特殊的未知 Token。</span><br><span class="hljs-string"></span><br><span class="hljs-string">fuse_unk (bool, optional) — 是否将连续的未知 Token 融合为一个单独的 Token.</span><br><span class="hljs-string"></span><br><span class="hljs-string">这里整体的意思是，把 byte-level 作为最后的兜底方案，确保任何输入字符都能被分词器处理，而不是直接归为未知 Token。</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br>        <span class="hljs-comment"># Normalizer: None</span><br>        tokenizer.normalizer = <span class="hljs-literal">None</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">不进行任何正规化处理，保持原始文本不变。（比如不转换大小写，不去除标点，重音符号等）</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br>        <span class="hljs-comment"># Pre-tokenizer: GPT-4 style</span><br>        <span class="hljs-comment"># the regex pattern used by GPT-4 to split text into groups before BPE</span><br>        <span class="hljs-comment"># <span class="hljs-doctag">NOTE:</span> The pattern was changed from \p&#123;N&#125;&#123;1,3&#125; to \p&#123;N&#125;&#123;1,2&#125; because I suspect it is harmful to</span><br>        <span class="hljs-comment"># very small models and smaller vocab sizes, because it is a little bit wasteful in the token space.</span><br>        <span class="hljs-comment"># (but I haven&#x27;t validated this! TODO)</span><br>        gpt4_split_regex = Regex(SPLIT_PATTERN)<br>        <span class="hljs-comment"># huggingface 要求 PATTERN 必须用 Regex 包裹</span><br>        tokenizer.pre_tokenizer = pre_tokenizers.<span class="hljs-type">Sequence</span>([<br>            pre_tokenizers.Split(pattern=gpt4_split_regex, behavior=<span class="hljs-string">&quot;isolated&quot;</span>, invert=<span class="hljs-literal">False</span>),<br>            pre_tokenizers.ByteLevel(add_prefix_space=<span class="hljs-literal">False</span>, use_regex=<span class="hljs-literal">False</span>)<br>        ])<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        behavior=&quot;isolated&quot;：表示匹配到的部分会被单独作为一个 token 处理，而不是与周围的文本合并。</span><br><span class="hljs-string">        invert=False：表示按照正则表达式的匹配结果进行分割，而不是取反。</span><br><span class="hljs-string">        pre_tokenizers.ByteLevel：将文本转换为字节级别的表示，确保每个字符都能被处理。</span><br><span class="hljs-string">        add_prefix_space=False：不在文本前添加额外的空格。</span><br><span class="hljs-string">        use_regex=False：不使用正则表达式进行字节级别的处理。</span><br><span class="hljs-string"></span><br><span class="hljs-string">        这个预分词器的设计目的是先用 GPT-4 风格的正则表达式进行初步分割，然后再将每个部分转换为字节级别的表示，确保任何字符都能被处理。</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        <span class="hljs-comment"># Decoder: ByteLevel (it pairs together with the ByteLevel pre-tokenizer)</span><br>        tokenizer.decoder = decoders.ByteLevel()<br>        <span class="hljs-comment"># Post-processor: None</span><br>        tokenizer.post_processor = <span class="hljs-literal">None</span><br>        <span class="hljs-comment"># Trainer: BPE</span><br>        trainer = BpeTrainer(<br>            vocab_size=vocab_size,<br>            show_progress=<span class="hljs-literal">True</span>,<br>            min_frequency=<span class="hljs-number">0</span>, <span class="hljs-comment"># no minimum frequency</span><br>            initial_alphabet=pre_tokenizers.ByteLevel.alphabet(),<br>            special_tokens=SPECIAL_TOKENS,<br>        )<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        vocab_size (int) — 词汇表的大小，决定了分词器可以识别的不同子词数量。</span><br><span class="hljs-string">        show_progress (bool) — 是否在训练过程中显示进度条。</span><br><span class="hljs-string">        min_frequency (int) — 一个子词被纳入词汇表的最低出现频率，这里设为 0，表示不设限制。</span><br><span class="hljs-string">        initial_alphabet (List[str]) — 初始字母表，这里使用 ByteLevel 预分词器的字母表，确保所有字节都被包含在内。</span><br><span class="hljs-string">        special_tokens (List[str]) — 特殊 Token 列表，如 &lt;|bos|&gt; 等，这些 Token 会被单独处理。</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        <span class="hljs-comment"># 调用Huggingface的训练方法</span><br>        tokenizer.train_from_iterator(text_iterator, trainer)<br>        <span class="hljs-keyword">return</span> cls(tokenizer)<br></code></pre></td></tr></table></figure><p><code>RustBPETokenizer</code> 类的实现：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> pickle<br><span class="hljs-keyword">import</span> rustbpe<br><span class="hljs-keyword">import</span> tiktoken<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">RustBPETokenizer</span>:<br>    <span class="hljs-string">&quot;&quot;&quot;Light wrapper around tiktoken (for efficient inference) but train with rustbpe&quot;&quot;&quot;</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, enc, bos_token</span>):<br>        <span class="hljs-variable language_">self</span>.enc = enc<br>        <span class="hljs-variable language_">self</span>.bos_token_id = <span class="hljs-variable language_">self</span>.encode_special(bos_token)<br><br><span class="hljs-meta">    @classmethod</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">train_from_iterator</span>(<span class="hljs-params">cls, text_iterator, vocab_size</span>):<br>        <span class="hljs-comment"># 1) train using rustbpe</span><br>        tokenizer = rustbpe.Tokenizer()<br>        <span class="hljs-comment"># the special tokens are inserted later in __init__, we don&#x27;t train them here</span><br>        vocab_size_no_special = vocab_size - <span class="hljs-built_in">len</span>(SPECIAL_TOKENS)<br>        <span class="hljs-keyword">assert</span> vocab_size_no_special &gt;= <span class="hljs-number">256</span>, <span class="hljs-string">f&quot;vocab_size_no_special must be at least 256, got <span class="hljs-subst">&#123;vocab_size_no_special&#125;</span>&quot;</span><br>        tokenizer.train_from_iterator(text_iterator, vocab_size_no_special, pattern=SPLIT_PATTERN)<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        先训练一个 RustBPE 分词器，减去特殊 token 的数量，确保基础词汇表大小足够大（至少256个，满足所有byte）。</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        <span class="hljs-comment"># 2) construct the associated tiktoken encoding for inference</span><br>        pattern = tokenizer.get_pattern()<br>        mergeable_ranks_list = tokenizer.get_mergeable_ranks()<br>        mergeable_ranks = &#123;<span class="hljs-built_in">bytes</span>(k): v <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> mergeable_ranks_list&#125;<br>        <span class="hljs-comment"># 此处将 rustbpe 输出的数据转换为 &#123;字节序列: 优先级数值&#125; 的字典格式。</span><br><br>        tokens_offset = <span class="hljs-built_in">len</span>(mergeable_ranks)<br>        special_tokens = &#123;name: tokens_offset + i <span class="hljs-keyword">for</span> i, name <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(SPECIAL_TOKENS)&#125;<br>        <span class="hljs-comment"># 为特殊 token 分配唯一的 token id，确保它们不会与基础词汇表冲突。</span><br>        enc = tiktoken.Encoding(<br>            name=<span class="hljs-string">&quot;rustbpe&quot;</span>,<br>            pat_str=pattern,<br>            mergeable_ranks=mergeable_ranks, <span class="hljs-comment"># dict[bytes, int] (token bytes -&gt; merge priority rank)</span><br>            special_tokens=special_tokens, <span class="hljs-comment"># dict[str, int] (special token name -&gt; token id)</span><br>        )<br>        <span class="hljs-keyword">return</span> cls(enc, <span class="hljs-string">&quot;&lt;|bos|&gt;&quot;</span>) <span class="hljs-comment"># 实例化并返回 RustBPETokenizer 对象</span><br><span class="hljs-comment"># ...</span><br></code></pre></td></tr></table></figure><h4 id="数据集">数据集</h4><p>数据是 fineweb_100b 数据集的一个子集，<ahref="https://huggingface.co/datasets/karpathy/fineweb-edu-100b-shuffle">link</a>。下载到<code>$BASE_DIR/base_data</code> 目录下。 数据被切片成多个<code>.parquet</code> 文件。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs py"><span class="hljs-keyword">def</span> <span class="hljs-title function_">parquets_iter_batched</span>(<span class="hljs-params">split, start=<span class="hljs-number">0</span>, step=<span class="hljs-number">1</span></span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    Iterate through the dataset, in batches of underlying row_groups for efficiency.</span><br><span class="hljs-string">    - split can be &quot;train&quot; or &quot;val&quot;. the last parquet file will be val.</span><br><span class="hljs-string">    - start/step are useful for skipping rows in DDP. e.g. start=rank, step=world_size</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-keyword">assert</span> split <span class="hljs-keyword">in</span> [<span class="hljs-string">&quot;train&quot;</span>, <span class="hljs-string">&quot;val&quot;</span>], <span class="hljs-string">&quot;split must be &#x27;train&#x27; or &#x27;val&#x27;&quot;</span><br>    parquet_paths = list_parquet_files()<br>    parquet_paths = parquet_paths[:-<span class="hljs-number">1</span>] <span class="hljs-keyword">if</span> split == <span class="hljs-string">&quot;train&quot;</span> <span class="hljs-keyword">else</span> parquet_paths[-<span class="hljs-number">1</span>:]<br>    <span class="hljs-keyword">for</span> filepath <span class="hljs-keyword">in</span> parquet_paths:<br>        pf = pq.ParquetFile(filepath)<br>        <span class="hljs-keyword">for</span> rg_idx <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(start, pf.num_row_groups, step):<br>            rg = pf.read_row_group(rg_idx)<br>            texts = rg.column(<span class="hljs-string">&#x27;text&#x27;</span>).to_pylist()<br>            <span class="hljs-keyword">yield</span> texts<br></code></pre></td></tr></table></figure><p>parquet 文件是一个列式存储格式，这里按行组（rowgroup）来读取，避免一次性加载过多数据。</p><p>而 DataLoader 负责读取数据并生成训练所需的一个批次的 token 序列：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br></pre></td><td class="code"><pre><code class="hljs py"><span class="hljs-keyword">def</span> <span class="hljs-title function_">tokenizing_distributed_data_loader_with_state</span>(<span class="hljs-params">B, T, split, tokenizer_threads=<span class="hljs-number">4</span>, tokenizer_batch_size=<span class="hljs-number">128</span>, device=<span class="hljs-string">&quot;cuda&quot;</span>, resume_state_dict=<span class="hljs-literal">None</span></span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    Stream pretraining text from parquet files, tokenize, yield training batches.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    This implementation became a bit more complex because we wish to support approximate resume training.</span><br><span class="hljs-string">    Instead of turning this into a Class, we opt to return the state_dict with every batch,</span><br><span class="hljs-string">    and then the caller can pass in a state_dict to resume training from a desired point.</span><br><span class="hljs-string">    Note that this resumption is atm only *approximate* for simplicity.</span><br><span class="hljs-string">    We won&#x27;t repeat the same documents but we might skip a few.</span><br><span class="hljs-string">    The state_dict that is returned can be later passed into this function via `resume_state_dict` to approximately resume.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Perfect state resumption is possible but would be a lot more bloated, probably not worth it atm.</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-keyword">assert</span> split <span class="hljs-keyword">in</span> [<span class="hljs-string">&quot;train&quot;</span>, <span class="hljs-string">&quot;val&quot;</span>], <span class="hljs-string">&quot;split must be &#x27;train&#x27; or &#x27;val&#x27;&quot;</span><br><br>    <span class="hljs-comment"># infinite iterator over document batches (list of text strings)</span><br>    ddp, ddp_rank, ddp_local_rank, ddp_world_size = get_dist_info()<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">document_batches</span>():<br>        parquet_paths = list_parquet_files()<br>        <span class="hljs-keyword">assert</span> <span class="hljs-built_in">len</span>(parquet_paths) != <span class="hljs-number">0</span>, <span class="hljs-string">&quot;No dataset parquet files found, did you run dataset.py?&quot;</span><br>        parquet_paths = parquet_paths[:-<span class="hljs-number">1</span>] <span class="hljs-keyword">if</span> split == <span class="hljs-string">&quot;train&quot;</span> <span class="hljs-keyword">else</span> parquet_paths[-<span class="hljs-number">1</span>:]<br>        resume_pq_idx = resume_state_dict[<span class="hljs-string">&quot;pq_idx&quot;</span>] <span class="hljs-keyword">if</span> resume_state_dict <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">else</span> <span class="hljs-number">0</span><br>        resume_rg_idx = resume_state_dict[<span class="hljs-string">&quot;rg_idx&quot;</span>] <span class="hljs-keyword">if</span> resume_state_dict <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">else</span> <span class="hljs-literal">None</span><br>        first_pass = <span class="hljs-literal">True</span><br>        pq_idx = resume_pq_idx <span class="hljs-comment"># we kick off parquet files at the resume index (or by default just 0)</span><br>        <span class="hljs-keyword">while</span> <span class="hljs-literal">True</span>: <span class="hljs-comment"># iterate infinitely (multi-epoch)</span><br>            pq_idx = resume_pq_idx <span class="hljs-keyword">if</span> first_pass <span class="hljs-keyword">else</span> <span class="hljs-number">0</span><br>            <span class="hljs-keyword">while</span> pq_idx &lt; <span class="hljs-built_in">len</span>(parquet_paths): <span class="hljs-comment"># iterate over all parquet files</span><br>                filepath = parquet_paths[pq_idx]<br>                pf = pq.ParquetFile(filepath)<br>                <span class="hljs-comment"># Start from resume point if resuming on same file, otherwise from DDP rank</span><br>                <span class="hljs-comment"># I know this state resumption is a little bit tricky and a little bit hacky... sigh.</span><br>                <span class="hljs-keyword">if</span> first_pass <span class="hljs-keyword">and</span> (resume_rg_idx <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>) <span class="hljs-keyword">and</span> (pq_idx == resume_pq_idx):<br>                    base_idx = resume_rg_idx // ddp_world_size <span class="hljs-comment"># in units of ddp_world_size</span><br>                    base_idx += <span class="hljs-number">1</span> <span class="hljs-comment"># advance by 1 so that we definitely don&#x27;t repeat data after resuming</span><br>                    rg_idx = base_idx * ddp_world_size + ddp_rank<br>                    <span class="hljs-keyword">if</span> rg_idx &gt;= pf.num_row_groups:<br>                        pq_idx += <span class="hljs-number">1</span><br>                        <span class="hljs-keyword">continue</span><br>                    resume_rg_idx = <span class="hljs-literal">None</span> <span class="hljs-comment"># set to None as we only want to do this a single time</span><br>                <span class="hljs-keyword">else</span>:<br>                    rg_idx = ddp_rank<br>                <span class="hljs-keyword">while</span> rg_idx &lt; pf.num_row_groups:<br>                    rg = pf.read_row_group(rg_idx)<br>                    batch = rg.column(<span class="hljs-string">&#x27;text&#x27;</span>).to_pylist() <span class="hljs-comment"># each batch is a parquet group, e.g. 1024 rows</span><br>                    <span class="hljs-comment"># the tokenizer encode might want to go in even smaller batches, e.g. 128 rows</span><br>                    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, <span class="hljs-built_in">len</span>(batch), tokenizer_batch_size):<br>                        <span class="hljs-keyword">yield</span> batch[i:i+tokenizer_batch_size], (pq_idx, rg_idx)<br>                    rg_idx += ddp_world_size <span class="hljs-comment"># advance to the next row group (in DDP)</span><br>                pq_idx += <span class="hljs-number">1</span> <span class="hljs-comment"># advance to the next parquet file</span><br>            first_pass = <span class="hljs-literal">False</span><br>    batches = document_batches()<br><br>    <span class="hljs-comment"># Now emit batches of tokens.</span><br>    needed_tokens = B * T + <span class="hljs-number">1</span> <span class="hljs-comment"># +1 is because we also need the target at the last token</span><br>    <span class="hljs-comment"># get the tokenizer and the bos token</span><br>    tokenizer = get_tokenizer()<br>    bos_token = tokenizer.get_bos_token_id()<br>    <span class="hljs-comment"># scratch buffer holds the tokens for one iteration</span><br>    token_buffer = deque() <span class="hljs-comment"># we stream tokens on the right and pop from the left</span><br>    <span class="hljs-keyword">while</span> <span class="hljs-literal">True</span>:<br>        <span class="hljs-comment"># Accumulate enough tokens for one iteration before yielding.</span><br>        <span class="hljs-keyword">while</span> <span class="hljs-built_in">len</span>(token_buffer) &lt; needed_tokens:<br>            doc_batch, (pq_idx, rg_idx) = <span class="hljs-built_in">next</span>(batches)<br>            token_lists = tokenizer.encode(doc_batch, prepend=bos_token, num_threads=tokenizer_threads)<br>            <span class="hljs-keyword">for</span> tokens <span class="hljs-keyword">in</span> token_lists:<br>                token_buffer.extend(tokens)<br>        <span class="hljs-comment"># Move tokens from the deque into the scratch buffer</span><br>        tokens = [token_buffer.popleft() <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(needed_tokens)]<br>        <span class="hljs-comment"># CUDA supports memory pinning for asynchronous transfers between CPU and GPU</span><br>        use_cuda_optimizations = device == <span class="hljs-string">&quot;cuda&quot;</span><br>        scratch = torch.tensor(tokens, dtype=torch.long, pin_memory=use_cuda_optimizations) <span class="hljs-comment"># in PyTorch, long=int64</span><br>        <span class="hljs-comment"># Create the inputs/targets as 1D tensors</span><br>        inputs_cpu = scratch[:-<span class="hljs-number">1</span>]<br>        targets_cpu = scratch[<span class="hljs-number">1</span>:]<br>        <span class="hljs-comment"># Reshape to 2D and move to GPU async</span><br>        inputs = inputs_cpu.view(B, T).to(device=device, non_blocking=use_cuda_optimizations)<br>        targets = targets_cpu.view(B, T).to(device=device, non_blocking=use_cuda_optimizations)<br>        state_dict = &#123;<span class="hljs-string">&quot;pq_idx&quot;</span>: pq_idx, <span class="hljs-string">&quot;rg_idx&quot;</span>: rg_idx&#125; <span class="hljs-comment"># we need this in case we wish to approximately resume training</span><br>        <span class="hljs-keyword">yield</span> inputs, targets, state_dict<br></code></pre></td></tr></table></figure><p>document_batches 函数流式 yields tokenized_batch_size个文本列表+(parquet文件索引，行组索引)，然后外层循环不断累积 token直到满足一个训练批次的需求，再切分成 inputs 和 targets返回，并附带当前的 parquet文件和行组索引状态，方便后续恢复训练时使用。</p><h3 id="优化器">优化器</h3><p>优化器代码在 <code>adamw.py</code> 和 <code>muon.py</code> 中。</p><h4 id="adamw-优化器">AdamW 优化器</h4><p>令参数为 <span class="math inline">\(\theta_t\)</span>，学习率为<span class="math inline">\(\alpha\)</span>，一阶矩估计为 <spanclass="math inline">\(m_t\)</span>，二阶矩估计为 <spanclass="math inline">\(v_t\)</span>，偏置修正后的一阶矩为 <spanclass="math inline">\(\hat{m}_t\)</span>，偏置修正后的二阶矩为 <spanclass="math inline">\(\hat{v}_t\)</span>，权重衰减系数为 <spanclass="math inline">\(\lambda\)</span>。 <span class="math display">\[\begin{align*}\theta_{t+1} &amp;= \theta_t - \alpha \frac{\hat{m}_t}{\sqrt{\hat{v}_t}+ \epsilon} - \alpha \lambda \theta_t \\m_{t+1} &amp;= \beta_1 m_t + (1 - \beta_1) g_t\\v_{t+1} &amp;= \beta_2 v_t + (1 - \beta_2) g_t^2\\\hat{m}_t &amp;= \frac{m_t}{1 - \beta_1^t} \\\hat{v}_t &amp;= \frac{v_t}{1 - \beta_2^t} \\\end{align*}\]</span></p><p>这里采取分布式训练的方式实现 AdamW 优化器，关键是利用了<code>torch.distributed.reduce_scatter_tensor</code>与<code>torch.distributed.all_gather_into_tensor</code>来实现梯度的分布式平均和切分，从而减少每个 GPU 的内存占用。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">@torch.compile</span><br><span class="hljs-meta">@torch.no_grad()</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">step</span>(<span class="hljs-params">self</span>):<br>    rank = dist.get_rank()<br>    world_size = dist.get_world_size()<br>    reduce_scatter_futures: <span class="hljs-built_in">list</span>[torch.Future] = []<br>    all_reduce_futures: <span class="hljs-built_in">list</span>[torch.Future] = []<br>    grad_slices = []<br>    <span class="hljs-keyword">for</span> group <span class="hljs-keyword">in</span> <span class="hljs-variable language_">self</span>.param_groups:<br>        params: <span class="hljs-built_in">list</span>[Tensor] = group[<span class="hljs-string">&quot;params&quot;</span>]<br>        <span class="hljs-keyword">for</span> base_i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(params)):<br>            <span class="hljs-keyword">assert</span> params[base_i].shape[<span class="hljs-number">0</span>] % world_size == <span class="hljs-number">0</span>, <span class="hljs-string">f&quot;First dim of parameter shape <span class="hljs-subst">&#123;params[base_i].shape&#125;</span> must be divisible by world size <span class="hljs-subst">&#123;world_size&#125;</span>&quot;</span><br>            grad = params[base_i].grad<br>            rank_size = grad.shape[<span class="hljs-number">0</span>] // world_size<br>            grad_slice = torch.empty_like(grad[:rank_size])<br>            reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=<span class="hljs-literal">True</span>).get_future())<br>            grad_slices.append(grad_slice)<br></code></pre></td></tr></table></figure><p>这里先对每个参数的梯度进行 <code>reduce_scatter</code>操作，将梯度平均后切分成多个片段，每个 GPU只保留自己负责的片段，减少内存占用。</p><p>目前有 grad_slices 列表，存储了每个参数在当前 GPU上的梯度片段和对应的 future 对象。 接下来对每个参数组，读取AdamW的超参数，再对每个参数进行更新：</p><p>状态存储在 self.state[p] 中，包括 step 计数器，一阶矩 exp_avg和二阶矩 exp_avg_sq。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><code class="hljs python">idx = <span class="hljs-number">0</span><br><span class="hljs-keyword">for</span> group <span class="hljs-keyword">in</span> <span class="hljs-variable language_">self</span>.param_groups:<br>    beta1, beta2 = group[<span class="hljs-string">&#x27;betas&#x27;</span>]<br>    eps = group[<span class="hljs-string">&#x27;eps&#x27;</span>]<br>    wd = group[<span class="hljs-string">&#x27;weight_decay&#x27;</span>]<br>    params = group[<span class="hljs-string">&#x27;params&#x27;</span>]<br>    <span class="hljs-keyword">for</span> base <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(params)):<br>        reduce_scatter_futures[idx].wait()<br>        p = params[base]<br>        rank_size = p.shape[<span class="hljs-number">0</span>] // world_size<br>        p_slice = p[rank * rank_size:(rank + <span class="hljs-number">1</span>) * rank_size]<br>        lr = group[<span class="hljs-string">&#x27;lr&#x27;</span>] * <span class="hljs-built_in">getattr</span>(p, <span class="hljs-string">&quot;lr_mul&quot;</span>, <span class="hljs-number">1.0</span>)<br>        state = <span class="hljs-variable language_">self</span>.state[p]<br>        g_slice = grad_slices[idx]<br>        <span class="hljs-comment"># State init</span><br>        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> state:<br>            state[<span class="hljs-string">&#x27;step&#x27;</span>] = torch.tensor(<span class="hljs-number">0</span>, dtype=torch.int64, device=p.device)<br>            state[<span class="hljs-string">&#x27;exp_avg&#x27;</span>] = torch.zeros_like(p_slice)<br>            state[<span class="hljs-string">&#x27;exp_avg_sq&#x27;</span>] = torch.zeros_like(p_slice)<br>        exp_avg = state[<span class="hljs-string">&#x27;exp_avg&#x27;</span>]<br>        exp_avg_sq = state[<span class="hljs-string">&#x27;exp_avg_sq&#x27;</span>]<br>        state[<span class="hljs-string">&#x27;step&#x27;</span>] += <span class="hljs-number">1</span><br>        t = state[<span class="hljs-string">&#x27;step&#x27;</span>]<br>        <span class="hljs-comment"># weight decay</span><br>        <span class="hljs-keyword">if</span> wd != <span class="hljs-number">0</span>:<br>            eff_weight_decay = lr * wd * <span class="hljs-built_in">getattr</span>(p, <span class="hljs-string">&quot;wd_mul&quot;</span>, <span class="hljs-number">1.0</span>)<br>            p_slice.mul_(<span class="hljs-number">1</span> - eff_weight_decay)<br>        <span class="hljs-comment"># update running averages</span><br>        exp_avg.mul_(beta1).add_(g_slice, alpha=<span class="hljs-number">1</span> - beta1)<br>        exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=<span class="hljs-number">1</span> - beta2)<br>        <span class="hljs-comment"># bias corrections</span><br>        bias1 = <span class="hljs-number">1</span> - beta1 ** t<br>        bias2 = <span class="hljs-number">1</span> - beta2 ** t<br>        <span class="hljs-comment"># compute step</span><br>        denom = (exp_avg_sq / bias2).sqrt().add_(eps)<br>        step_size = lr / bias1<br>        update = exp_avg.div(denom).mul_(step_size)<br>        p_slice.add_(other=update, alpha=-<span class="hljs-number">1.0</span>)<br>        idx += <span class="hljs-number">1</span><br>        all_reduce_futures.append(dist.all_gather_into_tensor(p, p_slice, async_op=<span class="hljs-literal">True</span>).get_future())<br>torch.futures.collect_all(all_reduce_futures).wait()<br><span class="hljs-comment"># 阻塞直到所有 all_gather 操作完成，确保所有 GPU 上的参数都同步更新完毕。</span><br></code></pre></td></tr></table></figure><h4 id="muon-优化器">Muon 优化器</h4><p>咕咕咕，可先看 <ahref="https://kexue.fm/archives/10592">link</a>。</p><h3 id="gpt-模型实现">GPT 模型实现</h3><p>架构流程： 1. 输入的Token进入输入嵌入层（TokenEmbeddings），输入类型： <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">inputs: Tensor  <span class="hljs-comment"># (B, T) long tensor of token indices</span><br>outputs: Tensor <span class="hljs-comment"># (B, T, C) float tensor of logits over vocabulary</span><br></code></pre></td></tr></table></figure> 2. RoPE 位置嵌入层（PositionalEmbeddings）为每个位置添加位置信息。 <figure class="highlight avrasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs avrasm"><span class="hljs-symbol">pos_emb:</span> Tensor <span class="hljs-meta"># (T, C) float tensor of positional embeddings</span><br></code></pre></td></tr></table></figure>加法广播到输入嵌入上</p><ol start="3" type="1"><li><p>多层 Transformer 块（TransformerBlocks）处理嵌入，捕捉上下文关系。</p></li><li><p>输出层（Output Layer）将 Transformer 的输出映射到词汇表大小的logits。</p></li><li><p>从 logits 计算损失（LossComputation），用于训练。或者按照multinomial分布采样生成下一个token，用于推理。</p></li></ol><p>我们需要的块包括： - 输入嵌入层 - 预计算位置嵌入层 - Transformer 块（多个堆叠+ 残差连接） - 层归一化 - 多头自注意力机制 （推理时缓存 K,V）- 前馈神经网络 - 输出层</p><p>表格如下</p><table><thead><tr><th>模块名称</th><th>输入维度</th><th>输出维度</th><th>核心功能</th></tr></thead><tbody><tr><td>Token Embedding</td><td>(B,T)</td><td>(B,T,C)</td><td>将离散索引转为连续向量</td></tr><tr><td>Pos Embedding</td><td>(T,C)</td><td>(T,C)</td><td>提供序列位置的绝对/相对信息</td></tr><tr><td>Transformer Block</td><td>(B,T,C)</td><td>(B,T,C)</td><td>包含 LayerNorm -&gt; Self-Attention -&gt; Residual -&gt; LayerNorm-&gt; FFN -&gt; Residual</td></tr><tr><td>Output Head</td><td>(B,T,C)</td><td>(B,T,V)</td><td>投影至词汇表空间</td></tr></tbody></table><p>优化方面：</p><p>nanochat 使用了 KV Cache 来加速推理过程中的自注意力计算，并且使用了GQA （Grouped Query Attention）来优化多头注意力的计算效率。</p><p>对于 KV Cache 的推理，要记录当前 Token 的位置索引，并索引正确的 RoPE位置嵌入（按绝对位置）。</p><p>对于 GQA 的实现，即保持 Q 的头数不变，但将 K 和 V的头数减少分组，每组共享 K 和 V。</p><p>先来看 <code>gpt.py</code> 中注意力模块的实现</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">CausalSelfAttention</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, config, layer_idx</span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        <span class="hljs-comment"># 记录是第几层 Transformer，用于 KV Cache 索引</span><br>        <span class="hljs-variable language_">self</span>.layer_idx = layer_idx <br>        <span class="hljs-comment"># 有多少个 Q 头，多少个 KV 头</span><br>        <span class="hljs-variable language_">self</span>.n_head = config.n_head <br>        <span class="hljs-variable language_">self</span>.n_kv_head = config.n_kv_head<br>        <span class="hljs-comment"># 嵌入维度和每个头的维度</span><br>        <span class="hljs-variable language_">self</span>.n_embd = config.n_embd<br>        <span class="hljs-variable language_">self</span>.head_dim = <span class="hljs-variable language_">self</span>.n_embd // <span class="hljs-variable language_">self</span>.n_head<br>        <span class="hljs-keyword">assert</span> <span class="hljs-variable language_">self</span>.n_embd % <span class="hljs-variable language_">self</span>.n_head == <span class="hljs-number">0</span><br>        <span class="hljs-keyword">assert</span> <span class="hljs-variable language_">self</span>.n_kv_head &lt;= <span class="hljs-variable language_">self</span>.n_head <span class="hljs-keyword">and</span> <span class="hljs-variable language_">self</span>.n_head % <span class="hljs-variable language_">self</span>.n_kv_head == <span class="hljs-number">0</span><br>        <span class="hljs-comment"># 定义线性层用于生成 Q、K、V 和输出投影</span><br>        <span class="hljs-variable language_">self</span>.c_q = nn.Linear(<span class="hljs-variable language_">self</span>.n_embd, <span class="hljs-variable language_">self</span>.n_head * <span class="hljs-variable language_">self</span>.head_dim, bias=<span class="hljs-literal">False</span>)<br>        <span class="hljs-variable language_">self</span>.c_k = nn.Linear(<span class="hljs-variable language_">self</span>.n_embd, <span class="hljs-variable language_">self</span>.n_kv_head * <span class="hljs-variable language_">self</span>.head_dim, bias=<span class="hljs-literal">False</span>)<br>        <span class="hljs-variable language_">self</span>.c_v = nn.Linear(<span class="hljs-variable language_">self</span>.n_embd, <span class="hljs-variable language_">self</span>.n_kv_head * <span class="hljs-variable language_">self</span>.head_dim, bias=<span class="hljs-literal">False</span>)<br>        <span class="hljs-variable language_">self</span>.c_proj = nn.Linear(<span class="hljs-variable language_">self</span>.n_embd, <span class="hljs-variable language_">self</span>.n_embd, bias=<span class="hljs-literal">False</span>)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x, cos_sin, kv_cache</span>):<br>        B, T, C = x.size()<br><br>        <span class="hljs-comment"># Project the input to get queries, keys, and values</span><br>        q = <span class="hljs-variable language_">self</span>.c_q(x).view(B, T, <span class="hljs-variable language_">self</span>.n_head, <span class="hljs-variable language_">self</span>.head_dim)<br>        k = <span class="hljs-variable language_">self</span>.c_k(x).view(B, T, <span class="hljs-variable language_">self</span>.n_kv_head, <span class="hljs-variable language_">self</span>.head_dim)<br>        v = <span class="hljs-variable language_">self</span>.c_v(x).view(B, T, <span class="hljs-variable language_">self</span>.n_kv_head, <span class="hljs-variable language_">self</span>.head_dim)<br><br>        <span class="hljs-comment"># Apply Rotary Embeddings to queries and keys to get relative positional encoding</span><br>        cos, sin = cos_sin<br>        q, k = apply_rotary_emb(q, cos, sin), apply_rotary_emb(k, cos, sin) <span class="hljs-comment"># QK rotary embedding</span><br>        q, k = norm(q), norm(k) <span class="hljs-comment"># QK norm</span><br>        q, k, v = q.transpose(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>), k.transpose(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>), v.transpose(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>) <span class="hljs-comment"># make head be batch dim, i.e. (B, T, H, D) -&gt; (B, H, T, D)</span><br><br>        <span class="hljs-comment"># Apply KV cache: insert current k,v into cache, get the full view so far</span><br>        <span class="hljs-keyword">if</span> kv_cache <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            k, v = kv_cache.insert_kv(<span class="hljs-variable language_">self</span>.layer_idx, k, v)<br>        <span class="hljs-comment"># 在预训练的时候，Tq == Tk，因为没有缓存</span><br>        <span class="hljs-comment"># 在推理的时候，Tq 可以小于 Tk，因为有缓存，每次只处理一个 token，然后询问前面的缓存 + 当前 token</span><br>        Tq = q.size(<span class="hljs-number">2</span>) <span class="hljs-comment"># number of queries in this forward pass</span><br>        Tk = k.size(<span class="hljs-number">2</span>) <span class="hljs-comment"># number of keys/values in total (in the cache + current forward pass)</span><br><br>        <span class="hljs-comment"># Attention: queries attend to keys/values autoregressively. A few cases to handle:</span><br>        enable_gqa = <span class="hljs-variable language_">self</span>.n_head != <span class="hljs-variable language_">self</span>.n_kv_head <span class="hljs-comment"># Group Query Attention (GQA): duplicate key/value heads to match query heads if desired</span><br>        <span class="hljs-keyword">if</span> kv_cache <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">or</span> Tq == Tk:<br>            <span class="hljs-comment"># During training (no KV cache), attend as usual with causal attention</span><br>            <span class="hljs-comment"># And even if there is KV cache, we can still use this simple version when Tq == Tk</span><br>            y = F.scaled_dot_product_attention(q, k, v, is_causal=<span class="hljs-literal">True</span>, enable_gqa=enable_gqa)<br>            <span class="hljs-comment"># 训练时没有缓存，或者推理时当前查询数等于缓存的键值数，都可以直接使用因果注意力。</span><br>        <span class="hljs-keyword">elif</span> Tq == <span class="hljs-number">1</span>:<br>            <span class="hljs-comment"># During inference but with a single query in this forward pass:</span><br>            <span class="hljs-comment"># The query has to attend to all the keys/values in the cache</span><br>            y = F.scaled_dot_product_attention(q, k, v, is_causal=<span class="hljs-literal">False</span>, enable_gqa=enable_gqa)<br>            <span class="hljs-comment"># 推理时如果只有一个查询，可以直接让这个查询关注缓存中的所有键值对。</span><br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-comment"># During inference AND we have a chunk of queries in this forward pass:</span><br>            <span class="hljs-comment"># First, each query attends to all the cached keys/values (i.e. full prefix)</span><br>            attn_mask = torch.zeros((Tq, Tk), dtype=torch.<span class="hljs-built_in">bool</span>, device=q.device) <span class="hljs-comment"># True = keep, False = mask</span><br>            prefix_len = Tk - Tq<br>            attn_mask[:, :prefix_len] = <span class="hljs-literal">True</span><br>            <span class="hljs-comment"># Then, causal attention within this chunk</span><br>            attn_mask[:, prefix_len:] = torch.tril(torch.ones((Tq, Tq), dtype=torch.<span class="hljs-built_in">bool</span>, device=q.device))<br>            y = F.scaled_dot_product_attention(q, k, v, attn_mask=attn_mask, enable_gqa=enable_gqa)<br>            <span class="hljs-comment"># 推理时如果有多个查询，需要先让每个查询关注缓存中的所有键值对，然后在当前查询块内进行因果注意力。（下三角掩码）</span><br><br>        <span class="hljs-comment"># Re-assemble the heads side by side and project back to residual stream</span><br>        y = y.transpose(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>).contiguous().view(B, T, -<span class="hljs-number">1</span>)<br>        y = <span class="hljs-variable language_">self</span>.c_proj(y)<br>        <span class="hljs-keyword">return</span> y<br></code></pre></td></tr></table></figure><p>简单的 MLP 和 Transformer Block 堆叠块</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">MLP</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, config</span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        <span class="hljs-variable language_">self</span>.c_fc = nn.Linear(config.n_embd, <span class="hljs-number">4</span> * config.n_embd, bias=<span class="hljs-literal">False</span>)<br>        <span class="hljs-variable language_">self</span>.c_proj = nn.Linear(<span class="hljs-number">4</span> * config.n_embd, config.n_embd, bias=<span class="hljs-literal">False</span>)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        x = <span class="hljs-variable language_">self</span>.c_fc(x)<br>        x = F.relu(x).square()<br>        x = <span class="hljs-variable language_">self</span>.c_proj(x)<br>        <span class="hljs-keyword">return</span> x<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Block</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, config, layer_idx</span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        <span class="hljs-variable language_">self</span>.attn = CausalSelfAttention(config, layer_idx)<br>        <span class="hljs-variable language_">self</span>.mlp = MLP(config)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x, cos_sin, kv_cache</span>):<br>        x = x + <span class="hljs-variable language_">self</span>.attn(norm(x), cos_sin, kv_cache)<br>        x = x + <span class="hljs-variable language_">self</span>.mlp(norm(x))<br>        <span class="hljs-keyword">return</span> x<br></code></pre></td></tr></table></figure><p>最后是 GPT 模型的整体实现（去除部分辅助函数）：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">GPT</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, config, pad_vocab_size_to=<span class="hljs-number">64</span></span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        <span class="hljs-variable language_">self</span>.config = config<br>        <span class="hljs-comment"># For DDP, we want vocab_size divisible by world_size. Also, there are potential performance benefits, see:</span><br>        <span class="hljs-comment"># https://huggingface.co/docs/transformers/main_classes/model#transformers.PreTrainedModel.resize_token_embeddings</span><br>        padded_vocab_size = ((config.vocab_size + pad_vocab_size_to - <span class="hljs-number">1</span>) // pad_vocab_size_to) * pad_vocab_size_to<br>        <span class="hljs-keyword">if</span> padded_vocab_size != config.vocab_size:<br>            print0(<span class="hljs-string">f&quot;Padding vocab_size from <span class="hljs-subst">&#123;config.vocab_size&#125;</span> to <span class="hljs-subst">&#123;padded_vocab_size&#125;</span> to be divisible by <span class="hljs-subst">&#123;pad_vocab_size_to&#125;</span>&quot;</span>)<br>        <span class="hljs-variable language_">self</span>.transformer = nn.ModuleDict(&#123;<br>            <span class="hljs-string">&quot;wte&quot;</span>: nn.Embedding(padded_vocab_size, config.n_embd),<br>            <span class="hljs-string">&quot;h&quot;</span>: nn.ModuleList([Block(config, layer_idx) <span class="hljs-keyword">for</span> layer_idx <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(config.n_layer)]),<br>        &#125;)<br>        <span class="hljs-comment"># wte 是 word to embedding 的缩写，表示词嵌入层</span><br>        <span class="hljs-comment"># h 是 transformer blocks </span><br>        <span class="hljs-variable language_">self</span>.lm_head = nn.Linear(config.n_embd, padded_vocab_size, bias=<span class="hljs-literal">False</span>)<br>        <span class="hljs-comment"># To support meta device initialization, we init the rotary embeddings here, but it&#x27;s just &quot;fake&quot; meta tensors only.</span><br>        <span class="hljs-comment"># As for rotary_seq_len, these rotary embeddings are pretty small/cheap in memory,</span><br>        <span class="hljs-comment"># so let&#x27;s just over-compute them by 10X, but assert fail if we ever reach that amount.</span><br>        <span class="hljs-comment"># In the future we can dynamically grow the cache, for now it&#x27;s fine.</span><br>        <span class="hljs-variable language_">self</span>.rotary_seq_len = config.sequence_len * <span class="hljs-number">10</span> <span class="hljs-comment"># 10X over-compute should be enough, TODO make nicer?</span><br>        <span class="hljs-comment"># 这里的意思是预计算 RoPE 位置嵌入，长度是序列长度的 10 倍，以防止在推理时超出范围，比如模型上下文长度是 1024，则预计算长度是 10240。理论上 sin, cos 可以滚动计算，但为了简单起见，直接预计算一个较大的长度。</span><br>        head_dim = config.n_embd // config.n_head<br>        cos, sin = <span class="hljs-variable language_">self</span>._precompute_rotary_embeddings(<span class="hljs-variable language_">self</span>.rotary_seq_len, head_dim)<br>        <span class="hljs-variable language_">self</span>.register_buffer(<span class="hljs-string">&quot;cos&quot;</span>, cos, persistent=<span class="hljs-literal">False</span>) <span class="hljs-comment"># persistent=False means it&#x27;s not saved to the checkpoint</span><br>        <span class="hljs-variable language_">self</span>.register_buffer(<span class="hljs-string">&quot;sin&quot;</span>, sin, persistent=<span class="hljs-literal">False</span>)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">init_weights</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        Initialize the full model in this one function for maximum clarity.</span><br><span class="hljs-string"></span><br><span class="hljs-string">        wte (embedding):     normal, std=1.0</span><br><span class="hljs-string">        lm_head:             normal, std=0.001</span><br><span class="hljs-string">        for each block:</span><br><span class="hljs-string">            attn.c_q:        uniform, std=1/sqrt(n_embd)</span><br><span class="hljs-string">            attn.c_k:        uniform, std=1/sqrt(n_embd)</span><br><span class="hljs-string">            attn.c_v:        uniform, std=1/sqrt(n_embd)</span><br><span class="hljs-string">            attn.c_proj:     zeros</span><br><span class="hljs-string">            mlp.c_fc:        uniform, std=1/sqrt(n_embd)</span><br><span class="hljs-string">            mlp.c_proj:      zeros</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br><br>        <span class="hljs-comment"># Embedding and unembedding</span><br>        torch.nn.init.normal_(<span class="hljs-variable language_">self</span>.transformer.wte.weight, mean=<span class="hljs-number">0.0</span>, std=<span class="hljs-number">1.0</span>)<br>        torch.nn.init.normal_(<span class="hljs-variable language_">self</span>.lm_head.weight, mean=<span class="hljs-number">0.0</span>, std=<span class="hljs-number">0.001</span>)<br><br>        <span class="hljs-comment"># Transformer blocks: uniform init with bound = sqrt(3) * std (same standard deviation as normal)</span><br>        n_embd = <span class="hljs-variable language_">self</span>.config.n_embd<br>        s = <span class="hljs-number">3</span>**<span class="hljs-number">0.5</span> * n_embd**-<span class="hljs-number">0.5</span> <span class="hljs-comment"># sqrt(3) multiplier makes sure Uniform achieves the same std as Normal</span><br>        <span class="hljs-keyword">for</span> block <span class="hljs-keyword">in</span> <span class="hljs-variable language_">self</span>.transformer.h:<br>            torch.nn.init.uniform_(block.attn.c_q.weight, -s, s) <span class="hljs-comment"># weights use Uniform to avoid outliers</span><br>            torch.nn.init.uniform_(block.attn.c_k.weight, -s, s)<br>            torch.nn.init.uniform_(block.attn.c_v.weight, -s, s)<br>            torch.nn.init.zeros_(block.attn.c_proj.weight) <span class="hljs-comment"># projections are zero</span><br>            torch.nn.init.uniform_(block.mlp.c_fc.weight, -s, s)<br>            torch.nn.init.zeros_(block.mlp.c_proj.weight)<br><br>        <span class="hljs-comment"># Rotary embeddings</span><br>        head_dim = <span class="hljs-variable language_">self</span>.config.n_embd // <span class="hljs-variable language_">self</span>.config.n_head<br>        cos, sin = <span class="hljs-variable language_">self</span>._precompute_rotary_embeddings(<span class="hljs-variable language_">self</span>.rotary_seq_len, head_dim)<br>        <span class="hljs-variable language_">self</span>.cos, <span class="hljs-variable language_">self</span>.sin = cos, sin<br><br>        <span class="hljs-comment"># Cast token embeddings to bf16: optimizer can tolerate it and it saves memory</span><br>        <span class="hljs-keyword">if</span> <span class="hljs-variable language_">self</span>.transformer.wte.weight.device.<span class="hljs-built_in">type</span> == <span class="hljs-string">&quot;cuda&quot;</span>:<br>            <span class="hljs-variable language_">self</span>.transformer.wte.to(dtype=torch.bfloat16)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_precompute_rotary_embeddings</span>(<span class="hljs-params">self, seq_len, head_dim, base=<span class="hljs-number">10000</span>, device=<span class="hljs-literal">None</span></span>):<br>        <span class="hljs-comment"># <span class="hljs-doctag">TODO:</span> bump base theta more? e.g. 100K is more common more recently</span><br>        <span class="hljs-comment"># autodetect the device from model embeddings</span><br>        <span class="hljs-keyword">if</span> device <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>            device = <span class="hljs-variable language_">self</span>.transformer.wte.weight.device<br>        <span class="hljs-comment"># stride the channels</span><br>        channel_range = torch.arange(<span class="hljs-number">0</span>, head_dim, <span class="hljs-number">2</span>, dtype=torch.float32, device=device)<br>        inv_freq = <span class="hljs-number">1.0</span> / (base ** (channel_range / head_dim))<br>        <span class="hljs-comment"># stride the time steps</span><br>        t = torch.arange(seq_len, dtype=torch.float32, device=device)<br>        <span class="hljs-comment"># calculate the rotation frequencies at each (time, channel) pair</span><br>        freqs = torch.outer(t, inv_freq)<br>        cos, sin = freqs.cos(), freqs.sin()<br>        cos, sin = cos.bfloat16(), sin.bfloat16() <span class="hljs-comment"># keep them in bfloat16</span><br>        cos, sin = cos[<span class="hljs-literal">None</span>, :, <span class="hljs-literal">None</span>, :], sin[<span class="hljs-literal">None</span>, :, <span class="hljs-literal">None</span>, :] <span class="hljs-comment"># add batch and head dims for later broadcasting</span><br>        <span class="hljs-keyword">return</span> cos, sin<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">get_device</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">return</span> <span class="hljs-variable language_">self</span>.transformer.wte.weight.device<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">setup_optimizers</span>(<span class="hljs-params">self, unembedding_lr=<span class="hljs-number">0.004</span>, embedding_lr=<span class="hljs-number">0.2</span>, matrix_lr=<span class="hljs-number">0.02</span>, weight_decay=<span class="hljs-number">0.0</span>, adam_betas=(<span class="hljs-params"><span class="hljs-number">0.8</span>, <span class="hljs-number">0.95</span></span>)</span>):<br>        model_dim = <span class="hljs-variable language_">self</span>.config.n_embd<br>        ddp, rank, local_rank, world_size = get_dist_info()<br>        <span class="hljs-comment"># Separate out all parameters into 3 groups (matrix, embedding, lm_head)</span><br>        matrix_params = <span class="hljs-built_in">list</span>(<span class="hljs-variable language_">self</span>.transformer.h.parameters())<br>        embedding_params = <span class="hljs-built_in">list</span>(<span class="hljs-variable language_">self</span>.transformer.wte.parameters())<br>        lm_head_params = <span class="hljs-built_in">list</span>(<span class="hljs-variable language_">self</span>.lm_head.parameters())<br>        <span class="hljs-keyword">assert</span> <span class="hljs-built_in">len</span>(<span class="hljs-built_in">list</span>(<span class="hljs-variable language_">self</span>.parameters())) == <span class="hljs-built_in">len</span>(matrix_params) + <span class="hljs-built_in">len</span>(embedding_params) + <span class="hljs-built_in">len</span>(lm_head_params)<br>        <span class="hljs-comment"># Create the AdamW optimizer for the embedding and lm_head</span><br>        <span class="hljs-comment"># Scale the LR for the AdamW parameters by ∝1/√dmodel (having tuned the LRs for 768 dim model)</span><br>        dmodel_lr_scale = (model_dim / <span class="hljs-number">768</span>) ** -<span class="hljs-number">0.5</span><br>        print0(<span class="hljs-string">f&quot;Scaling the LR for the AdamW parameters ∝1/√(<span class="hljs-subst">&#123;model_dim&#125;</span>/768) = <span class="hljs-subst">&#123;dmodel_lr_scale:<span class="hljs-number">.6</span>f&#125;</span>&quot;</span>)<br>        adam_groups = [<br>            <span class="hljs-built_in">dict</span>(params=lm_head_params, lr=unembedding_lr * dmodel_lr_scale),<br>            <span class="hljs-built_in">dict</span>(params=embedding_params, lr=embedding_lr * dmodel_lr_scale),<br>        ]<br>        adamw_kwargs = <span class="hljs-built_in">dict</span>(betas=adam_betas, eps=<span class="hljs-number">1e-10</span>, weight_decay=weight_decay)<br>        AdamWFactory = DistAdamW <span class="hljs-keyword">if</span> ddp <span class="hljs-keyword">else</span> partial(torch.optim.AdamW, fused=<span class="hljs-literal">True</span>)<br>        adamw_optimizer = AdamWFactory(adam_groups, **adamw_kwargs)<br>        <span class="hljs-comment"># Create the Muon optimizer for the linear layers</span><br>        muon_kwargs = <span class="hljs-built_in">dict</span>(lr=matrix_lr, momentum=<span class="hljs-number">0.95</span>)<br>        MuonFactory = DistMuon <span class="hljs-keyword">if</span> ddp <span class="hljs-keyword">else</span> Muon<br>        muon_optimizer = MuonFactory(matrix_params, **muon_kwargs)<br><br>        <span class="hljs-comment"># 对2d矩阵参数使用 Muon 优化器，对1d的嵌入和输出层参数使用 AdamW 优化器。</span><br>        <br>        <span class="hljs-comment"># Combine them the two optimizers into one list</span><br>        optimizers = [adamw_optimizer, muon_optimizer]<br>        <span class="hljs-keyword">for</span> opt <span class="hljs-keyword">in</span> optimizers:<br>            <span class="hljs-keyword">for</span> group <span class="hljs-keyword">in</span> opt.param_groups:<br>                group[<span class="hljs-string">&quot;initial_lr&quot;</span>] = group[<span class="hljs-string">&quot;lr&quot;</span>]<br>        <span class="hljs-keyword">return</span> optimizers<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, idx, targets=<span class="hljs-literal">None</span>, kv_cache=<span class="hljs-literal">None</span>, loss_reduction=<span class="hljs-string">&#x27;mean&#x27;</span></span>):<br>        B, T = idx.size()<br><br>        <span class="hljs-comment"># Grab the rotary embeddings for the current sequence length (they are of shape (1, seq_len, 1, head_dim/2))</span><br>        <span class="hljs-keyword">assert</span> T &lt;= <span class="hljs-variable language_">self</span>.cos.size(<span class="hljs-number">1</span>), <span class="hljs-string">f&quot;Sequence length grew beyond the rotary embeddings cache: <span class="hljs-subst">&#123;T&#125;</span> &gt; <span class="hljs-subst">&#123;self.cos.size(<span class="hljs-number">1</span>)&#125;</span>&quot;</span><br>        <span class="hljs-keyword">assert</span> idx.device == <span class="hljs-variable language_">self</span>.cos.device, <span class="hljs-string">f&quot;Rotary embeddings and idx are on different devices: <span class="hljs-subst">&#123;idx.device&#125;</span> != <span class="hljs-subst">&#123;self.cos.device&#125;</span>&quot;</span><br>        <span class="hljs-keyword">assert</span> <span class="hljs-variable language_">self</span>.cos.dtype == torch.bfloat16, <span class="hljs-string">&quot;Rotary embeddings must be in bfloat16&quot;</span><br>        <span class="hljs-comment"># if kv cache exists, we need to offset the rotary embeddings to the current position in the cache</span><br>        T0 = <span class="hljs-number">0</span> <span class="hljs-keyword">if</span> kv_cache <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">else</span> kv_cache.get_pos()<br>        cos_sin = <span class="hljs-variable language_">self</span>.cos[:, T0:T0+T], <span class="hljs-variable language_">self</span>.sin[:, T0:T0+T] <span class="hljs-comment"># truncate cache to current sequence length</span><br><br>        <span class="hljs-comment"># Forward the trunk of the Transformer</span><br>        x = <span class="hljs-variable language_">self</span>.transformer.wte(idx)<br>        x = norm(x)<br>        <span class="hljs-keyword">for</span> block <span class="hljs-keyword">in</span> <span class="hljs-variable language_">self</span>.transformer.h:<br>            x = block(x, cos_sin, kv_cache)<br>        x = norm(x)<br><br>        <span class="hljs-comment"># Forward the lm_head (compute logits)</span><br>        softcap = <span class="hljs-number">15</span> <span class="hljs-comment"># smoothly cap the logits to the range [-softcap, softcap]</span><br>        logits = <span class="hljs-variable language_">self</span>.lm_head(x) <span class="hljs-comment"># (B, T, padded_vocab_size) &lt;- very big tensor, large amount of memory</span><br>        logits = logits[..., :<span class="hljs-variable language_">self</span>.config.vocab_size] <span class="hljs-comment"># slice to remove padding</span><br>        logits = logits.<span class="hljs-built_in">float</span>() <span class="hljs-comment"># switch to fp32 for logit softcap and loss computation</span><br>        logits = softcap * torch.tanh(logits / softcap) <span class="hljs-comment"># squash the logits</span><br><br>        <span class="hljs-comment"># 这里利用 tanh 函数对 logits 进行平滑限制在 [-softcap, softcap]，防止极端值导致训练不稳定。</span><br><br>        <span class="hljs-keyword">if</span> targets <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            <span class="hljs-comment"># training: given the targets, compute and return the loss</span><br>            <span class="hljs-comment"># TODO experiment with chunked cross-entropy?</span><br>            loss = F.cross_entropy(logits.view(-<span class="hljs-number">1</span>, logits.size(-<span class="hljs-number">1</span>)), targets.view(-<span class="hljs-number">1</span>), ignore_index=-<span class="hljs-number">1</span>, reduction=loss_reduction)<br>            <span class="hljs-keyword">return</span> loss<br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-comment"># inference: just return the logits directly</span><br>            <span class="hljs-keyword">return</span> logits<br><br><span class="hljs-meta">    @torch.inference_mode()</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">generate</span>(<span class="hljs-params">self, tokens, max_tokens, temperature=<span class="hljs-number">1.0</span>, top_k=<span class="hljs-literal">None</span>, seed=<span class="hljs-number">42</span></span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        Naive autoregressive streaming inference.</span><br><span class="hljs-string">        To make it super simple, let&#x27;s assume:</span><br><span class="hljs-string">        - batch size is 1</span><br><span class="hljs-string">        - ids and the yielded tokens are simple Python lists and ints</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        <span class="hljs-keyword">assert</span> <span class="hljs-built_in">isinstance</span>(tokens, <span class="hljs-built_in">list</span>)<br>        device = <span class="hljs-variable language_">self</span>.get_device()<br>        rng = <span class="hljs-literal">None</span><br>        <span class="hljs-keyword">if</span> temperature &gt; <span class="hljs-number">0</span>:<br>            rng = torch.Generator(device=device)<br>            rng.manual_seed(seed)<br>        ids = torch.tensor([tokens], dtype=torch.long, device=device) <span class="hljs-comment"># add batch dim</span><br>        <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(max_tokens):<br>            logits = <span class="hljs-variable language_">self</span>.forward(ids) <span class="hljs-comment"># (B, T, vocab_size)</span><br>            logits = logits[:, -<span class="hljs-number">1</span>, :] <span class="hljs-comment"># (B, vocab_size)</span><br>            <span class="hljs-keyword">if</span> top_k <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>                v, _ = torch.topk(logits, <span class="hljs-built_in">min</span>(top_k, logits.size(-<span class="hljs-number">1</span>)))<br>                logits[logits &lt; v[:, [-<span class="hljs-number">1</span>]]] = -<span class="hljs-built_in">float</span>(<span class="hljs-string">&#x27;Inf&#x27;</span>)<br>            <span class="hljs-keyword">if</span> temperature &gt; <span class="hljs-number">0</span>:<br>                logits = logits / temperature<br>                probs = F.softmax(logits, dim=-<span class="hljs-number">1</span>)<br>                next_ids = torch.multinomial(probs, num_samples=<span class="hljs-number">1</span>, generator=rng)<br>            <span class="hljs-keyword">else</span>:<br>                next_ids = torch.argmax(logits, dim=-<span class="hljs-number">1</span>, keepdim=<span class="hljs-literal">True</span>)<br>            ids = torch.cat((ids, next_ids), dim=<span class="hljs-number">1</span>)<br>            token = next_ids.item()<br>            <span class="hljs-keyword">yield</span> token<br></code></pre></td></tr></table></figure><p>以下是 Gemini 的总结</p><p>权重初始化：</p><ol type="1"><li><p>嵌入层与输出层的极端标准差差异 <code>wte</code> (TokenEmbedding): <code>std=1.0</code>: Token 嵌入被初始化为标准正态分布。由于<span class="math inline">\(wte\)</span>的权重通常会随后被层归一化（LayerNorm）处理，较大的初始标准差可以为模型提供丰富的初始特征表示。<code>lm_head</code>:<code>std=0.001</code> 输出层使用了非常小的标准差。目的：在训练开始时，使模型对所有词汇的预测概率趋于均匀分布。如果初始权重过大，模型会产生强烈的随机偏见，导致初始损失值（Loss）极高，增加收敛难度。</p></li><li><p>均匀分布初始化与 <span class="math inline">\(\sqrt{3}\)</span>的数学推导代码中使用了均匀分布 <code>uniform_(-s, s)</code>而非正态分布，并定义了 <span class="math inline">\(s = \sqrt{3} \times\frac{1}{\sqrt{n\_embd}}\)</span>。为什么用 Uniform：注释提到是为了“避免离群值（outliers）”。正态分布理论上可能产生极大或极小的权重，而均匀分布的范围是严格受限的。<spanclass="math inline">\(\sqrt{3}\)</span> 的来源： 对于均匀分布 <spanclass="math inline">\(U(-s, s)\)</span>，其方差为 <spanclass="math inline">\(Var = \frac{(s - (-s))^2}{12} = \frac{4s^2}{12} =\frac{s^2}{3}\)</span>。为了让均匀分布的方差等于期望的方差 <spanclass="math inline">\(\sigma^2\)</span>（即 <spanclass="math inline">\(1/n\_embd\)</span>），则需要： <spanclass="math display">\[\frac{s^2}{3} = \sigma^2 \implies s = \sigma \sqrt{3}\]</span> 这确保了无论使用哪种分布，权重的统计特性是一致的。</p></li><li><p>投影层初始化为零 (<code>c_proj</code>:zeros)这是一个非常关键的技巧，常见于高性能模型实现（如 GPT-2或部分版本的 Llama）：残差流的恒等映射： 在训练初始时刻，如果注意力投影<code>c_proj</code> 为零，那么 Transformer块的输出就等于输入（因为残差连接 <span class="math inline">\(x + 0 =x\)</span>）。逻辑：这种做法类似于“恒等函数”初始化，让模型先学习直通的数据流动，再逐渐通过训练学习如何修改残差流中的特征。这极大地提高了极深网络的训练稳定性。</p></li><li><p>旋转位置嵌入（RoPE）的预计算操作： 调用内部函数生成<code>cos</code> 和 <code>sin</code> 表。逻辑：如前所述，这部分内容是确定性的数学值（频率分布），不需要学习，因此在初始化阶段一次性生成并缓存，以供推理和训练时快速索引。</p></li><li><p>混合精度转换 (<code>bfloat16</code>)操作： 显式将<code>wte.weight</code> 转为 <code>bfloat16</code>。优势： 显存节省：词表往往很大，将其从 float32 转为 bf16可以直接节省一半的词表显存。精度特性： <code>bf16</code> 具有与<code>fp32</code>相同的指数位范围，能够有效防止训练中的溢出风险。</p></li></ol><p>RoPE 位置嵌入预计算公式：</p><p>设 <span class="math inline">\(d\)</span> 为每个注意力头的维度，<spanclass="math inline">\(i\)</span> 为通道索引，<spanclass="math inline">\(t\)</span> 为时间步索引，<spanclass="math inline">\(\theta\)</span> 为基频（通常取 10000）。则每个位置<span class="math inline">\(t\)</span> 和通道 <spanclass="math inline">\(i\)</span> 的旋转频率和角度为： <spanclass="math display">\[\text{freq}(t, i) = \frac{1}{\theta^{\frac{2i}{d}}}\]</span> <span class="math display">\[\text{angle}(t, i) = t \times \text{freq}(t, i) =\frac{t}{\theta^{\frac{2i}{d}}}\]</span></p>]]></content>
    
    
    <categories>
      
      <category>Programming</category>
      
    </categories>
    
    
    <tags>
      
      <tag>LLM</tag>
      
      <tag>Open Source</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>随机微分方程（SDE）</title>
    <link href="/2026/01/01/%E9%9A%8F%E6%9C%BA%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B%EF%BC%88SDE%EF%BC%89/"/>
    <url>/2026/01/01/%E9%9A%8F%E6%9C%BA%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B%EF%BC%88SDE%EF%BC%89/</url>
    
    <content type="html"><![CDATA[<h2 id="参考文献">参考文献</h2><p>基本上参考 Bernt Øksendal 的《Stochastic Differential Equations: AnIntroduction with Applications》。</p><p>加上一些个人的理解，和测度论上的一些补充。也许会简单过一些习题。</p><p>这里主要是速通，不会太过深入某些定理的证明细节。</p><p>前置：基本的概率论，实分析，最好有测度论知识。</p><h2 id="测度概率论和随机过程基础-chapter-2-of-øksendal">1.测度概率论和随机过程基础 (Chapter 2 of Øksendal)</h2><p>在正式进入随机微分方程之前，先简单回顾一下测度论和概率论的基础知识。</p><p>一个代数 <span class="math inline">\(\mathcal{A}\)</span>是定义在集合 <span class="math inline">\(\Omega\)</span>上的子集族，满足：</p><ol type="1"><li><span class="math inline">\(\Omega \in \mathcal{A}\)</span>；</li><li>如果 <span class="math inline">\(A \in \mathcal{A}\)</span>，则<span class="math inline">\(A^c \in \mathcal{A}\)</span>；</li><li>如果 <span class="math inline">\(A_1, A_2, \ldots, A_n \in\mathcal{A}\)</span>，则 <span class="math inline">\(\bigcup_{i=1}^{n}A_i \in \mathcal{A}\)</span>。</li></ol><p>一个 <span class="math inline">\(\sigma\)</span>-代数 <spanclass="math inline">\(\mathcal{F}\)</span> 是定义在集合 <spanclass="math inline">\(\Omega\)</span> 上的子集族，满足： 1. <spanclass="math inline">\(\Omega \in \mathcal{F}\)</span>； 2. 如果 <spanclass="math inline">\(A \in \mathcal{F}\)</span>，则 <spanclass="math inline">\(A^c \in \mathcal{F}\)</span>； 3. 如果 <spanclass="math inline">\(A_1, A_2, \ldots \in \mathcal{F}\)</span>，则<span class="math inline">\(\bigcup_{i=1}^{\infty} A_i \in\mathcal{F}\)</span>。（补集一下自然也对可列交成立）</p><p>一个测度是定义在 <span class="math inline">\(\sigma\)</span>-代数<span class="math inline">\(\mathcal{F}\)</span> 上的函数 <spanclass="math inline">\(\mu: \mathcal{F} \to [0, \infty]\)</span>，满足：1. <span class="math inline">\(\mu(\emptyset) = 0\)</span>； 2. 如果<span class="math inline">\(\{A_i\}_{i=1}^{\infty}\)</span> 是 <spanclass="math inline">\(\mathcal{F}\)</span> 中的可列不交集族，则 <spanclass="math inline">\(\mu\left(\bigcup_{i=1}^{\infty} A_i\right) =\sum_{i=1}^{\infty} \mu(A_i)\)</span>。</p><p>我们可以证明测度满足以下几个有用的性质： 1. 单调性：如果 <spanclass="math inline">\(A, B \in \mathcal{F}\)</span> 且 <spanclass="math inline">\(A \subseteq B\)</span>，则 <spanclass="math inline">\(\mu(A) \leq \mu(B)\)</span>。 2. 次可加性：如果<span class="math inline">\(\{A_i\}_{i=1}^{\infty}\)</span> 是 <spanclass="math inline">\(\mathcal{F}\)</span> 中的任意集合族，则 <spanclass="math inline">\(\mu\left(\bigcup_{i=1}^{\infty} A_i\right) \leq\sum_{i=1}^{\infty} \mu(A_i)\)</span>。 3. 上连续性：如果 <spanclass="math inline">\(\{A_n\}_{n=1}^{\infty}\)</span> 是 <spanclass="math inline">\(\mathcal{F}\)</span> 中的递增集合族，即 <spanclass="math inline">\(A_1 \subseteq A_2 \subseteq \ldots\)</span>，则<span class="math inline">\(\mu\left(\bigcup_{n=1}^{\infty} A_n\right) =\lim_{n \to \infty} \mu(A_n)\)</span>。 4. 下连续性：如果 <spanclass="math inline">\(\{A_n\}_{n=1}^{\infty}\)</span> 是 <spanclass="math inline">\(\mathcal{F}\)</span> 中的递减集合族，即 <spanclass="math inline">\(A_1 \supseteq A_2 \supseteq \ldots\)</span>，且<span class="math inline">\(\mu(A_1) &lt; \infty\)</span>，则 <spanclass="math inline">\(\mu\left(\bigcap_{n=1}^{\infty} A_n\right) =\lim_{n \to \infty} \mu(A_n)\)</span>。</p><p>给定一族子集 <span class="math inline">\(\mathcal{U} \subseteq\mathcal{P}(\Omega)\)</span>，存在唯一的最小 <spanclass="math inline">\(\sigma\)</span>-代数 <spanclass="math inline">\(\mathcal{H}_\mathcal{U}\)</span>，使得 <spanclass="math inline">\(\mathcal{U} \subseteq\mathcal{H}_\mathcal{U}\)</span>，称为由 <spanclass="math inline">\(\mathcal{U}\)</span> 生成的 <spanclass="math inline">\(\sigma\)</span>-代数。由此，我们可以定义 Borel<span class="math inline">\(\sigma\)</span>-代数 <spanclass="math inline">\(\mathcal{B}\)</span> 为 拓扑空间 <spanclass="math inline">\(\Omega\)</span> 的所有开集生成的 <spanclass="math inline">\(\sigma\)</span>-代数。</p><p>一个概率空间是一个三元组 <span class="math inline">\((\Omega,\mathcal{F}, P)\)</span>，其中 <spanclass="math inline">\(\Omega\)</span> 是样本空间，<spanclass="math inline">\(\mathcal{F}\)</span> 是定义在 <spanclass="math inline">\(\Omega\)</span> 上的 <spanclass="math inline">\(\sigma\)</span>-代数，<spanclass="math inline">\(P\)</span> 是定义在 <spanclass="math inline">\(\mathcal{F}\)</span> 上的概率测度，满足 <spanclass="math inline">\(P(\Omega) = 1\)</span>。如果 <spanclass="math inline">\(\mathcal{F}\)</span> 包含所有<spanclass="math inline">\(P\)</span>-外测度零的子集 <spanclass="math inline">\(G \subseteq \Omega\)</span>，即 <spanclass="math inline">\(\inf{ P(A) : A \in \mathcal{F}, G \subseteq A } =0\)</span> ，则称概率空间 <span class="math inline">\((\Omega,\mathcal{F}, P)\)</span> 是完备的。</p><p>给定一个概率空间 <span class="math inline">\((\Omega, \mathcal{F},P)\)</span>，一个函数是<spanclass="math inline">\(\mathcal{F}\)</span>-可测的，如果对于所有的 Borel集合 <span class="math inline">\(B \in\mathcal{B}(\mathbb{R})\)</span>，有 <spanclass="math inline">\(X^{-1}(B) \in\mathcal{F}\)</span>。随机变量是定义在概率空间 <spanclass="math inline">\((\Omega, \mathcal{F}, P)\)</span> 上的 <spanclass="math inline">\(\mathcal{F}\)</span>-可测函数，一般取值在 <spanclass="math inline">\(\mathbb{R}^n\)</span> 上。</p><p>反过来，给定一个函数 <span class="math inline">\(X: \Omega \to\mathbb{R}^n\)</span>，定义由 <spanclass="math inline">\(\Omega\)</span> 生成的 <spanclass="math inline">\(\sigma\)</span>-代数为 <spanclass="math inline">\(\sigma(X) = \{X^{-1}(B) : B \in\mathcal{B}\}\)</span>。 其中 <spanclass="math inline">\(\mathcal{B}\)</span> 是 <spanclass="math inline">\(\mathbb{R}^n\)</span> 上的 Borel <spanclass="math inline">\(\sigma\)</span>-代数 （或写作 <spanclass="math inline">\(\mathcal{H}_X\)</span>）。同时还引导一个 <spanclass="math inline">\(\mathbb{R}^n\)</span> 上的测度 <spanclass="math inline">\(\mu_X\)</span>，定义为 <spanclass="math inline">\(\mu_X(B) = P(X^{-1}(B))\)</span>，称为 <spanclass="math inline">\(X\)</span> 的分布（或诱导测度），又写作 <spanclass="math inline">\(\mathrm{Law}(X)\)</span>。我们说随机变量 <spanclass="math inline">\(X\)</span> 服从分布 <spanclass="math inline">\(\mu_X\)</span>，记作 <span class="math inline">\(X\sim \mu_X\)</span>。</p><h3 id="doob-dynkin-引理">Doob-Dynkin 引理</h3><p>设 <span class="math inline">\(X: \Omega \to \mathbb{R}^n\)</span> 和<span class="math inline">\(Y: \Omega \to \mathbb{R}^n\)</span>是两个随机变量，则存在一个 Borel 可测函数 <span class="math inline">\(f:\mathbb{R}^n \to \mathbb{R}^n\)</span>，使得 <spanclass="math inline">\(Y = f(X)\)</span> 当且仅当 <spanclass="math inline">\(\sigma(Y) \subseteq \sigma(X)\)</span>（<spanclass="math inline">\(Y\)</span> 是 <spanclass="math inline">\(\mathcal{\mathcal{H}}_X\)</span>-可测的）。</p><p>证明：</p><p>(<span class="math inline">\(\Rightarrow\)</span>) 如果存在 Borel可测函数 <span class="math inline">\(f\)</span> 使得 <spanclass="math inline">\(Y = f(X)\)</span>，则对于任意的 Borel 集合 <spanclass="math inline">\(B \in \mathcal{B}\)</span>，<spanclass="math inline">\(Y^{-1}(B) = X^{-1}(f^{-1}(B)).\)</span> 由于 <spanclass="math inline">\(f\)</span> 是 Borel 可测的，<spanclass="math inline">\(f^{-1}(B)\)</span> 也是 Borel 集合，因此 <spanclass="math inline">\(Y^{-1}(B) \in \sigma(X)\)</span>，即 <spanclass="math inline">\(\sigma(Y) \subseteq \sigma(X)\)</span>。</p><p>(<span class="math inline">\(\Leftarrow\)</span>) 如果 <spanclass="math inline">\(Y = \mathbf{1}_A\)</span> 是集合 <spanclass="math inline">\(A \in \sigma(X)\)</span> 的指示函数，则存在 Borel集合 <span class="math inline">\(B \in \mathcal{B}\)</span>，使得 <spanclass="math inline">\(A = X^{-1}(B)\)</span>。定义函数 <spanclass="math inline">\(f: \mathbb{R}^n \to \mathbb{R}\)</span> 为 <spanclass="math inline">\(f(x) = \mathbf{1}_B(x)\)</span>，则 <spanclass="math inline">\(Y(\omega) = f(X(\omega))\)</span> 对所有 <spanclass="math inline">\(\omega \in \Omega\)</span>成立。然后可以将该结论推广到简单函数 <span class="math inline">\(Y =\sum a_i \mathbf{1}_{A_i}\)</span>，其中 <span class="math inline">\(A_i\in \sigma(X)\)</span>，再通过测度论里的套路构造逐点极限收敛序列<spanclass="math inline">\(Y_k\)</span>，推广到非负的随机变量 <spanclass="math inline">\(Y\)</span>，最后正负分解推广到任意随机变量 <spanclass="math inline">\(Y\)</span>。</p><p>这个引理的意义在于，它告诉 <span class="math inline">\(Y\)</span>是否可以通过 <span class="math inline">\(X\)</span> 来表示，取决于 <spanclass="math inline">\(Y\)</span> 的信息（引导的测度）是否包含在 <spanclass="math inline">\(X\)</span> 的信息中。</p><h3 id="期望">期望</h3><p>设 <span class="math inline">\((\Omega, \mathcal{F}, P)\)</span>是一个概率空间，<span class="math inline">\(X: \Omega \to\mathbb{R}^n\)</span> 是定义在该空间上的随机变量。<spanclass="math inline">\(X\)</span> 的期望值（或数学期望）定义为 <spanclass="math display">\[E[X] = \int_{\Omega} X(\omega) dP(\omega),\]</span> 前提是 <span class="math inline">\(\int_{\Omega} \|X(\omega)\|dP(\omega) &lt; \infty\)</span>，即绝对可积。</p><p>一般地，如果 <span class="math inline">\(f: \mathbb{R}^n \to\mathbb{R}\)</span> 是一个 Borel 可测函数，且 <spanclass="math inline">\(E[|f(X)|] &lt; \infty\)</span>，则 <spanclass="math inline">\(f(X)\)</span> 的期望值定义为 <spanclass="math display">\[E[f(X)] = \int_{\Omega} f(X(\omega)) dP(\omega) = \int_{\mathbb{R}^n}f(x) d\mu_X(x),\]</span></p><p>特别地，随机变量 <span class="math inline">\(X\)</span> 的 <spanclass="math inline">\(n\)</span> 阶矩定义为 <spanclass="math display">\[E[X^n] = \int_{\Omega} X(\omega)^n dP(\omega),\]</span></p><h3 id="lp-空间"><span class="math inline">\(L^p\)</span> 空间</h3><p>设 <span class="math inline">\((\Omega, \mathcal{F}, P)\)</span>是一个概率空间，<span class="math inline">\(p\in[1,\infty)\)</span>。我们定义随机变量 <spanclass="math inline">\(X:\Omega \to \mathbb{R}^n\)</span> 的 <spanclass="math inline">\(L^p\)</span> 范数为 <span class="math display">\[\|X\|_p = \|X\|_{L^{p}(P)}= \left( E[\|X\|^p] \right)^{1/p} = \left(\int_{\Omega} \|X(\omega)\|^p dP(\omega) \right)^{1/p},\]</span> 如果 <span class="math inline">\(p=\infty\)</span>，则定义为<span class="math display">\[\|X\|_\infty = \|X\|_{L^{\infty}(P)}= \inf \{ N \geq 0 :  \|X(\omega)\|\leq N \text{ a.s.} \}.\]</span> 我们定义 <span class="math inline">\(L^p\)</span> 空间为 <spanclass="math display">\[L^p(P) = L^p(\Omega) = \{ X: \Omega \to \mathbb{R}^n ; X \text{ 是 }\mathcal{F}\text{-可测的且 } \|X\|_p &lt; \infty \}.\]</span></p><p><span class="math inline">\(L^p\)</span> 空间配备范数 <spanclass="math inline">\(\|\cdot\|_p\)</span> 后是一个 Banach 空间。当<span class="math inline">\(p=2\)</span> 时，<spanclass="math inline">\(L^2\)</span> 空间是一个 Hilbert 空间，内积定义为<span class="math display">\[\langle X, Y \rangle = E[X \cdot Y] = \int_{\Omega} X(\omega) \cdotY(\omega) dP(\omega).\]</span></p><h3 id="独立与条件期望">独立与条件期望</h3><p>两个集合 <span class="math inline">\(A, B \in \mathcal{F}\)</span>称为独立的，如果 <span class="math inline">\(P(A \cap B) =P(A)P(B)\)</span>。一般的，一个集合族 <spanclass="math inline">\(\{A_i\}_{i \in I} \subseteq \mathcal{F}\)</span>称为相互独立的，如果对于任意有限子集 <span class="math inline">\(J\subseteq I\)</span>，有 <span class="math display">\[P\left( \bigcap_{j \in J} A_j \right) = \prod_{j \in J} P(A_j).\]</span> 随机变量 <span class="math inline">\(X: \Omega \to\mathbb{R}^n\)</span> 和 <span class="math inline">\(Y: \Omega \to\mathbb{R}^m\)</span> 称为独立的，如果它们引导的 <spanclass="math inline">\(\sigma\)</span>-代数 <spanclass="math inline">\(\sigma(X)\)</span> 和 <spanclass="math inline">\(\sigma(Y)\)</span>独立。我们也同时考虑一个随机变量 <span class="math inline">\(X\)</span>和一个 <span class="math inline">\(\sigma\)</span>-代数 <spanclass="math inline">\(\mathcal{G} \subseteq \mathcal{F}\)</span>的独立性，定义为 <span class="math inline">\(\sigma(X)\)</span> 和 <spanclass="math inline">\(\mathcal{G}\)</span> 独立。</p><p>容易验证，若 <span class="math inline">\(X\)</span> 和 <spanclass="math inline">\(Y\)</span> 独立，则 <spanclass="math inline">\(E[XY] =E[X]E[Y]\)</span>。证明是考虑简单函数（随机变量）。</p><p>设 <span class="math inline">\((\Omega, \mathcal{F}, P)\)</span>是一个概率空间，<span class="math inline">\(\mathcal{G} \subseteq\mathcal{F}\)</span> 是 <spanclass="math inline">\(\sigma\)</span>-代数，<spanclass="math inline">\(X: \Omega \to \mathbb{R}^n\)</span>是一个随机变量，且 <span class="math inline">\(E[\|X\|] &lt;\infty\)</span>。则条件期望 <span class="math inline">\(E[X |\mathcal{G}]\)</span> 定义为满足以下性质的 <spanclass="math inline">\(\mathcal{G}\)</span>-可测随机变量： 1. <spanclass="math inline">\(E[X | \mathcal{G}]\)</span> 是 <spanclass="math inline">\(\mathcal{G}\)</span>-可测的； 2. 对于所有 <spanclass="math inline">\(A \in \mathcal{G}\)</span>，有 <spanclass="math inline">\(E[X \mathbf{1}_A] = E[E[X | \mathcal{G}]\mathbf{1}_A]\)</span>。换言之， <span class="math display">\[\int_{A} X(\omega) dP(\omega) = \int_{A} E[X | \mathcal{G}](\omega)dP(\omega).\]</span> 条件期望的存在性和唯一性（几乎处处相等意义下）可以通过Radon-Nikodym 定理来证明。</p><p>条件期望的一些重要性质包括： 1. 线性：对于任意的随机变量 <spanclass="math inline">\(X, Y\)</span> 和标量 <spanclass="math inline">\(a, b \in \mathbb{R}\)</span>，有 <spanclass="math display">\[E[aX + bY | \mathcal{G}] = aE[X | \mathcal{G}] + bE[Y | \mathcal{G}].\]</span> 2. 全期望公式：<span class="math inline">\(E[E[X |\mathcal{G}]] = E[X]\)</span>。 3. 如果 <spanclass="math inline">\(X\)</span> 是 <spanclass="math inline">\(\mathcal{G}\)</span>-可测的，则 <spanclass="math inline">\(E[X | \mathcal{G}] = X\)</span> 几乎处处成立。 4.如果 <span class="math inline">\(X\)</span> 独立于 <spanclass="math inline">\(\mathcal{G}\)</span>，则 <spanclass="math inline">\(E[X | \mathcal{G}] = E[X]\)</span>几乎处处成立。</p><h3 id="随机过程">随机过程</h3><p>一个随机过程是定义在概率空间 <span class="math inline">\((\Omega,\mathcal{F}, P)\)</span> 上的随机变量族 <spanclass="math inline">\(\{X_t : t \in T\}\)</span>，其中 <spanclass="math inline">\(T\)</span> 是一个索引集，通常取为时间参数（如<span class="math inline">\(T = [0, \infty)\)</span> 或 <spanclass="math inline">\(T = \mathbb{N}\)</span>）。对于每个固定的 <spanclass="math inline">\(\omega \in \Omega\)</span>，函数 <spanclass="math inline">\(t \mapsto X_t(\omega)\)</span>称为该随机过程的一个样本路径。一个随机过程也可以被视为一个映射 <spanclass="math inline">\(X: \Omega \times T \to\mathbb{R}^n\)</span>，满足对于每个固定的 <span class="math inline">\(t\in T\)</span>，<span class="math inline">\(X_t(\cdot)\)</span>是一个随机变量（<spanclass="math inline">\(\mathcal{F}\)</span>-可测的）。这样考虑是因为我们经常需要<span class="math inline">\(X\)</span> 是联合可测的，即 <spanclass="math inline">\(X\)</span> 作为 <span class="math inline">\(\Omega\times T\)</span> 上的函数是可测的。</p><blockquote><p>Q: 一个联合可测的随机过程，和一个只对 <spanclass="math inline">\(t\)</span>可测的随机过程，有什么区别？和一个只有<spanclass="math inline">\(X_t\)</span> 可测的随机过程，有什么区别？</p><p>A: 对 <span class="math inline">\(t\)</span>可测的随机过程，意味着对于每个固定的 <spanclass="math inline">\(\omega\)</span>，函数 <spanclass="math inline">\(t \mapsto X_t(\omega)\)</span>是可测的。但是，对于联合可测的随机过程，我们可以应用 Fubini 定理，这说明<span class="math inline">\(I(\omega) = \int_{[0,r]} X_t(\omega)dt\)</span> 也是一个随机变量（<spanclass="math inline">\(\mathcal{F}\)</span>-可测的）。而仅仅对 <spanclass="math inline">\(t\)</span> 可测的随机过程，不能保证这一点。</p></blockquote><p>我们可以将 <span class="math inline">\(\omega\)</span> 视同于路径<span class="math inline">\(t \mapsto X_t(\omega)\)</span>，从而将 <spanclass="math inline">\(\Omega\)</span> 视作所有从 <spanclass="math inline">\(T\)</span> 到 <span class="math inline">\(\mathbbR^n\)</span> 的函数空间 <span class="math inline">\(\tilde \Omega =(\mathbb R^n)^T\)</span> 上的一个子集。</p><p>定义在 <span class="math inline">\(\tilde \Omega\)</span> 上的自然<span class="math inline">\(\sigma\)</span>-代数为 <spanclass="math inline">\(\tilde{\mathcal{F}}\)</span>，由所有形如 <spanclass="math inline">\(\mathbb W=\{\tilde \omega \in \tilde \Omega :\tilde \omega(t_1) \in B_1, \ldots, \tilde \omega(t_k) \inB_k\}\)</span> 的集合生成，其中 <span class="math inline">\(t_i \inT\)</span>，<span class="math inline">\(B_i \in\mathcal{B}(\mathbb{R}^n)\)</span>。称为柱集（cylindersets）。这个代数记作 <span class="math inline">\(\mathcal{B}(\mathbbW)\)</span>。（注意这个实际上包含了可数点，这里的有限 <spanclass="math inline">\(k\)</span> 可以加强为 <spanclass="math inline">\(1\)</span>，但是实际应用中有限比较方便）</p><blockquote><p>对于连续函数空间，这个 <spanclass="math inline">\(\sigma\)</span>-代数恰好等于由一致收敛拓扑诱导的Borel <spanclass="math inline">\(\sigma\)</span>-代数。（因为连续函数的上下极限可以只考虑稠密可数的有理数点）</p></blockquote><p>对于随机过程 <span class="math inline">\(X: \Omega \times T \to\mathbb{R}^n\)</span>，我们可以定义映射 <span class="math inline">\(\pi:\Omega \to \tilde \Omega\)</span>，使得对于每个 <spanclass="math inline">\(\omega \in \Omega\)</span>，<spanclass="math inline">\(\pi(\omega)\)</span> 是路径 <spanclass="math inline">\(t \mapsto X_t(\omega)\)</span>。然后，我们可以通过<span class="math inline">\(\pi\)</span> 将概率测度 <spanclass="math inline">\(P\)</span> 从 <spanclass="math inline">\(\Omega\)</span> 推到 <spanclass="math inline">\(\tilde \Omega\)</span> 上，得到测度 <spanclass="math inline">\(\tilde P\)</span>，定义为对于所有 <spanclass="math inline">\(A \in \tilde{\mathcal{F}}\)</span>， <spanclass="math display">\[\tilde P(A) = P(\pi^{-1}(A)).\]</span></p><p>这样，我们就得到了一个新的概率空间 <spanclass="math inline">\((\tilde \Omega, \tilde{\mathcal{F}}, \tildeP)\)</span>，其中 <spanclass="math inline">\(\tilde{\mathcal{F}}\)</span> 是 <spanclass="math inline">\(\tilde \Omega\)</span> 上的 <spanclass="math inline">\(\sigma\)</span>-代数，<spanclass="math inline">\(\tilde P\)</span> 是定义在 <spanclass="math inline">\(\tilde{\mathcal{F}}\)</span> 上的概率测度。</p><p>这样，随机过程 <span class="math inline">\(X\)</span>可以被视为定义在路径空间 <span class="math inline">\((\tilde \Omega =(\mathbb R^n)^{T}, \tilde{\mathcal{F}} = \mathcal{B}(\mathbb W), \tildeP)\)</span> 上的路径概率测度 <span class="math inline">\(\tildeP\)</span>。</p><p>现在考虑从有限观测点出发，构造随机过程。</p><h4 id="kolmogorov-扩展定理kolmogorov-extension-theorem">Kolmogorov扩展定理（Kolmogorov Extension Theorem）</h4><p>观测 <span class="math inline">\(k\in \mathbb{N}\)</span>次，假定存在 <span class="math inline">\(\nu_{t_1, t_2, \ldots,t_k}\)</span> 是 <span class="math inline">\(\mathbb{R}^{nk}\)</span>上的概率测度。</p><p>满足一致性条件：对于 <span class="math inline">\(\forall k \in\mathbb{N},t_1, t_2, \ldots, t_k \in T\)</span>，以及任意的 Borel 集合<span class="math inline">\(B_1, B_2, \ldots, B_k \in\mathcal{B}(\mathbb{R}^n)\)</span>，有 <span class="math display">\[\nu_{t_1, t_2, \ldots, t_k}(B_1 \times B_2 \times \ldots \times B_k) =\nu_{t_1, t_2, \ldots, t_k, t_{k+1}}(B_1 \times B_2 \times \ldots \timesB_k \times \mathbb{R}^n).\]</span></p><p>那么存在 一个随机过程 <spanclass="math inline">\(\{X_t\}\)</span>，定义在某个概率空间 <spanclass="math inline">\((\Omega, \mathcal{F}, P)\)</span>上，使得对于任意的 <span class="math inline">\(k \in \mathbb{N}\)</span>和 <span class="math inline">\(t_1, t_2, \ldots, t_k \inT\)</span>，随机变量 <span class="math inline">\((X_{t_1}, X_{t_2},\ldots, X_{t_k})\)</span> 的分布为 <span class="math inline">\(\nu_{t_1,t_2, \ldots, t_k}\)</span>。换言之，对任意 Borel 集合 <spanclass="math inline">\(B_1, B_2, \ldots, B_k \in\mathcal{B}(\mathbb{R}^n)\)</span>，有 <span class="math display">\[P(X_{t_1} \in B_1, X_{t_2} \in B_2, \ldots, X_{t_k} \in B_k) = \nu_{t_1,t_2, \ldots, t_k}(B_1 \times B_2 \times \ldots \times B_k).\]</span></p><p>注意，这个定理并没有保证样本路径的正则性（如连续性或可测性）。它仅仅保证了存在一个随机过程，其有限维分布与给定的一致性条件相符。也就是说目前的<span class="math inline">\(\Omega\)</span> 只能视作 <spanclass="math inline">\((\mathbb R^n)^T\)</span>。</p><p>为了得到具有更好路径性质的随机过程，我们需要以下定理。</p><h4 id="kolmogorov-连续性定理kolmogorov-continuity-theorem">Kolmogorov连续性定理（Kolmogorov Continuity Theorem）</h4><p>设 <span class="math inline">\(\{X_t : t \in [0, T]\}\)</span>是定义在概率空间 <span class="math inline">\((\Omega, \mathcal{F},P)\)</span> 上的随机过程。假设存在常数 <spanclass="math inline">\(\alpha, \beta, C &gt; 0\)</span>，使得对于所有的<span class="math inline">\(s, t \in [0, T]\)</span>，有 <spanclass="math display">\[E[\|X_t - X_s\|^\alpha] \leq C |t - s|^{1 + \beta}.\]</span> 那么存在一个修改版本 <span class="math inline">\(\{\tilde{X}_t: t \in [0, T]\}\)</span>，使得对于几乎所有的 <spanclass="math inline">\(\omega \in \Omega\)</span>，路径 <spanclass="math inline">\(t \mapsto \tilde{X}_t(\omega)\)</span> 是 Hölder连续的，指数为 <span class="math inline">\(\gamma\)</span>，其中 <spanclass="math inline">\(0 &lt; \gamma &lt;\frac{\beta}{\alpha}\)</span>。也就是说，存在随机变量 <spanclass="math inline">\(K(\omega)\)</span>，使得对于所有的 <spanclass="math inline">\(s, t \in [0, T]\)</span>，有 <spanclass="math display">\[\|\tilde{X}_t(\omega) - \tilde{X}_s(\omega)\| \leq K(\omega) |t -s|^{\gamma}.\]</span></p><p>证明：</p><p>考虑所有二进位网格点 <span class="math inline">\(\mathcal{D}_n = \{\frac{k}{2^n} : k = 0, 1, \dots, 2^n \}\)</span> 上的随机变量 <spanclass="math inline">\(X_t\)</span>。利用假设的矩不等式与切比雪夫不等式<span class="math display">\[P\left( |X_{\frac{k}{2^n}} - X_{\frac{k-1}{2^n}}| &gt; \epsilon \right)\le \frac{E[|X_{\frac{k}{2^n}} -X_{\frac{k-1}{2^n}}|^\alpha]}{\epsilon^\alpha} \le \frac{C(2^{-n})^{1+\beta}}{\epsilon^\alpha}\]</span> <span class="math inline">\(M_n = \max_{1 \le k \le 2^n}|X_{\frac{k}{2^n}} - X_{\frac{k-1}{2^n}}|\)</span>, 则 <spanclass="math display">\[P(M_n &gt; \epsilon) \le \sum_{k=1}^{2^n} P\left( |X_{\frac{k}{2^n}} -X_{\frac{k-1}{2^n}}| &gt; \epsilon \right) \le \frac{C2^{-n\beta}}{\epsilon^\alpha}\]</span> 取 <span class="math inline">\(\epsilon =2^{-n\gamma}\)</span>，其中 <span class="math inline">\(0 &lt; \gamma&lt; \frac{\beta}{\alpha}\)</span>，则 <span class="math display">\[P(M_n &gt; 2^{-n\gamma}) \le C 2^{n(\alpha \gamma - \beta)}\]</span> 由于 <span class="math inline">\(\alpha \gamma - \beta &lt;0\)</span>，级数 <span class="math inline">\(\sum_{n=1}^{\infty} P(M_n&gt; 2^{-n\gamma})\)</span> 收敛。由 Borel-Cantelli引理，几乎处处存在随机变量 <spanclass="math inline">\(N(\omega)\)</span>，使得对于所有的 <spanclass="math inline">\(n \geq N(\omega)\)</span>， 有 <spanclass="math inline">\(M_n(\omega) \leq 2^{-n\gamma}\)</span>。因此，对于几乎所有的 <span class="math inline">\(\omega \in\Omega\)</span>，存在 <spanclass="math inline">\(N(\omega)\)</span>，使得对于所有的 <spanclass="math inline">\(n \geq N(\omega)\)</span> 和 <spanclass="math inline">\(k = 1, 2, \ldots, 2^n\)</span>，有 <spanclass="math display">\[|X_{\frac{k}{2^n}}(\omega) - X_{\frac{k-1}{2^n}}(\omega)| \leq2^{-n\gamma}.\]</span> 这表明对于几乎所有的 <span class="math inline">\(\omega \in\Omega\)</span>，路径 <span class="math inline">\(t \mapstoX_t(\omega)\)</span> 在二进位网格点上是 Hölder 连续的。我们可以将这种Hölder 连续性扩展到整个区间 <span class="math inline">\([0,T]\)</span>，从而得到修改版本 <spanclass="math inline">\(\tilde{X}_t\)</span>，使得对于几乎所有的 <spanclass="math inline">\(\omega \in \Omega\)</span>，路径 <spanclass="math inline">\(t \mapsto \tilde{X}_t(\omega)\)</span> 是 <spanclass="math inline">\(\gamma\)</span>-Hölder 连续的。</p><p>所以对于矩被控的随机过程，我们总能找到一个修改版本，使得路径具有Hölder 连续性。</p><h3 id="布朗运动-brownian-motion">布朗运动 （Brownian Motion）</h3><p>考虑这样的一个过程，在任何时刻 <spanclass="math inline">\(t_1\)</span> 观察，在 <spanclass="math inline">\(t_2\)</span> 时候的概率分布只与经过的时间 <spanclass="math inline">\(t_2 - t_1\)</span> 与位移有关。 <spanclass="math display">\[p(t,x,y) = (2\pi t)^{-n/2}\cdot \exp(-\frac{|x-y|^2}{2t}) \forall y\in\mathbb R^n, t &gt; 0\]</span> 给定 <span class="math inline">\(k\)</span> 个观测点, <spanclass="math inline">\(0\leq t_1 \leq t_2 \leq \cdots \leqt_k\)</span>，定义这样的概率度量为 <spanclass="math inline">\(\nu_{t_1,\ldots, t_k}\)</span> 在 <spanclass="math inline">\(\mathbb R^{nk}\)</span> 为 <spanclass="math display">\[v_{t_1,\ldots, t_k}(F_1\times\cdots \times F_k) = \int_{F_1\times \cdots\times F_k} p(t_1, x, x_1) p(t_2-t_1,x_1,x_2) \cdots p(t_k-t_{k-1},x_{k-1}, x_k) d x_1 \cdots d x_k\]</span> 并且规定 <span class="math inline">\(p(0,x,y) dy =\delta_x(y)\)</span>，其中 <span class="math inline">\(dy=dy_1\cdotsdy_k\)</span> 是 Lebesgue 测度。 这些测度满足一致性条件，因此由Kolmogorov 扩展定理，存在一个概率空间 <spanclass="math inline">\((\Omega, \mathcal{F}, P^{x})\)</span> 和随机过程<span class="math inline">\(\{B_t : t \geq 0\}\)</span>，使得对于任意的<span class="math inline">\(k \in \mathbb{N}\)</span> 和 <spanclass="math inline">\(0 \leq t_1 \leq t_2 \leq \cdots \leqt_k\)</span>，随机变量 <span class="math inline">\((B_{t_1}, B_{t_2},\ldots, B_{t_k})\)</span> 的分布为 <span class="math inline">\(\nu_{t_1,t_2, \ldots, t_k}\)</span>。 这个随机过程称为从点 <spanclass="math inline">\(x\)</span> 开始的布朗运动（Brownian Motion），记作<span class="math inline">\(B_t^x\)</span> 或简称 <spanclass="math inline">\(B_t\)</span>。</p><p>根据 Kolmogorov连续性定理，布朗运动存在一个修改版本，使得对于几乎所有的 <spanclass="math inline">\(\omega \in \Omega\)</span>，路径 <spanclass="math inline">\(t \mapsto B_t(\omega)\)</span> 是 Hölder连续的，指数为 <span class="math inline">\(\gamma\)</span>，其中 <spanclass="math inline">\(0 &lt; \gamma &lt;\frac{1}{2}\)</span>。故可以视为 <spanclass="math inline">\(C([0,\infty);\mathbb R^n)\)</span>上的概率测度。</p><p>布朗运动具有以下重要性质： 1. 是高斯过程：对于任意的 <spanclass="math inline">\(k \in \mathbb{N}\)</span> 和 <spanclass="math inline">\(0 \leq t_1 &lt; t_2 &lt; \cdots &lt;t_k\)</span>，随机变量 <span class="math inline">\(Z=(B_{t_1}, B_{t_2},\ldots, B_{t_k})\)</span> 服从多元正态分布。 <spanclass="math display">\[\mathbb E^{x} \left[\exp\left(i \sum_j^{nk} u_j Z_j\right)\right] =\exp\left(i \sum_j^{nk} u_j M_j - \frac{1}{2} \sum_{j,l}^{nk} u_j c_{jl}u_l \right)\]</span></p><p>其中 <span class="math display">\[M = [x, x, \ldots, x]^T \in \mathbb R^{nk}\]</span> <span class="math display">\[C = \begin{bmatrix}t_1 I_n &amp; t_1 I_n &amp; \cdots &amp; t_1 I_n \\t_1 I_n &amp; t_2 I_n &amp; \cdots &amp; t_2 I_n \\\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\t_1 I_n &amp; t_2 I_n &amp; \cdots &amp; t_k I_n\end{bmatrix} \in \mathbb R^{nk \times nk}\]</span></p><p>我们有 <span class="math inline">\(\mathbb E^{x}[(B_t - x)^2] = nt\)</span>。即布朗运动的方差与时间成正比； <spanclass="math inline">\(\mathbb E^{x}[(B_t - x) (B_s - x)] = n\min(t,s)\)</span>。即布朗运动的协方差与时间的最小值成正比。</p><p>因此，<span class="math inline">\(\mathbb E^{x}[ (B_t - B_s)^2] = n(t-s), \forall 0 \leq s &lt; t\)</span>。</p><ol start="2" type="1"><li><p>具有独立增量：对于任意的 <span class="math inline">\(0 \leq t_1&lt; t_2 &lt; \cdots &lt; t_k\)</span>，增量 <spanclass="math inline">\(B_{t_2} - B_{t_1}, B_{t_3} - B_{t_2}, \ldots,B_{t_k} - B_{t_{k-1}}\)</span> 是相互独立的随机变量。</p></li><li><p>几乎处处连续路径：布朗运动存在一个修改版本，使得对于几乎所有的<span class="math inline">\(\omega \in \Omega\)</span>，路径 <spanclass="math inline">\(t \mapsto B_t(\omega)\)</span> 是连续的。</p></li><li><p>几乎处处不可微：对于几乎所有的 <span class="math inline">\(\omega\in \Omega\)</span>，路径 <span class="math inline">\(t \mapstoB_t(\omega)\)</span> 在任意时刻 <span class="math inline">\(t\)</span>都不可微。</p></li></ol><p>1, 2, 3 是直接由构造和 Kolmogorov 连续性定理得到的。</p><p>4 的详细证明这里略过，对于单点来说，考虑增量商 <spanclass="math inline">\(\geq M\)</span>的概率，可以证明几乎处处存在一个无穷序列，使得增量商 <spanclass="math inline">\(\geqM\)</span>，从而不可微。但是对于整个区间不存在可微点的概率，需要更复杂的论证。</p><h3 id="一些重要的概率论定理">一些重要的概率论定理</h3><h4 id="pi-lambda-定理"><span class="math inline">\(\pi\)</span>-<spanclass="math inline">\(\lambda\)</span> 定理</h4><p>设 <span class="math inline">\(\mathcal{P}\)</span> 是 <spanclass="math inline">\(\Omega\)</span> 上的一个 <spanclass="math inline">\(\pi\)</span>-系（即对于任意的 <spanclass="math inline">\(A, B \in \mathcal{P}\)</span>，有 <spanclass="math inline">\(A \cap B \in \mathcal{P}\)</span>），<spanclass="math inline">\(\mathcal{L}\)</span> 是包含 <spanclass="math inline">\(\Omega\)</span> 的一个 <spanclass="math inline">\(\lambda\)</span>-系（即满足包含全集、补集封闭性、可列不交并封闭性）。如果<span class="math inline">\(\mathcal{P} \subseteq\mathcal{L}\)</span>，则由 <spanclass="math inline">\(\mathcal{P}\)</span> 生成的 <spanclass="math inline">\(\sigma\)</span>-代数 <spanclass="math inline">\(\sigma(\mathcal{P})\)</span> 包含在 <spanclass="math inline">\(\mathcal{L}\)</span> 中，即 <spanclass="math inline">\(\sigma(\mathcal{P}) \subseteq\mathcal{L}\)</span>。</p><p>证明： 定义 <span class="math inline">\(\mathcal{L}&#39; = \{ A \in\sigma(\mathcal{P}) : A \in \mathcal{L} \}\)</span>。显然，<spanclass="math inline">\(\mathcal{L}&#39;\)</span> 是一个 <spanclass="math inline">\(\lambda\)</span>-系，并且包含 <spanclass="math inline">\(\mathcal{P}\)</span>。因此，由于 <spanclass="math inline">\(\sigma(\mathcal{P})\)</span> 是由 <spanclass="math inline">\(\mathcal{P}\)</span> 生成的最小的 <spanclass="math inline">\(\sigma\)</span>-代数，必有 <spanclass="math inline">\(\sigma(\mathcal{P}) \subseteq\mathcal{L}&#39;\)</span>。换言之，<spanclass="math inline">\(\sigma(\mathcal{P}) \subseteq\mathcal{L}\)</span>。</p><blockquote><p>特别地，如果一个集合族又是 <spanclass="math inline">\(\pi\)</span>-系 又是 <spanclass="math inline">\(\lambda\)</span>-系，则它是一个 <spanclass="math inline">\(\sigma\)</span>-代数。</p></blockquote><h4 id="borel-cantelli-引理">Borel-Cantelli 引理</h4><p>设 <span class="math inline">\(\{A_n\}_{n=1}^{\infty}\)</span>是概率空间 <span class="math inline">\((\Omega, \mathcal{F}, P)\)</span>上的一列事件。</p><p><span class="math inline">\(\limsup_{n \to \infty} A_n =\bigcap_{m=1}^{\infty} \bigcup_{n=m}^{\infty} A_n\)</span> 表示事件<span class="math inline">\(A_n\)</span>无限次发生的事件（无论多远都能找到某个 <spanclass="math inline">\(A_n\)</span> 发生）。</p><p><span class="math inline">\(\liminf_{n \to \infty} A_n =\bigcup_{m=1}^{\infty} \bigcap_{n=m}^{\infty} A_n\)</span> 表示事件<span class="math inline">\(A_n\)</span>最终总是发生的事件（从某个时刻开始，所有的 <spanclass="math inline">\(A_n\)</span> 都发生）。</p><p>Borel-Cantelli 引理说 <span class="math display">\[\sum_{n=1}^{\infty} P(A_n) &lt; \infty \Rightarrow P\left( \limsup_{n\to \infty} A_n \right) = 0.\\\]</span> <span class="math display">\[\text{如果 } \{A_n\} \text{ 相互独立且 } \sum_{n=1}^{\infty} P(A_n) =\infty\Rightarrow P\left( \limsup_{n \to \infty} A_n \right) = 1.\]</span> 证明： <span class="math display">\[P\left( \limsup_{n \to \infty} A_n \right) = P\left(\bigcap_{m=1}^{\infty} \bigcup_{n=m}^{\infty} A_n \right) = \lim_{m \to\infty} P\left( \bigcup_{n=m}^{\infty} A_n \right) \leq \lim_{m \to\infty} \sum_{n=m}^{\infty} P(A_n) = 0.\]</span> <span class="math display">\[\begin{aligned}P\left( \limsup_{n \to \infty} A_n \right) &amp;= 1 - P\left( \liminf_{n\to \infty} A_n^c \right) \\&amp;= 1 - \lim_{m \to \infty} P\left( \bigcap_{n=m}^{\infty} A_n^c\right)\\&amp;= 1 - \lim_{m \to \infty} \prod_{n=m}^{\infty} (1 - P(A_n)) \\&amp;\geq 1 - \lim_{m \to \infty} \exp\left( -\sum_{n=m}^{\infty} P(A_n)\right) = 1.\end{aligned}\]</span></p><h4 id="kolmogorov-0-1-定理">Kolmogorov 0-1 定理</h4><p>设 <span class="math inline">\(\{X_n\}_{n=1}^{\infty}\)</span>是定义在概率空间 <span class="math inline">\((\Omega, \mathcal{F},P)\)</span> 上的一列独立随机变量。定义尾 <spanclass="math inline">\(\sigma\)</span>-代数为 <spanclass="math display">\[\mathcal{T} = \bigcap_{n=1}^{\infty} \sigma(X_n, X_{n+1}, \ldots).\]</span> 那么对于任意的事件 <span class="math inline">\(A \in\mathcal{T}\)</span>，有 <span class="math inline">\(P(A) \in \{0,1\}\)</span>。</p><p>证明： 设 <span class="math inline">\(A \in\mathcal{T}\)</span>，则对于任意的 <span class="math inline">\(n \in\mathbb{N}\)</span>，<span class="math inline">\(A \in \sigma(X_n,X_{n+1}, \ldots)\)</span>。由于 <spanclass="math inline">\(\{X_n\}\)</span> 是独立的，<spanclass="math inline">\(\sigma(X_1, X_2, \ldots, X_{n-1})\)</span> 与<span class="math inline">\(\sigma(X_n, X_{n+1}, \ldots)\)</span>独立。因此，<span class="math inline">\(A\)</span> 与 <spanclass="math inline">\(\sigma(X_1, X_2, \ldots, X_{n-1})\)</span>独立。由于 <span class="math inline">\(n\)</span> 是任意的，<spanclass="math inline">\(A\)</span> 与 <spanclass="math inline">\(\mathcal{L}=\bigcup_{n=1}^{\infty} \sigma(X_1,X_2, \ldots, X_{n-1})\)</span> 独立。注意到 <spanclass="math inline">\(\mathcal{L}\)</span> 是一个 <spanclass="math inline">\(\pi\)</span>-系，而 <spanclass="math inline">\(\sigma(\mathcal{L}) = \mathcal{F}\)</span>，因此由<span class="math inline">\(\pi\)</span>-<spanclass="math inline">\(\lambda\)</span> 定理，<spanclass="math inline">\(A\)</span> 与 <spanclass="math inline">\(\mathcal{F}\)</span> 独立。特别地，<spanclass="math inline">\(A\)</span> 与自身独立，因此 <spanclass="math display">\[P(A) = P(A \cap A) = P(A) P(A) \Rightarrow P(A) \in \{0, 1\}.\]</span></p><h2 id="ito-积分ito-引理-和-ito-公式-chapter-3-4-of-øksendal">2. Ito积分，Ito 引理 和 Ito 公式 (Chapter 3, 4 of Øksendal)</h2><h3 id="ito-积分">Ito 积分</h3><p>首先我们要问，什么叫做随机积分？我们希望定义类似于 Riemann/Lebesgue积分的东西，但是积分的变元是随机过程。这样的过程是因为现实中很多现象都具有随机性，比如股票价格的变化，粒子的无规则运动等。</p><p>形式化地说，假如我们有一个随机过程 <spanclass="math inline">\(X_t\)</span> 和布朗运动 <spanclass="math inline">\(B_t\)</span>，我们希望定义积分 <spanclass="math display">\[\int_S^T X_t dB_t\]</span> 这里的 <span class="math inline">\(dB_t\)</span>“表示布朗运动的增量”。</p><p>回忆一下 Riemann 积分的定义，是将区间 <spanclass="math inline">\([S,T]\)</span>划分为小区间，然后在每个小区间上取一个点，计算函数值与区间长度的乘积之和，最后取极限。或者等价地，用上方控制的简单函数与下方控制的简单函数来逼近，然后两边取极限。</p><p>类似地，我们可以尝试定义随机积分为 <span class="math display">\[\sum_{j} f(t_j^*,\omega) (B_{t_{j+1}} - B_{t_j})(\omega)\]</span> 其中 <span class="math inline">\(\{t_j\}\)</span> 是 <spanclass="math inline">\([S,T]\)</span> 的一个划分，<spanclass="math inline">\(t_j^* \in [t_j, t_{j+1}]\)</span>是每个小区间内的某一个点。</p><p>我们现在考虑 <span class="math inline">\(X_t = B_t\)</span>的情况，即积分 <span class="math display">\[\int_0^T B_t dB_t\]</span> 我们尝试用上面的和式来定义这个积分。取一个划分 <spanclass="math inline">\(S=0 = t_0 &lt; t_1 &lt; \cdots &lt; t_n =T\)</span>，则 <span class="math display">\[\begin{aligned}\sum_{j=0}^{n-1} B_{t_j} (B_{t_{j+1}} - B_{t_j}) &amp;= \frac{1}{2}\sum_{j=0}^{n-1} (B_{t_{j+1}}^2 - B_{t_j}^2) - \frac{1}{2}\sum_{j=0}^{n-1} (B_{t_{j+1}} - B_{t_j})^2 \\= &amp; \frac{1}{2} (B_T^2 - B_0^2) - \frac{1}{2} \sum_{j=0}^{n-1}(B_{t_{j+1}} - B_{t_j})^2\end{aligned}\]</span> 而 <span class="math display">\[\mathbb E\left[\sum_{j=0}^{n-1} (B_{t_{j+1}} - B_{t_j})^2\right]  =\sum_{j=0}^{n-1} (t_{j+1} - t_j) = T\]</span></p><p>而考虑求和 <span class="math display">\[\begin{aligned}\sum_{j=0}^{n-1} B_{t_{j+1}} (B_{t_{j+1}} - B_{t_j}) &amp;= \frac{1}{2}\sum_{j=0}^{n-1} (B_{t_{j+1}}^2 - B_{t_j}^2) + \frac{1}{2}\sum_{j=0}^{n-1} (B_{t_{j+1}} - B_{t_j})^2 \\&amp;= \frac{1}{2} (B_T^2 - B_0^2) + \frac{1}{2} \sum_{j=0}^{n-1}(B_{t_{j+1}} - B_{t_j})^2\end{aligned}\]</span> 这说明，随机积分的结果与取点的位置有关。</p><p>这里的变化，正是因为布朗运动的增量 <spanclass="math inline">\((B_{t_{j+1}} - B_{t_j})\)</span>的平方的期望与区间长度成正比，而不是像确定性函数那样趋近于零。</p><p>对于 Ito 积分，我们选择在每个小区间的左端点取值，即定义为 <spanclass="math display">\[\int_S^T X_t dB_t := \lim_{|\Delta| \to 0} \sum_{j} X_{t_j} (B_{t_{j+1}}- B_{t_j})\]</span> 令最大区间长度 <span class="math inline">\(|\Delta| = \max_j(t_{j+1} - t_j)\)</span> 趋近于零。</p><p>顺便，我们也可以定义 Stratonovich积分，其选择在每个小区间的中点取值，即定义为 <spanclass="math display">\[\int_S^T X_t \circ dB_t := \lim_{|\Delta| \to 0} \sum_{j} X_{\frac{t_j +t_{j+1}}{2}} (B_{t_{j+1}} - B_{t_j})\]</span></p><p>那么现在我们就可以保证我们的积分在 <spanclass="math inline">\(L^2\)</span> 意义下收敛到一个随机变量了吗？为了考察这个问题，我们先考虑简单函数的情况。</p><p>我们称随机过程 <span class="math inline">\(X_t\)</span>是一个简单过程，如果它可以表示为 <span class="math display">\[X_t(\omega) = \sum_{j} e_j(\omega) \mathbf{1}_{[t_j, t_{j+1})}(t)\]</span> 其中 <span class="math inline">\(\{t_j\}\)</span> 是 <spanclass="math inline">\([S,T]\)</span> 的一个划分。</p><p>显然，对于简单过程 <spanclass="math inline">\(X_t\)</span>，我们可以定义 Ito 积分为 <spanclass="math display">\[\int_S^T X_t dB_t := \sum_{j} e_j (B_{t_{j+1}} - B_{t_j})\]</span></p><p>然后我们考虑在 <span class="math inline">\(L^2\)</span>意义下的收敛性。即计算 <span class="math display">\[\begin{aligned}\mathbb E\left[\left(\int_S^T X_t dB_t\right)^2\right] &amp;= \mathbbE\left[\left(\sum_{j} e_j (B_{t_{j+1}} - B_{t_j})\right)^2\right] \\&amp; = \sum_{i,j} \mathbb E[e_i e_j \Delta B_i \Delta B_j] \\\end{aligned}\]</span></p><p>为了进一步计算，我们假设 <span class="math inline">\(e_j\)</span> 是<span class="math inline">\(\mathcal{F}_{t_j}\)</span>-可测的，其中<span class="math inline">\(\mathcal{F}_t\)</span> 是由 <spanclass="math inline">\(\{B_s : s \leq t\}\)</span> 生成的 <spanclass="math inline">\(\sigma\)</span>-代数（则<spanclass="math inline">\(\mathcal{F}_{s} \subseteq \mathcal{F}_{t}\)</span>对于 <span class="math inline">\(s &lt; t\)</span>）。</p><p>对于 <span class="math inline">\(i &lt; j\)</span>，有 <spanclass="math inline">\(e_i, e_j, \Delta B_i\)</span> 都是 <spanclass="math inline">\(\mathcal{F}_{t_j}\)</span>-可测的，而 <spanclass="math inline">\(\Delta B_j\)</span> 独立于 <spanclass="math inline">\(\mathcal{F}_{t_j}\)</span>，因此 <spanclass="math display">\[\begin{aligned}\mathbb E[e_i e_j \Delta B_i \Delta B_j] &amp;= \mathbb E\left[ \mathbbE[e_i e_j \Delta B_i \Delta B_j | \mathcal{F}_{t_j}] \right] \quad\text{（全期望公式）} \\&amp;= \mathbb E\left[ e_i e_j \Delta B_i \mathbb E[\Delta B_j |\mathcal{F}_{t_j}] \right] \quad \text{（条件期望的线性性质）} \\&amp;= 0 \quad \text{（} \mathbb E[\Delta B_j | \mathcal{F}_{t_j}] =\mathbb E[\Delta B_j] = 0 \text{）}\end{aligned}\]</span></p><p>类似地，对于 <span class="math inline">\(i &gt; j\)</span>，也有<span class="math inline">\(\mathbb E[e_i e_j \Delta B_i \Delta B_j] =0\)</span>。 因此，只有当 <span class="math inline">\(i = j\)</span>时，<span class="math inline">\(\mathbb E[e_i e_j \Delta B_i \DeltaB_j]\)</span> 才可能非零。此时， <span class="math display">\[\mathbb E[e_j^2 (\Delta B_j)^2] = \mathbb E[e_j^2] \mathbb E[(\DeltaB_j)^2] = \mathbb E[e_j^2] (t_{j+1} - t_j)\]</span> 综上所述，我们得到了 <span class="math display">\[\mathbb E\left[\left(\int_S^T X_t dB_t\right)^2\right] = \sum_{j}\mathbb E[e_j^2] (t_{j+1} - t_j) = \mathbb E\left[\int_S^T X_t^2dt\right]\]</span></p><p>因此，我们得到了 Ito 等距公式 （简单过程版本）： <spanclass="math display">\[\mathbb E\left[\left(\int_S^T X_t dB_t\right)^2\right] = \mathbbE\left[\int_S^T X_t^2 dt\right]\]</span></p><p>左边是一个 Ito 积分作为随机变量的 <spanclass="math inline">\(L^2\)</span> 范数，右边是一个随机过程被视作 <spanclass="math inline">\(\Omega \times [S,T]\)</span>上的概率测度/随机变量的 <span class="math inline">\(L^2\)</span>范数。因此叫做等距公式。</p><p>这个实际上就在告诉我们，<strong>我们在时间这一维积分后，整体的长度是不变的。</strong></p><p>现在我们将它推广到较为一般的过程并给出正式定义。</p><p>令 <span class="math inline">\(\{\mathcal{N}_t\}\)</span>为一族递增的 <spanclass="math inline">\(\sigma\)</span>-代数，称为过滤（filtration）。随机过程<span class="math inline">\(X_t\)</span> 称为适应于过滤 <spanclass="math inline">\(\{\mathcal{N}_t\}\)</span> 的，如果对于每个 <spanclass="math inline">\(t\)</span>，<spanclass="math inline">\(X_t\)</span> 是 <spanclass="math inline">\(\mathcal{N}_t\)</span>-可测的。</p><p>令 <span class="math inline">\(\mathcal{F}_t\)</span> 为由布朗运动<span class="math inline">\(\{B_s : s \leq t\}\)</span>生成的自然过滤，即 <span class="math display">\[\mathcal{F}_t = \sigma(B_s : s \leq t) = \sigma(\omega; \omega(t_1)\inB_1, \ldots, \omega(t_k) \in B_k, \text{其中 } t_i \leq t, B_i \in\mathcal{B}(\mathbb{R}^n))\]</span> 这里我们假设 <spanclass="math inline">\(\mathcal{F}_t\)</span> 是完备化的，即包含所有的<span class="math inline">\(P\)</span>-零测集。</p><h4 id="逼近过程">逼近过程</h4><p>定义空间 <spanclass="math inline">\(\mathcal{V}=\mathcal{V}(S,T)\)</span>为所有适应于过滤 <span class="math inline">\(\{\mathcal{F}_t\}\)</span>的随机过程 <span class="math inline">\(X_t\)</span>（或函数 <spanclass="math inline">\(X: [S,T] \times \Omega \to\mathbb{R}\)</span>）且满足 1. <span class="math inline">\(X\)</span> 在<span class="math inline">\([S,T] \times \Omega\)</span> 上联合可测； 2.<span class="math inline">\(\int_S^T E[X_t^2] dt &lt;\infty\)</span>，即 <span class="math inline">\(X_t \in L^2([S,T] \times\Omega)\)</span>。</p><h5 id="step-1">Step 1</h5><p>首先，令 <span class="math inline">\(g \in \mathcal{V}\)</span>是有界的，并且 <span class="math inline">\(g\)</span> 对于每个 <spanclass="math inline">\(\omega\)</span> 是连续的。则存在一列简单过程 <spanclass="math inline">\(\{g_n\} \subset \mathcal{V}\)</span>，使得 <spanclass="math display">\[\lim_{n \to \infty} E\left[\int_S^T (g_n(t,\omega) - g(t,\omega))^2dt\right] = 0.\]</span></p><p>证明： 取 <span class="math inline">\(\phi_n = \sum_j g(t_j, \omega)\mathbf{1}_{[t_j, t_{j+1})}(t)\)</span>，其中 <spanclass="math inline">\(\{t_j\}\)</span> 是 <spanclass="math inline">\([S,T]\)</span> 的一个划分，且当 <spanclass="math inline">\(n \to \infty\)</span>，划分的间距趋近于零。显然有<span class="math inline">\(\phi_n \in \mathcal{V}\)</span>，并且由于<span class="math inline">\(g\)</span> 对于每个 <spanclass="math inline">\(\omega\)</span> 连续，因此 <spanclass="math inline">\(\phi_n(t,\omega)\)</span> 对于每个 <spanclass="math inline">\(\omega\)</span> 在 <spanclass="math inline">\([S,T]\)</span> 上逐点收敛于 <spanclass="math inline">\(g(t,\omega)\)</span>，且一致有界。</p><p>由有界收敛定理，我们有 <span class="math display">\[\lim_{n \to \infty} \mathbb E\left[\int_S^T (\phi_n(t,\omega) -g(t,\omega))^2 dt\right] = \mathbb E\left[\int_S^T \lim_{n \to \infty}(\phi_n(t,\omega) - g(t,\omega))^2 dt\right] = 0.\]</span> 因此，<span class="math inline">\(\{\phi_n\}\)</span>是所需的简单过程列。</p><h5 id="step-2">Step 2</h5><p>令 <span class="math inline">\(h\in \mathcal V\)</span>是有界的。则存在以上有界过程 <span class="math inline">\(g_n \in\mathcal V\)</span>，使得 <span class="math display">\[\lim_{n\to \infty} \mathbb E \left[\int_S^T(h(t,\omega)-g_n(t,\omega))^2 dt\right] = 0\]</span></p><p>证明：在实分析中，任取非负连续列 <spanclass="math inline">\(\{\phi_n(x)\}\)</span> 弱收敛到 <spanclass="math inline">\(\delta_0(x)\)</span>，令 <spanclass="math inline">\(g_n(t)=(\phi_n * h)(t)=\int_S^T h(s, \omega)\phi_n(s-t)\, ds\)</span> 作为卷积，易知有界连续且弱收敛到 <spanclass="math inline">\(h\)</span>。但是这里不可利用未来信息，所以取支撑在<span class="math inline">\(\mathbb R^+\)</span>的列即可。而 <spanclass="math inline">\(g_n(t, \cdot)\)</span>是 <spanclass="math inline">\(\mathcal{F_t}\)</span> 可测的，因为 <spanclass="math inline">\(F(s, \omega) = h(s, \omega) \phi_n(s-t)\)</span>是 <span class="math inline">\(\mathcal{B}([S,T]) \otimes\mathcal{F_t}\)</span> 上可测的，所以 <spanclass="math inline">\(\int_{S}^{T}F(s,\omega)\, ds\)</span> 根据 Fubini定理也是 <span class="math inline">\(\mathcal{F_t}\)</span>上可测的。</p><h5 id="step-3">Step 3</h5><p>令 <span class="math inline">\(f \in \mathcalV\)</span>。则存在以上有界过程 <span class="math inline">\(h_n \in\mathcal V\)</span>，<span class="math inline">\(h_n\)</span> 对每个<span class="math inline">\(n\)</span> 有界，而且 <spanclass="math display">\[\lim_{n\to \infty} \mathbb E\left[\int_S^T (f(t,\omega)-h_n(t,\omega))^2dt\right] = 0\]</span> 证明：令 <span class="math display">\[h_n =\begin{cases}-n &amp; \text{if  $f(t,\omega) &lt; -n$ } \\f(t,\omega) &amp; \text{if  $-n\leq f(t,\omega) \leq n$ } \\n &amp; \text{if  $f(t,\omega) &gt; n$ } \\\end{cases}\]</span></p><p>然后使用控制收敛定理交换极限和积分即可。</p><p>于是我们现在可以定义一个随机过程 <span class="math inline">\(f \in\mathcal V\)</span> 的 Ito 积分为</p><p><span class="math display">\[\mathcal{I}[f](\omega):=\int_S^T f(t,\omega) \, dB_t(\omega)=\lim_{n\to\infty} \int_S^T \phi_n(t,\omega)\, dB_t(\omega)\]</span></p><p>其中 <span class="math inline">\(\phi_n\)</span> 是简单函数列，满足<span class="math display">\[\lim_{n\to \infty} \mathbb E\left[\int_S^T |f-\phi_n|^2 \,dt \right] = 0\]</span> 由以上的证明保证存在性，并且由 Ito等距公式（简单过程版），我们知道 <span class="math display">\[\mathbb E\left[\int_S^T \phi_n(t,\omega)^2 \, dt \right] = \mathbbE\left[\left(\int_S^T \phi_n(t,\omega) \, dB_t(\omega)\right)^2\right]\]</span> 因此，<span class="math inline">\(\left\{\int_S^T\phi_n(t,\omega) \, dB_t(\omega)\right\}\)</span> 是 <spanclass="math inline">\(L^2(\Omega)\)</span> 中的 Cauchy 列，从而 <spanclass="math inline">\(\mathcal{I}[f](\omega)\)</span> 是良定义的。</p><h5 id="总结">总结</h5><ol type="1"><li><p>对于 <span class="math inline">\(f \in \mathcal V\)</span>，Ito积分 <span class="math inline">\(\int_S^T f(t,\omega) \,dB_t(\omega)\)</span> 是良定义的随机变量，且满足 Ito 等距公式 <spanclass="math display">\[\mathbb E\left[\left(\int_S^T f(t,\omega) \,dB_t(\omega)\right)^2\right] = \mathbb E\left[\int_S^T f(t,\omega)^2 \,dt\right]\]</span></p></li><li><p>对于 <span class="math inline">\(f \in \mathcal V\)</span> 和一列 <span class="math inline">\(\left\{f_n\right\} \subset \mathcalV\)</span>，如果 <span class="math display">\[\lim_{n\to \infty} \mathbb E\left[\int_S^T |f_n(t,\omega)-f(t,\omega)|^2\, dt\right] = 0\]</span> 则 <span class="math display">\[\int_S^T f_n(t,\omega) \, dB_t(\omega) \xrightarrow{L^2(\Omega)}\int_S^T f(t,\omega) \, dB_t(\omega)\]</span> 换句话说，如果随机过程列在 <spanclass="math inline">\(L^2([S,T] \times \Omega)\)</span>意义下收敛，则对应的 Ito 积分列在 <spanclass="math inline">\(L^2(\Omega)\)</span> 意义下收敛。 <strong>Ito积分是一个在两个 <span class="math inline">\(L^2\)</span>空间之间的连续线性映射。</strong></p></li></ol><p>以上逻辑为，第一，因为布朗运动的二次变差是时间的线性函数，所以在<span class="math inline">\(L^2\)</span>下，简单过程对布朗运动的积分是良定义并且等距的。第二、简单过程（在适应过程的）<span class="math inline">\(L^2\)</span>意义下是<strong>稠密</strong>的，所以可以推广到所有过程。</p><h4 id="ito积分的鞅性质">Ito积分的鞅性质</h4><p>设 <span class="math inline">\(f \in\mathcal{V}(S,T)\)</span>，我们容易知道 <span class="math display">\[\mathbb E\left[\int_S^T f\, dB_t\right] = 0\]</span> 因为对于简单过程，每一段增量的期望均是零。</p><p>更强的结果是，Ito积分得到的过程本身是一个鞅，也就是说如果我们只考虑事件点 <spanclass="math inline">\(t\)</span> 之前的信息 <spanclass="math inline">\(\mathcal{F}_t\)</span>，那么这个随机变量的期望是当前点的值。</p><p>具体地说，给定概率空间 <span class="math inline">\((\Omega,\mathcal{F}, P)\)</span> 上的一个滤过 <spanclass="math inline">\(\{\mathcal{F}_t\}\)</span>，以及一个适应于 <spanclass="math inline">\(\{\mathcal{F}_t\}\)</span> 的随机过程 <spanclass="math inline">\(M_t\)</span>，并且假设对于每个 <spanclass="math inline">\(t\)</span>，<span class="math inline">\(\mathbbE[|M_t|] &lt; \infty\)</span>。</p><p>如果对于所有的 <span class="math inline">\(s &lt; t\)</span>，都有<span class="math display">\[\mathbb E[M_t | \mathcal{F}_s] = M_s\]</span> 则称 <span class="math inline">\(M_t\)</span>是一个鞅（martingale）。</p><p>现在我们来证明 Ito 积分过程是一个鞅。</p><blockquote><p>证明： 首先取逼近列 <spanclass="math inline">\(\{f_n\}\)</span>，则对应的 Ito 积分过程为 <spanclass="math display">\[M_t^n = \int_S^t f_n(u,\omega) \,dB_u(\omega)\]</span> 对于 <span class="math inline">\(s &lt;t\)</span>，我们有 <span class="math display">\[\begin{aligned}\mathbb E[M_t^n | \mathcal{F}_s] &amp;= \mathbb E\left[\int_S^sf_n(u,\omega) \, dB_u(\omega) + \int_s^t f_n(u,\omega) \, dB_u(\omega)\bigg| \mathcal{F}_s\right] \\&amp;= \int_S^s f_n(u,\omega) \, dB_u(\omega) + \mathbb E\left[\int_s^tf_n(u,\omega) \, dB_u(\omega) \bigg| \mathcal{F}_s\right] \text{（因为左侧 $\mathcal{F}_s$ 可测）} \\&amp;= M_s^n + \mathbb E\left[\sum_{j} f_n(t_j,\omega) (B_{t_{j+1}} -B_{t_j}) \bigg| \mathcal{F}_s\right] \\&amp;= M_s^n + 0 \quad \text{（增量独立于 $\mathcal{F}_s$）} \\&amp;= M_s^n\end{aligned}\]</span></p><p>现在令 <span class="math inline">\(n \to \infty\)</span>，由于 Ito积分在 <span class="math inline">\(L^2(\Omega)\)</span>意义下连续，我们有 <span class="math display">\[\mathbb E[M_t | \mathcal{F}_s] = M_s\]</span> 因此，Ito 积分过程 <span class="math inline">\(M_t = \int_S^tf(u,\omega) \, dB_u(\omega)\)</span> 是一个鞅。</p></blockquote><p>但是我们首先注意到，目前我们只说明了，Ito积分的结果是一个鞅过程。但是，这个过程是否具有连续路径呢？是否能类似于在定义布朗运动时的情况，我们找一个修改版本，使得几乎对于所有的<span class="math inline">\(\omega\)</span>，路径 <spanclass="math inline">\(t \mapsto M_t(\omega)\)</span> 是连续的呢？</p><p>为了应用 Kolmogorov 连续性定理，我们需要估计增量的矩。也即 <spanclass="math display">\[\mathbb E[|M_t - M_s|^\alpha] \leq C |t - s|^{1 + \beta}.\]</span> 对于某个 <span class="math inline">\(\alpha &gt; 0\)</span>。利用 Ito 等距公式，我们有 <span class="math display">\[\mathbb E[|M_t - M_s|^2] = \mathbb E\left[\left(\int_s^t f(u,\omega) \,dB_u(\omega)\right)^2\right] = \mathbb E\left[\int_s^t f(u,\omega)^2 \,du\right]\]</span> 但是，我们实际上需要更高阶的矩估计来应用 Kolmogorov连续性定理，所以用等距公式还不够。</p><h4 id="doob-鞅不等式doobs-martingale-inequality">Doob 鞅不等式（Doob’sMartingale Inequality）</h4><p>设 <span class="math inline">\(M_t\)</span> 是一个鞅过程，且对于每个<span class="math inline">\(t\)</span>，<span class="math inline">\(t\mapsto M_t(\omega)\)</span> 是连续的。则对于任意的 <spanclass="math inline">\(p &gt; 1\)</span>，<span class="math inline">\(T&gt; 0\)</span>，<span class="math inline">\(\lambda &gt; 0\)</span>，有<span class="math display">\[P\left(\sup_{0 \leq t \leq T} |M_t| \geq \lambda\right) \leq\frac{\mathbb E[|M_T|^p]}{\lambda^p}\]</span></p><p>这个实际上是类比概率论中的 Markov 不等式。回顾 Markov不等式的证明过程，给定一个 <spanclass="math inline">\(\lambda\)</span>，我们只考虑那些 <spanclass="math inline">\(|X| \geq \lambda\)</span> 的事件，每一个都至少贡献<span class="math inline">\(\lambda^p\)</span>，所以总贡献至少是 <spanclass="math inline">\(\lambda^p P(|X| \geq\lambda)\)</span>，而这个贡献不能超过 <spanclass="math inline">\(E[|X|^p]\)</span>。</p><p>那么现在，我们处理的是一个随机过程 <spanclass="math inline">\(M_t\)</span>，我们考虑在区间 <spanclass="math inline">\([0,T]\)</span> 上的最大值 <spanclass="math inline">\(\sup_{0 \leq t \leq T}|M_t|\)</span>。对于那些路径上最大值超过 <spanclass="math inline">\(\lambda\)</span> 的事件 <spanclass="math inline">\(\omega\)</span>，我们需要考虑它对 <spanclass="math inline">\(|M_T|^p\)</span> 的贡献。</p><p>换句话说，我们需要一个不等式，控制路径最大值和终点值之间的关系。那么，现在我们考虑第一次超过<span class="math inline">\(\lambda\)</span> 的时间点 <spanclass="math inline">\(\tau\)</span>。在这个时候，鞅性质告诉我们，因为我们不知道未来信息，未来的期望值仍然是当前值，<spanclass="math inline">\(\mathbb E[M_{T}(\omega) | \mathcal{F}_\tau] =M_\tau(\omega)\)</span>。</p><h5 id="可选停时定理-optional-stopping-theorem">可选停时定理 （OptionalStopping Theorem）</h5><p>对于过滤 <spanclass="math inline">\(\{\mathcal{F}_t\}\)</span>，随机变量 <spanclass="math inline">\(\tau: \Omega \to [0, \infty]\)</span> 称为一个停时（stopping time），如果对于每个 <span class="math inline">\(t \geq0\)</span>，事件 <span class="math inline">\(\{\tau \leq t\} \in\mathcal{F}_t\)</span>。</p><p>现给定随机过程 <span class="math inline">\(M_t\)</span> 和停时 <spanclass="math inline">\(\tau\)</span>，定义截断过程 <spanclass="math inline">\(M_{t \wedge \tau}\)</span> 为 <spanclass="math display">\[M_{t \wedge \tau}(\omega) = M_{\min(t, \tau(\omega))}(\omega)\]</span> 也就是在时间 <span class="math inline">\(\tau(\omega)\)</span>之后，过程保持不变，因此称作停止过程。</p><p>停止过程<strong>保持适应性</strong>：如果 <spanclass="math inline">\(M_t\)</span> 是关于 <spanclass="math inline">\(\{\mathcal{F}_t\}\)</span> 的适应过程，则 <spanclass="math inline">\(M_{t \wedge \tau}\)</span> 也是关于 <spanclass="math inline">\(\{\mathcal{F}_t\}\)</span> 的适应过程。证明：对于任意的 <span class="math inline">\(t \geq 0\)</span>，有 <spanclass="math display">\[\{ \omega : M_{t \wedge \tau}(\omega) \in B \} = \{\omega : \tau(\omega)\leq t, M_{\tau(\omega)}(\omega) \in B\} \cup \{\omega : \tau(\omega)&gt; t, M_t(\omega) \in B\}\]</span> 其中 <span class="math inline">\(\{\omega : \tau(\omega) \leqt\} \in \cal{F}_t\)</span>，且 <span class="math inline">\(\{\omega :M_{\tau(\omega)}(\omega) \in B\} \in \cal{F}_{\tau(\omega)} \subseteq\cal{F}_t\)</span>，因此第一部分在 <spanclass="math inline">\(\cal{F}_t\)</span> 中。同理，第二部分也在 <spanclass="math inline">\(\cal{F}_t\)</span> 中，因此整体也在 <spanclass="math inline">\(\cal{F}_t\)</span> 中。</p><p>注意到 <span class="math inline">\(M_{t \wedge \tau_2} - M_{t \wedge\tau_1} = \mathbf{1}_{\{\tau_1 &lt; t\}} (M_{t \wedge \tau_2} -M_{\tau_1})\)</span>。特别地，<span class="math inline">\(M_{t \wedge\tau} - M_0 = \mathbf{1}_{\{\tau &lt; t\}} (M_{t \wedge \tau} -M_0)\)</span>。</p><p>设 <span class="math inline">\(\{M_t\}_{t \geq 0}\)</span> 是关于<span class="math inline">\(\{\mathcal{F}_t\}\)</span>的右连续鞅（即对于每个 <spanclass="math inline">\(\omega\)</span>，<span class="math inline">\(t\mapsto M_t(\omega)\)</span> 是右连续的），满足 <spanclass="math inline">\(M_{t\wedge \tau}\)</span> 是均匀可积的（uniformlyintegrable），即 <span class="math display">\[\lim_{K \to \infty} \sup_{t\geq 0} \mathbb E[|M_t| \mathbf{1}_{\{|M_t|&gt; K\}}] = 0.\]</span> （一个充分条件是 <span class="math inline">\(\tau\)</span>是有界的，或者被某个可积随机变量控制。）</p><p>则对于两个有限停时（<span class="math inline">\(P(\tau&lt;\infty) =1\)</span>） <span class="math inline">\(\tau_1 \leq \tau_2\)</span>，有<span class="math display">\[\mathbb E[M_{\tau_2} | \mathcal{F}_{\tau_1}] = M_{\tau_1}, \quad\text{a.s.}\]</span> 特别地， <span class="math display">\[\mathbb E[M_{\tau_2}] = \mathbb E[M_{\tau_1}].\]</span></p><p>证明： 首先考虑简单停时的情况。设 <spanclass="math inline">\(\tau_1\)</span> 和 <spanclass="math inline">\(\tau_2\)</span> 是简单停时，分别取值于有限集合<span class="math inline">\(\{t_1, t_2, \ldots, t_n\}\)</span> 和 <spanclass="math inline">\(\{s_1, s_2, \ldots, s_m\}\)</span>。则 <spanclass="math display">\[\begin{aligned}\mathbb E[M_{\tau_2} | \mathcal{F}_{\tau_1}] &amp;= \sum_{i=1}^n \mathbbE[M_{\tau_2} | \mathcal{F}_{t_i}] \mathbf{1}_{\tau_1 = t_i} \\&amp;= \sum_{i=1}^n \left( \sum_{j=1}^m \mathbb E[M_{s_j} |\mathcal{F}_{t_i}] \mathbf{1}_{\tau_2 = s_j} \right) \mathbf{1}_{\tau_1= t_i} \\&amp;= \sum_{i=1}^n \left( \sum_{j=1}^m M_{t_i} \mathbf{1}_{\tau_2 =s_j} \right) \mathbf{1}_{\tau_1 = t_i} \quad \text{（鞅性质）} \\&amp;= \sum_{i=1}^n M_{t_i} \mathbf{1}_{\tau_1 = t_i} \\&amp;= M_{\tau_1}\end{aligned}\]</span> 而对于一般的停时 <span class="math inline">\(\tau_1,\tau_2\)</span>，我们可以找到一列简单停时 <spanclass="math inline">\(\{\tau_1^n\}, \{\tau_2^n\}\)</span>，使得 <spanclass="math inline">\(\tau_1^n \downarrow \tau_1\)</span>，<spanclass="math inline">\(\tau_2^n \downarrow \tau_2\)</span>。</p><p>对于每一组 <span class="math inline">\(\tau_1^n,\tau_2^n\)</span>，我们已经知道： <span class="math display">\[\mathbb{E}[M_{\tau_2^n} \mathbf{1}_{A}] = \mathbb{E}[M_{\tau_1^n}\mathbf{1}_{A}] \quad (\text{对所有 } A \in \mathcal{F}_{\tau_1^n})\]</span> 现在约定 <span class="math inline">\(A \in\mathcal{F}_{\tau_1}\)</span>，则对于每个 <spanclass="math inline">\(n\)</span>，都有 <span class="math inline">\(A \in\mathcal{F}_{\tau_1^n}\)</span>，因此，两侧取极限得到 <spanclass="math display">\[\mathbb{E}[M_{\tau_2} \mathbf{1}_{A}] = \mathbb{E}[M_{\tau_1}\mathbf{1}_{A}]\]</span> 于是 <span class="math inline">\(\mathbb{E}[M_{\tau_2} |\mathcal{F}_{\tau_1}] = M_{\tau_1}\)</span>几乎处处成立，这里我们使用了均匀可积性来交换极限和期望。</p><h5 id="次鞅的可选停时定理">次鞅的可选停时定理</h5><p>定义次鞅（submartingale）：如果对于所有的 <spanclass="math inline">\(s &lt; t\)</span>，都有 <spanclass="math display">\[\mathbb E[M_t | \mathcal{F}_s] \geq M_s,\]</span> 则称 <span class="math inline">\(M_t\)</span>是一个次鞅。而可选停时定理同样适用于次鞅，结论变为 <spanclass="math display">\[\mathbb E[M_{\tau_2} | \mathcal{F}_{\tau_1}] \geq M_{\tau_1}, \quad\text{a.s.}\]</span></p><hr /><p>回到 Doob 鞅不等式的证明，我们取停时 <span class="math inline">\(\tau= \inf\{t \geq 0 : |M_t| \geq \lambda\}\)</span>，则 <spanclass="math display">\[P\left(\sup_{0 \leq t \leq T} |M_t| \geq \lambda\right) = P(\tau \leqT).\]</span> 根据 Jensen 不等式 的条件期望版本，对于 <spanclass="math inline">\(s &lt; t\)</span>： <span class="math display">\[E[|M_t|^p \mid \mathcal{F}_s] \geq |E[M_t \mid \mathcal{F}_s]|^p =|M_s|^p\]</span> 因此，<span class="math inline">\(|M_t|^p\)</span>是一个次鞅。由可选停时定理，有 <span class="math display">\[\mathbb E[|M_{T \wedge \tau}|^p] \geq \mathbb E[|M_0|^p] = |M_0|^p.\]</span></p><p>注意到当 <span class="math inline">\(\tau \leq T\)</span> 时，<spanclass="math inline">\(|M_{T \wedge \tau}| \geq \lambda\)</span>，因此<span class="math display">\[\begin{aligned}\mathbb E[|M_{T \wedge \tau}|^p] &amp;\geq \mathbb E[|M_{T \wedge\tau}|^p \mathbf{1}_{\{\tau \leq T\}}] \\&amp;\geq \lambda^p P(\tau \leq T).\end{aligned}\]</span> 综上所述，我们得到 <span class="math display">\[P\left(\sup_{0 \leq t \leq T} |M_t| \geq \lambda\right) = P(\tau \leq T)\leq \frac{\mathbb E[|M_{T \wedge \tau}|^p]}{\lambda^p} \leq\frac{\mathbb E[|M_T|^p]}{\lambda^p}.\]</span> 这就完成了 Doob 鞅不等式的证明。</p><h4 id="ito-积分路径的连续性">Ito 积分路径的连续性</h4><p>现在我们回到 Ito 积分路径的连续性问题。设 <spanclass="math inline">\(f \in \mathcal{V}(0,T)\)</span>，我们定义 Ito积分过程 <span class="math display">\[M_t = \int_0^t f(s,\omega) \, dB_s(\omega).\]</span> 我们希望证明 <span class="math inline">\(M_t\)</span>存在一个修改版本，使得对于几乎所有的 <spanclass="math inline">\(\omega\)</span>，路径 <spanclass="math inline">\(t \mapsto M_t(\omega)\)</span> 是连续的。</p><p>令 <span class="math inline">\(\phi_n\)</span> 是逼近 <spanclass="math inline">\(f\)</span> 的简单过程列，使得 <spanclass="math display">\[\lim_{n \to \infty} \mathbb E\left[\int_0^T |f(t,\omega) -\phi_n(t,\omega)|^2 \, dt\right] = 0.\]</span> 对应的 Ito 积分过程为 <span class="math display">\[I_n(t,\omega) = \int_0^t \phi_n(s,\omega) \, dB_s(\omega).\]</span> 同时定义 <span class="math display">\[I(t,\omega) = \int_0^t f(s,\omega) \,dB_s(\omega).\]</span> 由于 <span class="math inline">\(\phi_n\)</span>是简单过程，<span class="math inline">\(I_n(t,\omega)\)</span> 对于每个<span class="math inline">\(\omega\)</span> 都是连续的。 现在我们来估计<span class="math inline">\(I_n(t,\omega)\)</span> 和 <spanclass="math inline">\(I_m(t,\omega)\)</span> 之间的差异。利用 Doob鞅不等式，对于任意的 <span class="math inline">\(\epsilon &gt;0\)</span>，有 <span class="math display">\[P\left(\sup_{0 \leq t \leq T} |I_n(t,\omega) - I_m(t,\omega)| \geq\epsilon\right) \leq \frac{1}{\epsilon^2} \mathbb{E}\left[|I_n(T,\omega)- I_m(T,\omega)|^2\right].\]</span> 根据 Ito 等距公式，我们有 <span class="math display">\[\lim_{n,m\to \infty} \mathbb{E}\left[|I_n(T,\omega) -I_m(T,\omega)|^2\right] = \mathbb{E}\left[\int_0^T |\phi_n(s,\omega) -\phi_m(s,\omega)|^2 \, ds\right]=0\]</span> 所以我们可以选取一个子序列 <spanclass="math inline">\(\{I_{n_k}\}\)</span>，使得 <spanclass="math display">\[\sum_{k=1}^\infty P\left(\sup_{0 \leq t \leq T} |I_{n_{k+1}}(t,\omega) -I_{n_k}(t,\omega)| \geq 2^{-k}\right) &lt; \infty.\]</span> 根据 Borel-Cantelli 引理，几乎所有的 <spanclass="math inline">\(\omega\)</span>，存在一个 <spanclass="math inline">\(k_0(\omega)\)</span>，使得对于所有的 <spanclass="math inline">\(k \geq k_0(\omega)\)</span>，都有 <spanclass="math display">\[\sup_{0 \leq t \leq T} |I_{n_{k+1}}(t,\omega) - I_{n_k}(t,\omega)| &lt;2^{-k}.\]</span> 这说明对于几乎所有的 <spanclass="math inline">\(\omega\)</span>，序列 <spanclass="math inline">\(\{I_{n_k}(t,\omega)\}\)</span> 在 <spanclass="math inline">\(C([0,T])\)</span> 空间中是一致收敛的。因此，定义<span class="math display">\[I(t,\omega) = \lim_{k \to \infty} I_{n_k}(t,\omega),\]</span> 则对于几乎所有的 <spanclass="math inline">\(\omega\)</span>，路径 <spanclass="math inline">\(t \mapsto I(t,\omega)\)</span> 是连续的。</p><p>因此，我们得出结论：对于任意的 <span class="math inline">\(f \in\mathcal{V}(0,T)\)</span>，Ito 积分过程 <span class="math inline">\(M_t= \int_0^t f(s,\omega) \, dB_s(\omega)\)</span>存在一个修改版本，使得对于几乎所有的 <spanclass="math inline">\(\omega\)</span>，路径 <spanclass="math inline">\(t \mapsto M_t(\omega)\)</span>是连续的。<strong>以后我们默认使用这个连续版本的 Ito积分过程。</strong></p><p>而且我们有 <span class="math display">\[P\left(\sup_{0 \leq t \leq T} |M_t| \geq \lambda\right) \leq\frac{1}{\lambda^2} \mathbb{E}\left[|M_T|^2\right] = \frac{1}{\lambda^2}\mathbb{E}\left[\int_0^T f(s,\omega)^2 \, ds\right].\]</span></p><h3 id="ito-引理">Ito 引理</h3><p>在讨论 Ito 引理之前，我们先具体计算一下几个 Ito 积分和 Stratonovich积分的例子，来帮助理解它们和普通微积分的区别。</p><h4 id="计算-ito-积分-int_0t-f-db_t">计算 Ito 积分 <spanclass="math inline">\(\int_0^T f \, dB_t\)</span></h4><p>假设我们什么公式都没学过，我们拿到这个积分在手，唯一能做的就是按照定义展开，取 <span class="math inline">\(\phi_n= \sum f(t_j,\cdot)\mathbf{1}_{t_j\leq t &lt; t_{j+1}}, t_j = \frac{j}{2^n} T\)</span><span class="math display">\[\int_{0}^{T} f \, dB_t = \lim_{n\to\infty } \sum_{j}\phi_n(t_j)(B_{t_{j+1}}-B_{t_j}) =  \lim_{n\to\infty } \sum_{j}f(t_j,\cdot)(B_{t_{j+1}}-B_{t_j})\]</span></p><p>仿照之前的计算，我们希望把 <span class="math inline">\(f\)</span> 在<span class="math inline">\(t_j\)</span>的取值分成一些部分，每个部分都是某个不定积分的增量。我们先假定我们的积分结果形式为<span class="math display">\[\int_S^t f \, dB_t = F(t,B_t, \omega) - F(S,B_S, \omega)\]</span> 其中 <span class="math inline">\(F\)</span>是某个待定函数。然后我们来计算增量 <span class="math display">\[\begin{aligned}\Delta F_j &amp;:= F(t_{j+1}, B_{t_{j+1}}, \omega) - F(t_j, B_{t_j},\omega) \\&amp;= F(t_{j+1}, B_{t_j} + \Delta B_j, \omega) - F(t_j, B_{t_j},\omega) \\&amp;= \sum_{k=1}^\infty \frac{1}{k!} \frac{\partial^k F}{\partialx^k}(t_j, B_{t_j}, \omega) (\Delta B_j)^k + \frac{\partial F}{\partialt}(t_j, B_{t_j}, \omega) \Delta t_j + o(\Delta t_j)\end{aligned}\]</span> 这里的展开是对 <span class="math inline">\(x\)</span>变量的泰勒展开加上对 <span class="math inline">\(t\)</span>变量的线性近似。我们简单考虑一下对 <spanclass="math inline">\(x\)</span> 的二阶截断误差项 <spanclass="math inline">\(R_j\)</span> 在 <spanclass="math inline">\(L^2\)</span> 意义下的收敛性。我们有 <spanclass="math display">\[E \left[ \left( \sum_j R_j \right)^2 \right] = \sum_j E[R_j^2] + \sum_{i\neq j} E[R_i R_j]\]</span> 根据拉格朗日余项，<span class="math inline">\(R_j =\frac{1}{6} F_{xxx}(\eta_j) (\Delta B_j)^3\)</span>。 假设 <spanclass="math inline">\(F_{xxx}\)</span> 一致有界，则：</p><p>第一部分（平方项）：<span class="math inline">\(E[R_j^2] \leq C \cdotE[(\Delta B_j)^6]\)</span>。由正态分布矩性质，<spanclass="math inline">\(E[(\Delta B_j)^6] = 15(\Delta t_j)^3\)</span>。<span class="math display">\[\sum_j E[R_j^2] \leq 15C \sum_j (\Delta t_j)^3 \leq 15C \cdot \delta^2\sum_j \Delta t_j = 15C \cdot \delta^2 (T-S) \to 0\]</span></p><p>第二部分（交叉项）：</p><p>根据全期望公式，<span class="math inline">\(E[R_i R_j] = E[E[R_i R_j| \mathcal{F}_{t_{\max(i,j)}}]]\)</span>。假设 <spanclass="math inline">\(i &lt; j\)</span>，则 <spanclass="math display">\[E[R_i R_j | \mathcal{F}_{t_j}] = R_i E[R_j |\mathcal{F}_{t_j}] = R_i \cdot 0 = 0\]</span> 因为 <spanclass="math inline">\(\Delta B_j\)</span> 独立于 <spanclass="math inline">\(\mathcal{F}_{t_j}\)</span>，且 <spanclass="math inline">\(E[\Delta B_j] = 0\)</span>。所以交叉项为零。</p><p>所以当我们对所有的增量求和时，<span class="math inline">\(n\geq2\)</span> 的误差项 在 <span class="math inline">\(L^2\)</span>意义下收敛到零。</p><p>我们只需要对这个展开式取前两项： <span class="math display">\[\Delta F_j \approx \frac{\partial F}{\partial x}(t_j, B_{t_j}, \omega)\Delta B_j + \frac{1}{2} \frac{\partial^2 F}{\partial x^2}(t_j, B_{t_j},\omega) (\Delta B_j)^2 + \frac{\partial F}{\partial t}(t_j, B_{t_j},\omega) \Delta t_j\]</span> 因此，我们有 <span class="math display">\[\sum_j \Delta F_j \approx \sum_j \frac{\partial F}{\partial x}(t_j,B_{t_j}, \omega) \Delta B_j + \frac{1}{2} \sum_j \frac{\partial^2F}{\partial x^2}(t_j, B_{t_j}, \omega) (\Delta B_j)^2 + \sum_j\frac{\partial F}{\partial t}(t_j, B_{t_j}, \omega) \Delta t_j\]</span> 注意到 <span class="math inline">\((\Delta B_j)^2\)</span>收敛到 <span class="math inline">\(\Deltat_j\)</span>，所以在极限下，第二项变成 <spanclass="math display">\[\frac{1}{2} \int_S^T \frac{\partial^2 F}{\partialx^2}(t, B_t, \omega) dt\]</span> 而第三项则是 <spanclass="math display">\[\int_S^T \frac{\partial F}{\partial t}(t, B_t,\omega) dt\]</span></p><p>因此，我们得出 Ito 引理的基本形式：</p><h4 id="ito-引理itos-lemma">Ito 引理（Ito’s Lemma）</h4><p>设 <span class="math inline">\(B_t\)</span> 是一个标准布朗运动，<spanclass="math inline">\(F: [0,T] \times \mathbb{R} \to \mathbb{R}\)</span>是一个二次连续可微函数（即 <span class="math inline">\(F \inC^{1,2}([0,T] \times \mathbb{R})\)</span>）。定义随机过程 <spanclass="math display">\[X_t = F(t, B_t).\]</span> 则 <span class="math inline">\(X_t\)</span> 的微分满足 <spanclass="math display">\[dX_t = \frac{\partial F}{\partial t}(t, B_t) dt + \frac{\partialF}{\partial x}(t, B_t) dB_t + \frac{1}{2} \frac{\partial^2 F}{\partialx^2}(t, B_t) dt.\]</span></p><p>于是我们记住以下微分的规则：</p><ul><li><p><span class="math inline">\(dt \cdot dt = 0\)</span></p></li><li><p><span class="math inline">\(dt \cdot dB_t = 0\)</span></p></li><li><p><span class="math inline">\(dB_t \cdot dB_t =dt\)</span></p></li></ul><p>这些都是在 <span class="math inline">\(L^2\)</span>意义下对积分求和逼近意义下成立的。</p><p>回到上面的例子，我们现在试图构造一个函数 <spanclass="math inline">\(F\)</span>，使得 <span class="math display">\[\frac{\partial F}{\partial x}(t, B_t) = f(t, B_t).\]</span> 这就意味着我们可以取 <span class="math display">\[F(t,x) = \int_0^x f(t,y) \, dy - G(t),\]</span> 其中 <span class="math inline">\(G(t)\)</span> 是任意的关于<span class="math inline">\(t\)</span> 的可微函数。</p><p>根据 Ito 引理，我们有 <span class="math display">\[dX_t = \frac{\partial F}{\partial t}(t, B_t) dt + f(t, B_t) dB_t +\frac{1}{2} \frac{\partial f}{\partial x}(t, B_t) dt.\]</span> 因此，Ito 积分 <span class="math inline">\(\int_0^T f(t, B_t)\, dB_t\)</span> 可以表示为 <span class="math display">\[\int_0^T f(t, B_t) \, dB_t = X_T - X_0 - \int_0^T \left( \frac{\partialF}{\partial t}(t, B_t) + \frac{1}{2} \frac{\partial f}{\partial x}(t,B_t) \right) dt.\]</span></p><p>这就是我们通过 Ito 引理计算 Ito 积分的一个基本方法。首先对 <spanclass="math inline">\(f\)</span> 非时间依赖部分进行不定积分，得到 <spanclass="math inline">\(F\)</span>，然后应用 Ito 引理计算 <spanclass="math inline">\(dX_t\)</span>，最后减去时间积分部分即可得到所需的Ito 积分表达式。</p><p>在以上的例子中，我们取 <spanclass="math inline">\(f(x,t)=x\)</span>， <spanclass="math inline">\(G(t) = 0\)</span>，<spanclass="math inline">\(F(t,x) = \int_0^x y \, dy = \frac{1}{2}x^2\)</span>，$ X_t = F(t, B_t)$。</p><p><span class="math display">\[\int_0^T B_t \, dB_t = X_T - X_0 - \int_0^T \left( 0 + \frac{1}{2} \cdot1 \right) dt = \frac{1}{2} B_T^2 - \frac{1}{2} T\]</span> 这和我们之前直接计算的结果是一致的。</p><p>如果我们想寻找一个类似于微积分中直接的原函数形式的表达式，根据 Ito引理，我们需要修改 <spanclass="math inline">\(G(t)\)</span>，使得剩下的时间积分部分抵消，也即<span class="math display">\[\frac{dG}{dt}(t) = \int_0^{x} \frac{\partial f}{\partial t}(t,y) \, dy +\frac{1}{2} \frac{\partial f}{\partial x}(t, x)\]</span></p><h3 id="ito-积分的推广">Ito 积分的推广</h3><p>到目前为止，我们已经定义了适应过程 <span class="math inline">\(f \in\mathcal V\)</span> 上的 Ito 积分 <span class="math inline">\(\int_S^Tf(t,\omega) \,dB_t(\omega)\)</span>，并且证明了它的鞅性质和路径连续性。</p><p>我们可以拓展它的定义域，使得 <span class="math inline">\(f\)</span>适应于更复杂的滤过，不再局限于一维布朗运动。</p><p>首先，在 <span class="math inline">\(\mathcal{V}(S,T)\)</span>的定义中，对应的滤过可以改为满足以下条件的滤过 <spanclass="math inline">\(\{\mathcal{H}_t\}\)</span>： 1. <spanclass="math inline">\(B_t\)</span> 是关于 <spanclass="math inline">\(\{\mathcal{H}_t\}\)</span> 的鞅。 2. <spanclass="math inline">\(f_t\)</span> 是关于 <spanclass="math inline">\(\{\mathcal{H}_t\}\)</span> 的适应过程，即对于每个<span class="math inline">\(t\)</span>，<spanclass="math inline">\(f_t\)</span> 是 <spanclass="math inline">\(\mathcal{H}_t\)</span> 可测的。</p><p>注意 (1) 蕴含了 <span class="math inline">\(\mathcal{F}_t \subseteq\mathcal{H}_t\)</span>。</p><p>实际上，这里的意思是，我们允许 <span class="math inline">\(f\)</span>依赖于比布朗运动更多的信息，只要布朗运动仍然是这个更大滤过下的鞅即可，因为在我们证明Ito 积分的鞅性质时，只用到了布朗运动增量独立于过去的信息和 <spanclass="math inline">\(f\)</span>的适应性，在上面证明的过程中已经标注清楚了。</p><h4 id="多维-ito-积分">多维 Ito 积分</h4><p>设 <span class="math inline">\(\mathbf{B}_t = (B_t^{(1)}, B_t^{(2)},\ldots, B_t^{(n)})\)</span> 是一个 <spanclass="math inline">\(n\)</span> 维标准布朗运动，即每个分量 <spanclass="math inline">\(B_t^{(i)}\)</span> 都是独立的标准布朗运动。</p><p>令 <span class="math inline">\(\mathcal{V}^{m\timesn}_{\mathcal{H}}(S,T)\)</span> 为 <span class="math inline">\(m\timesn\)</span> 矩阵 <spanclass="math inline">\(v=[v_{i,j}(t,\omega)]\)</span>构成的集合，其中，每个 <span class="math inline">\(v_{i,j}\)</span>满足以上的条件，（联合可测，对某个滤过 <spanclass="math inline">\(\mathcal{H_t}\)</span> 适应，<spanclass="math inline">\(L^2\)</span> 有界）。</p><p>我们定义多维 Ito 积分为 <span class="math display">\[\int_S^T v\, d\mathbf{B}_t = \int_S^T \begin{pmatrix}v_{1,1} &amp; \cdots &amp; v_{1,n} \\\vdots  &amp; \ddots &amp; \vdots \\v_{m,1} &amp; \cdots &amp; v_{m,n}\end{pmatrix} \, d\begin{pmatrix}B_t^{(1)} \\\vdots \\B_t^{(n)}\end{pmatrix} = \begin{pmatrix}\sum_{j=1}^n \int_S^T v_{1,j}(t,\omega) \, dB_t^{(j)}(\omega) \\\vdots \\\sum_{j=1}^n \int_S^T v_{m,j}(t,\omega) \, dB_t^{(j)}(\omega)\end{pmatrix}\]</span></p><p>这里的每个分量都是一个一维 Ito 积分。类似地，我们可以定义多维 Ito积分的等距性质和鞅性质。</p><ol type="1"><li><p>等距性质： <span class="math display">\[\mathbb E\left[\left\|\int_S^T v \, d\mathbf{B}_t\right\|^2\right] =\mathbb E\left[\int_S^T \|v(t,\omega)\|_F^2 \, dt\right]\]</span> 其中 <span class="math inline">\(\|v(t,\omega)\|_F\)</span>是矩阵 <span class="math inline">\(v(t,\omega)\)</span> 的 Frobenius范数，定义为 <span class="math display">\[\|v(t,\omega)\|_F^2 = \sum_{i=1}^m \sum_{j=1}^n |v_{i,j}(t,\omega)|^2.\]</span></p></li><li><p>鞅性质： 设 <span class="math inline">\(M_t = \int_S^tv(t,\omega) \,d\mathbf{B}_t(\omega)\)</span>，则对于 <spanclass="math inline">\(s &lt; t\)</span>，有 <spanclass="math display">\[\mathbb E[M_t | \mathcal{H}_s] = M_s.\]</span></p></li></ol><p>我们将以上过程构成的空间分别记作 <spanclass="math inline">\(\mathcal{W}_{\mathcal{H}}(S,T),\mathcal{W}_{\mathcal{H}}^{m\times n}(S,T)\)</span>，并记 <spanclass="math inline">\(\mathcal{W}_{\mathcal{H}} = \bigcup_{T&gt;0}\mathcal{W}_{\mathcal{H}}(0,T)\)</span>.</p><h3 id="与-stratonovich-积分的比较">与 Stratonovich 积分的比较</h3><p>回顾之前所作的，我们可以给这样的一个随机微分方程 <spanclass="math display">\[\frac{d X}{dt} = b(t, X_t) + \sigma(t, X_t) \cdot W_t\]</span> 在 Ito 积分框架下一个合理的解： <span class="math display">\[X_t = X_0 + \int_0^t b(s, X_s) \, ds + \int_0^t \sigma(s, X_s) dB_s\]</span> 其中 <span class="math inline">\(W_t\)</span>是某种“白噪声”（看作布朗运动的“导数”）。</p><p>Ito 积分的意义是，考虑简单过程 <spanclass="math inline">\(\phi_n\)</span> 逼近 <spanclass="math inline">\(\sigma(t, X_t)\)</span>，<spanclass="math inline">\(L_t = \int_0^t \sigma(s, X_s) dB_s\)</span> 就是<span class="math inline">\(\int_0^t \phi_n(s) dB_s\)</span> 的 <spanclass="math inline">\(L^2\)</span> 极限。</p><p>也就是认为 <span class="math inline">\(\sigma(t, X_t)\)</span>在每个时间点 <span class="math inline">\(t\)</span>的取值在极小时间内保持不变并且等于左侧点的取值，然后考虑布朗运动的波动的影响并求和汇总。等距公式告诉我们，<spanclass="math inline">\(L_t\)</span> 的方差是 <spanclass="math inline">\(\int_0^t \sigma(s, X_s)^2 ds\)</span>。</p><p>这里取左侧点和其他点会得到不同的结果是因为布朗运动的增量可能与 <spanclass="math inline">\(\sigma(t, X_t)\)</span> 的取值相关联了，而 Ito积分保证了这个增量独立于 <span class="math inline">\(\sigma(t,X_t)\)</span> 的取值，也就是鞅性质的体现。</p><p>这个关联项，由上面的 Ito引理的展开分析，我们大致知道，它意味着某种额外的漂移项的出现，对应结果中<span class="math inline">\(B_t\)</span> 的二阶影响。</p><p>现在我们来严格分析，从这样的一个展开式出发。 <spanclass="math display">\[\sum_j \Delta F_j \approx \sum_j \frac{\partial F}{\partial x}(t_j,B_{t_j}, \omega) \Delta B_j + \frac{1}{2} \sum_j \frac{\partial^2F}{\partial x^2}(t_j, B_{t_j}, \omega) (\Delta B_j)^2 + \sum_j\frac{\partial F}{\partial t}(t_j, B_{t_j}, \omega) \Delta t_j\]</span> 我们假设有 <span class="math inline">\(\frac{\partialF}{\partial x}(t_j, B_{t_j}, \omega) = \sigma(t_j,X_{t_j})\)</span>，则第一项就是我们定义的 Ito 积分的近似表达式。</p><p>这个告诉我们什么？首先 <span class="math inline">\(F\)</span>的增量可以分成两部分，一部分关于时间的增量，另一部分关于空间（布朗运动）增量的影响。对于时间的响应，我们只需要考虑一阶就足以在分割趋于零时得到正确的结果；但是对于空间的响应，如果只取左侧点并求和，就需要额外考虑一个二阶项，才能在分割趋于零时得到正确的结果。</p><p>但这样的一个二阶项展开，实际上就等价于我们使用导数在区间中点的取值了（误差一个三阶项会消失）。如果我们在定义积分时，取<span class="math inline">\(\sigma(t, X_t)\)</span> 在区间 <spanclass="math inline">\([t_j, t_{j+1}]\)</span>的中点处的取值，那么这个二阶项就会自然地被包含在内。</p><p>所以，我们有以下结论： <span class="math display">\[\sum_j \Delta F_j \approx \sum_j \sigma\left(\frac{t_j + t_{j+1}}{2},X_{\frac{t_j + t_{j+1}}{2}}\right) \Delta B_j + \sum_j \frac{\partialF}{\partial t}(t_j, B_{t_j}, \omega) \Delta t_j\]</span> 而Stratonovich 积分按定义就是： <span class="math display">\[\int_0^t \sigma(s, X_s) \circ dB_s = \lim_{n \to \infty} \sum_j\sigma\left(\frac{t_j + t_{j+1}}{2}, X_{\frac{t_j + t_{j+1}}{2}}\right)(B_{t_{j+1}} - B_{t_j}),\]</span> 那从微分的角度，我们简洁地有： <span class="math display">\[dX_t = b(t, X_t) dt + \sigma(t, X_t) \circ dB_t\]</span> 因此，Stratonovich积分的定义方式使得它在微分形式上与普通微积分的链式法则保持一致，而 Ito积分则需要额外的二阶项来修正。</p><p>利用以下公式，我们可以在 Ito 积分和 Stratonovich 积分之间进行转换：<span class="math display">\[\sigma(t, X_t) \circ dB_t = \sigma(t, X_t) dB_t + \frac{1}{2}\frac{\partial \sigma}{\partial x}(t, X_t) dt\]</span></p><p>而如果考虑一列 <span class="math inline">\(t\)</span>-可微的随机过程<span class="math inline">\(B_t^{(n)}\)</span>，使得 <spanclass="math inline">\(B_t^{(n)}\)</span> 在 <spanclass="math inline">\(L^2\)</span> 意义下一致收敛到 <spanclass="math inline">\(B_t\)</span>。 则，以上的随机微分方程变为 <spanclass="math display">\[\frac{d X^{(n)}}{dt} = b(t, X_t^{(n)}) + \sigma(t, X_t^{(n)}) \cdot\frac{dB_t^{(n)}}{dt}\]</span> 它的解满足以下的积分方程： <span class="math display">\[X_t^{(n)} = X_0 + \int_0^t b(s, X_s^{(n)}) \, ds + \int_0^t \sigma(s,X_s^{(n)}) \cdot \frac{dB_s^{(n)}}{ds} ds\]</span></p><p>存在唯一性，也许可以通过 Banach 不动点定理来证明，但是问了问 AI感觉证明需要各种不等式估计，就算了。</p><p>如果 <span class="math inline">\(B_t^{(n)}\)</span> 是 <spanclass="math inline">\(B_t\)</span> 的某种平滑近似，那么 <spanclass="math inline">\(\int_0^t \sigma(s, X_s^{(n)}) \cdot\frac{dB_s^{(n)}}{ds} ds\)</span> 就是一个普通的 Riemann积分。众所周知，黎曼积分取中点是作为二阶近似的。因此，随着 <spanclass="math inline">\(n \to \infty\)</span>，取中点的黎曼积分会收敛到Stratonovich 积分 <span class="math inline">\(\int_0^t \sigma(s, X_s)\circ dB_s\)</span>，取左侧的黎曼积分则收敛到 Ito 积分 <spanclass="math inline">\(\int_0^t \sigma(s, X_s) dB_s\)</span>。</p><p>然后在不动点迭代的过程中，最后收敛的结果是 Stratonovich积分的解。具体的收敛性证明可以查阅 Wong-Zakai定理。这里确实也很神奇。</p><h3 id="练习">练习</h3><p>学而不练就完蛋了，接下来是练习环节。</p><p>3.1. 用 Ito 积分的定义，证明以下的等式： <spanclass="math display">\[\int_0^t s dB_s = t B_t - \int_0^t B_s ds\]</span></p><p>3.2 按定义证明： <span class="math display">\[\int_0^t B_s^2 dB_s = \frac{1}{3} B_t^3 - \int_0^t B_s ds\]</span></p><p>3.3 <span class="math inline">\(X_t: \Omega \to \mathbb{R}^n\)</span>是一个随机过程，<span class="math inline">\(\mathcal{H}_t\)</span>是其生成的滤过（<spanclass="math inline">\(\sigma\)</span>-代数）。试解决以下问题：</p><ol type="1"><li>如果 <span class="math inline">\(X_t\)</span> 是关于某个滤过 <spanclass="math inline">\(\mathcal{N}_t\)</span> 的鞅，那么 <spanclass="math inline">\(X_t\)</span> 也是关于 <spanclass="math inline">\(\mathcal{H}_t\)</span> 的鞅。</li><li>如果 <span class="math inline">\(X_t\)</span> 是关于 <spanclass="math inline">\(\mathcal{H}_t\)</span> 的鞅，那么 <spanclass="math inline">\(\mathbb{E}[X_t] = \mathbb{E}[X_0], \forall t\geq0\)</span>。</li><li>举出一例 <span class="math inline">\(X_t\)</span> 满足 (2)但不是关于 <span class="math inline">\(\mathcal{H}_t\)</span>的鞅。</li></ol><p>3.4 检查以下的过程是否是关于 <spanclass="math inline">\(\mathcal{F}_t\)</span> 的鞅：</p><ol type="1"><li><span class="math inline">\(X_t = B_t + 4t\)</span></li><li><span class="math inline">\(X_t = B_t^2\)</span></li><li><span class="math inline">\(X_t = t^2 B_t - 2 \int_0^t s B_sds\)</span></li><li><span class="math inline">\(X_t = B_1(t) B_2(t)\)</span>，其中 <spanclass="math inline">\((B_1(t), B_2(t))\)</span>是一个二维布朗运动。</li></ol><h4 id="解答">解答</h4><p>3.1</p><p>还是展示一下直接使用 Ito 引理来证明这个等式，为了利用 Ito 引理，令<span class="math display">\[\frac{\partial F}{\partial x}(t,x) = t \implies F(t,x) = t x - G(t)\]</span> 取 <span class="math inline">\(G(t) = 0\)</span>，则 <spanclass="math inline">\(F(t,x) = t x\)</span>。根据 Ito 引理，我们有 <spanclass="math display">\[dF(t, B_t) = B_t dt + t dB_t\]</span> 因此， <span class="math display">\[\int_0^t s dB_s = F(t, B_t) - F(0, B_0) - \int_0^t B_s ds = t B_t -\int_0^t B_s ds\]</span></p><p>直接按照定义来证明需要我们用简单过程切片分段逼近 <spanclass="math inline">\(s\)</span>，例如 <spanclass="math inline">\(\phi_n(s) = \sum_{j=0}^{n-1} t_j \mathbf{1}_{[t_j,t_{j+1})}(s)\)</span>，其中 <span class="math inline">\(t_j =\frac{j}{n} t\)</span>，则 <span class="math display">\[\int_0^t s dB_s = \lim_{n \to \infty} \sum_{j=0}^{n-1} t_j (B_{t_{j+1}}- B_{t_j})\]</span></p><p>回顾求和的 Abel 转换：</p><p>对于 <span class="math inline">\(\sum_{j=0}^{n} \Delta A_jb_j\)</span>，<span class="math inline">\(a_j = \Delta A_j = A_{j+1} -A_j\)</span>，我们有 <span class="math display">\[\sum_{j=0}^{n} \Delta A_j b_j = A_{n+1} b_n - A_0 b_0 - \sum_{j=1}^{n}A_{j} \Delta b_{j-1}\]</span></p><p>这里 <span class="math inline">\(b_j=t_j\)</span>，<spanclass="math inline">\(A_j = B_{t_j}\)</span>，则 <spanclass="math display">\[\begin{aligned}\sum_{j=0}^{n-1} t_j \Delta B_j &amp;= t_n B_{t_{n-1}} - t_0 B_{t_0} -\sum_{j=1}^{n-1} B_{t_j} \Delta t_{j-1} \\&amp;= t B_{t} - \sum_{j=1}^{n-1} B_{t_j} \Delta t_{j-1}\end{aligned}\]</span> 而 <span class="math inline">\(n \to \infty\)</span> 时，<spanclass="math inline">\(\sum_{j=1}^{n-1} B_{t_j} \Delta t_{j-1}\)</span>就是 <span class="math inline">\(\int_0^t B_s ds\)</span> 的右端 Riemann和的近似（或者取右端点的简单过程逼近 <spanclass="math inline">\(B_s\)</span>，在 Lebesgue积分的意义下也是成立的），因此 <span class="math display">\[\int_0^t s dB_s = \lim_{n \to \infty} \sum_{j=0}^{n-1} t_j (B_{t_{j+1}}- B_{t_j}) = t B_t - \int_0^t B_s ds\]</span></p><p>3.2</p><p>同样地，我们先使用 Ito 引理来证明这个等式。令 <spanclass="math display">\[\frac{\partial F}{\partial x}(t,x) = x^2 \implies F(t,x) = \frac{1}{3}x^3 - G(t)\]</span> 取 <span class="math inline">\(G(t) = 0\)</span>，则 <spanclass="math inline">\(F(t,x) = \frac{1}{3} x^3\)</span>。根据 Ito引理，我们有 <span class="math display">\[dF(t, B_t) = \frac{1}{2} B_t dt + B_t^2 dB_t\]</span> 因此， <span class="math display">\[\int_0^t B_s^2 dB_s = F(t, B_t) - F(0, B_0) - \int_0^t \frac{1}{2} B_sds = \frac{1}{3} B_t^3 - \int_0^t B_s ds\]</span></p><p>按定义证明，我们取 <span class="math inline">\(\phi_n(t,\omega) =\sum_{j=0}^{n-1} B_{t_j}^2 \mathbf{1}_{[t_j, t_{j+1})}(s)\)</span>，其中<span class="math inline">\(t_j = \frac{j}{n} t\)</span>。继续利用 Abel转换，我们有 <span class="math display">\[\begin{aligned}\sum_{j=0}^{n-1} B_{t_j}^2 \Delta B_j &amp;= B_{t_n}^2 B_{t_{n-1}} -B_{t_0}^3 - \sum_{j=1}^{n-1} B_{t_j}^2 \Delta t_{j-1} \\&amp;= B_t^2 B_{t_{n-1}} - \sum_{j=1}^{n-1} B_{t_j}^2 \Delta t_{j-1}\end{aligned}\]</span> 当 <span class="math inline">\(n \to \infty\)</span> 时，<spanclass="math inline">\(B_{t_{n-1}} \to B_t\)</span>，因此 <spanclass="math inline">\(B_t^2 B_{t_{n-1}} \to B_t^3\)</span>，而 <spanclass="math inline">\(\sum_{j=1}^{n-1} B_{t_j}^2 \Delta t_{j-1}\)</span>就是 <span class="math inline">\(\int_0^t B_s^2 ds\)</span>。</p><p>3.3</p><ol type="1"><li>如果 <span class="math inline">\(X_t\)</span> 是关于某个滤过 <spanclass="math inline">\(\mathcal{N}_t\)</span> 的鞅，那么对于 <spanclass="math inline">\(s &lt; t\)</span>，有 <spanclass="math display">\[\mathbb{E}[X_t | \mathcal{N}_s] = X_s.\]</span>由于 <span class="math inline">\(\mathcal{H}_s \subseteq\mathcal{N}_s\)</span>，根据全期望公式，我们有<spanclass="math display">\[\mathbb{E}[X_t | \mathcal{H}_s] =\mathbb{E}[\mathbb{E}[X_t | \mathcal{N}_s] | \mathcal{H}_s] =\mathbb{E}[X_s | \mathcal{H}_s] = X_s.\]</span> 因此，<spanclass="math inline">\(X_t\)</span> 也是关于 <spanclass="math inline">\(\mathcal{H}_t\)</span> 的鞅。我们理解为，一个随机过程如果在一个大信息集合下是鞅，那么在一个子信息集合下也是鞅。而<span class="math inline">\(X_t\)</span>本身生成的滤过是最小的满足适应性的滤过。</li><li>如果 <span class="math inline">\(X_t\)</span> 是关于 <spanclass="math inline">\(\mathcal{H}_t\)</span> 的鞅，我们知道 <spanclass="math inline">\(\mathbb{E}[X_t | \mathcal{H}_0] =X_0\)</span>。然后根据全期望公式，<spanclass="math inline">\(\mathbb{E}[X_t] = \mathbb{E}[\mathbb{E}[X_t |\mathcal{H}_0]] = \mathbb{E}[X_0]\)</span>。</li></ol>]]></content>
    
    
    <categories>
      
      <category>Mathematics</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Stochastic Differential Equations</tag>
      
      <tag>Ito Calculus</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>QAOA</title>
    <link href="/2025/12/15/QAOA/"/>
    <url>/2025/12/15/QAOA/</url>
    
    <content type="html"><![CDATA[<h1 id="quantum-approximate-optimization-algorithm-qaoa">QuantumApproximate Optimization Algorithm (QAOA)</h1><h2 id="problem-setting">Problem Setting</h2><p>Let <span class="math inline">\(z=z_1 z_2 \ldots z_n\)</span> be astring of <span class="math inline">\(n\)</span> bits, where each bit<span class="math inline">\(z_i \in \{0, 1\}\)</span>. We define a costfunction <span class="math inline">\(C(z)\)</span> that assigns a realvalue to each bit string. The goal is to find the bit string <spanclass="math inline">\(z^*\)</span> that maximizes the cost function:<span class="math display">\[z^* = \arg\max_{z \in \{0,1\}^n} C(z)=\arg\max_{z \in \{0,1\}^n}\sum_{\alpha} C_{\alpha}(z)\]</span> where <span class="math inline">\(C_{\alpha}(z)\)</span> areindicator functions <span class="math inline">\(\mathbf{1}_{\text{clause} \alpha \text{ is satisfied by } z}\)</span>.</p><h2 id="hamiltonian-representation">Hamiltonian Representation</h2><p>We can represent the cost function as a Hamiltonian operator actingon a quantum state. The Hamiltonian <spanclass="math inline">\(H_C\)</span> corresponding to the cost function<span class="math inline">\(C(z)\)</span> is given by: <spanclass="math display">\[H_C = \sum_{\alpha} H_{C_{\alpha}}\]</span> where each <span class="math inline">\(H_{C_{\alpha}}\)</span>is a diagonal operator in the computational basis defined as: <spanclass="math display">\[H_{C_{\alpha}} \ket{z}= C_{\alpha}(z) \ket{z}\]</span> Each <span class="math inline">\(H_{C_{\alpha}}\)</span> isHermitian, so is <span class="math inline">\(H_C\)</span> and <spanclass="math inline">\(H_C \ket{z} = C(z) \ket{z}\)</span>.</p><p>We now seek to find the maximum eigenvalue <spanclass="math inline">\(\lambda^*\)</span> of <spanclass="math inline">\(H_C\)</span> and the corresponding eigenstate<span class="math inline">\(\ket{z^*}\)</span>.</p><h2 id="what-we-can-learn-from-score-based-model-sbm">What we can learnfrom Score Based Model (SBM)</h2><p>In EBM, we define a probability distribution via the energy function(Hamiltonian):</p><p><span class="math display">\[p(z) = \frac{\exp(-E(z))}{Z}, Z = \int \exp(-E(z)) dz\]</span></p><p>consider a parameterized model <span class="math inline">\(p(z;\theta)\)</span>, we can learn the parameters <spanclass="math inline">\(\theta\)</span> by learning the score function<span class="math inline">\(\nabla_z \log p(z; \theta)\)</span>.</p><p><span class="math display">\[\boxed{\mathcal{L}_{\mathrm{SM}}(\theta) = \mathbb{E}_{p_{data}(x)}\left[\mathrm{Tr}(\nabla_x s_{\theta}(x)) + \frac{1}{2} \| s_{\theta}(x)\|_2^2\right]}\]</span> where <span class="math inline">\(s_{\theta}(x)= \nabla_x \logp(x; \theta)\)</span> is the score function parameterized by <spanclass="math inline">\(\theta\)</span>.</p><hr /><p>In QAOA’s scenario, the energy function is fixed and we want to findthe state <span class="math inline">\(\ket{z^*}\)</span> that maximizesthe energy.</p><p>Let’s define a quantum state <spanclass="math inline">\(\ket{\psi(\theta)} = U(\theta) \ket{0}\)</span>parameterized by <span class="math inline">\(\theta\)</span>.</p><p>We can measure: <span class="math display">\[\langle H_C \rangle_{\theta} = \bra{\psi(\theta)} H_C \ket{\psi(\theta)}\]</span> We want to maximize: <span class="math display">\[J(\theta) = \langle \psi(\theta) | H_C | \psi(\theta) \rangle = \sum_{z\in \{0,1\}^n} P(z|\theta) C(z)\]</span> where <span class="math inline">\(P(z|\theta) = |\langle z |\psi(\theta) \rangle|^2\)</span> is the probability of measuring thestate <span class="math inline">\(\ket{z}\)</span> given the parameter<span class="math inline">\(\theta\)</span>.</p><p>In the first glance, we try to estimiate the log gradient: <spanclass="math display">\[\begin{aligned}\nabla_\theta J(\theta) &amp;= \nabla_\theta \sum_{z} P(z|\theta) C(z)\\&amp;= \sum_{z} C(z) \nabla_\theta P(z|\theta) \\&amp;= \sum_{z} C(z) P(z|\theta) \nabla_\theta \log P(z|\theta) \\&amp;= \mathbb{E}_{z \sim P(z|\theta)} \left[ C(z) \nabla_\theta \logP(z|\theta) \right]\end{aligned}\]</span> This is similar to the REINFORCE learning gradient method withreward <span class="math inline">\(C(z)\)</span>. However, this gradientis not calculable directly on quantum computer since <spanclass="math inline">\(\log P(z|\theta)\)</span> cannot be computedefficiently. (<span class="math inline">\(2^n\)</span> terms). And itdoes not use the result of measure <spanclass="math inline">\(J(\theta)\)</span> directly.</p><p>On the other hand, if we sampling through <spanclass="math display">\[\lim_{\epsilon \to 0} \frac{J(\theta + \epsilon) - J(\theta)}{\epsilon}= \nabla_\theta J(\theta)\]</span> This suffers from high variance issue. The high variance isdue to the high dimensionality of the output space.</p><h2 id="parameter-shift-rule">Parameter Shift Rule</h2><p>Remember that in Denoising Score Matching, we can avoid calculatingthe score function directly by introducing noise and learning todenoise, then the score function can be computed directly. And underGaussian noise, the score function has a closed form.</p><p>This inspires us to introduce a particular parameterization of <spanclass="math inline">\(U(\theta)\)</span> in QAOA such that we cancompute the gradient of <span class="math inline">\(J(\theta)\)</span>directly.</p><p>For <span class="math inline">\(U(\theta) = e^{-i \frac{\theta}{2}G}\)</span>, where <span class="math inline">\(G\)</span> is a Hermitianoperator (generator), we have: <span class="math display">\[\frac{d}{d\theta} U(\theta) = -\frac{i}{2} G U(\theta) = - \frac{i}{2}U(\theta) G\]</span> this uses the property that <spanclass="math inline">\(G\)</span> commutes with <spanclass="math inline">\(U(\theta)\)</span>.</p><p>Thus, <span class="math display">\[\begin{aligned}\nabla_\theta J(\theta) &amp;= \nabla_\theta \langle \psi_0 |U^\dagger(\theta) H_C U(\theta) | \psi_0 \rangle \\= &amp; \langle \psi_0 | \left( \frac{d}{d\theta} U^\dagger(\theta)\right) H_C U(\theta) | \psi_0 \rangle + \langle \psi_0 |U^\dagger(\theta) H_C \left( \frac{d}{d\theta} U(\theta) \right) |\psi_0 \rangle \\= &amp; \langle \psi_0 | \left( \frac{i}{2} U^\dagger(\theta) G \right)H_C U(\theta) | \psi_0 \rangle + \langle \psi_0 | U^\dagger(\theta) H_C\left( -\frac{i}{2} G U(\theta) \right) | \psi_0 \rangle \\= &amp; \frac{i}{2} \langle \psi(\theta) | [G, H_C] | \psi(\theta)\rangle\end{aligned}\]</span> where <span class="math inline">\([G, H_C] = G H_C - H_CG\)</span> is the commutator of <span class="math inline">\(G\)</span>and <span class="math inline">\(H_C\)</span>, <spanclass="math inline">\(\psi(\theta) = U(\theta)\ket{\psi_0}\)</span>.</p><p>Now, note that <span class="math display">\[\begin{aligned}H_+ &amp;= (e^{-i \frac{\pi}{4} G})^\dagger H_C (e^{-i \frac{\pi}{4} G})= e^{i \frac{\pi}{4} G} H_C e^{-i \frac{\pi}{4} G}\\&amp;= \frac{1}{2} (I + iG) H_C (I - iG) \\&amp;= \frac{1}{2} (H_C - i H_C G + i G H_C + G H_C G)\\H_- &amp;= (e^{i \frac{\pi}{4} G})^\dagger H_C (e^{i \frac{\pi}{4} G}) =e^{-i \frac{\pi}{4} G} H_C e^{i \frac{\pi}{4} G}\\&amp;= \frac{1}{2} (H_C + i H_C G - i G H_C + G H_C G)\end{aligned}\]</span> Thus, <span class="math display">\[H_+ - H_- = i (G H_C - H_C G) = i [G, H_C]\]</span> Therefore, we have: <span class="math display">\[\nabla_\theta J(\theta) = \frac{1}{2} \langle \psi(\theta) | (H_+ - H_-)| \psi(\theta) \rangle= \frac{1}{2} \left( J(\theta + \frac{\pi}{2}) - J(\theta -\frac{\pi}{2}) \right)\]</span> This means we can compute the gradient of <spanclass="math inline">\(J(\theta)\)</span> by evaluating <spanclass="math inline">\(J\)</span> at two shifted parameter values,avoiding the need to compute the score function directly.</p><h2 id="parameterized-quantum-circuit-in-qaoa">Parameterized QuantumCircuit in QAOA</h2><p>In QAOA, the parameterized quantum circuit is constructed using twotypes of unitary operators: the cost unitary <spanclass="math inline">\(U_C(\gamma)\)</span> and the mixing unitary <spanclass="math inline">\(U_B(\beta)\)</span>. The overall circuit for <spanclass="math inline">\(p\)</span> layers is given by: <spanclass="math display">\[U(\boldsymbol{\gamma}, \boldsymbol{\beta}) = U_B(\beta_p) U_C(\gamma_p)\ldots U_B(\beta_1) U_C(\gamma_1)\]</span></p><p>The cost unitary is defined as: <span class="math display">\[U_C(\gamma) = e^{-i \gamma H_C}\]</span> and the mixing unitary is defined as: <spanclass="math display">\[U_B(\beta) = e^{-i \beta H_B}\]</span> where <span class="math inline">\(H_B = \sum_{i=1}^nX_i\)</span> with <span class="math inline">\(X_i = I^{\otimes (i-1)}\otimes X \otimes I^{\otimes (n-i)}\)</span> being the Pauli-X operatoracting on qubit <span class="math inline">\(i\)</span>.</p><p>The initial state is typically chosen as the uniform superpositionstate: <span class="math display">\[\ket{\psi_0} = \frac{1}{\sqrt{2^n}} \sum_{z \in \{0,1\}^n} \ket{z}\]</span> which is the ground state of the mixing Hamiltonian <spanclass="math inline">\(H_B\)</span>.</p><p>Reason (Can be omitted, I don’t know very well about this quantumphysics part):</p><blockquote><p>The Adiabatic Theorem suggests that if we vary the parameters <spanclass="math inline">\(\boldsymbol{\gamma}\)</span> and <spanclass="math inline">\(\boldsymbol{\beta}\)</span> slowly enough, thesystem will remain in its ground state, allowing us to approximate theoptimal solution to the original combinatorial optimization problem.<span class="math display">\[H(t) = (1 - \frac{t}{T}) H_B + \frac{t}{T} H_C\]</span> The time evolution operator can be approximated using theTrotter-Suzuki decomposition: <span class="math display">\[e^{-i (A+B) t} \approx \left( e^{-i A \frac{t}{n}} e^{-i B \frac{t}{n}}\right)^n\]</span></p></blockquote><h2 id="algorithm-summary">Algorithm Summary</h2><ol type="1"><li>Initialize the quantum state <spanclass="math inline">\(\ket{\psi_0}\)</span> as the uniform superpositionstate.</li><li>Choose the number of layers <span class="math inline">\(p\)</span>and initialize the parameters <spanclass="math inline">\(\boldsymbol{\gamma},\boldsymbol{\beta}\)</span>.</li><li>Construct the parameterized quantum circuit <spanclass="math inline">\(U(\boldsymbol{\gamma},\boldsymbol{\beta})\)</span>.</li><li>Measure the expectation value <spanclass="math inline">\(J(\boldsymbol{\gamma}, \boldsymbol{\beta}) =\langle \psi(\boldsymbol{\gamma}, \boldsymbol{\beta}) | H_C |\psi(\boldsymbol{\gamma}, \boldsymbol{\beta}) \rangle\)</span>.</li><li>Use the parameter shift rule to compute the gradients <spanclass="math inline">\(\nabla_{\boldsymbol{\gamma}} J\)</span> and <spanclass="math inline">\(\nabla_{\boldsymbol{\beta}} J\)</span>.</li><li>Update the parameters <spanclass="math inline">\(\boldsymbol{\gamma}, \boldsymbol{\beta}\)</span>using a classical optimization algorithm (e.g., gradient descent).</li><li>Repeat steps 3-6 until convergence or a stopping criterion ismet.</li><li>Measure the final state to obtain a bit string <spanclass="math inline">\(z\)</span> that approximates the optimal solutionto the original problem.</li></ol>]]></content>
    
    
    <categories>
      
      <category>Machine Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Optimization</tag>
      
      <tag>Quantum Computing</tag>
      
      <tag>QAOA</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Reading notes of The Principle of Diffusion Models</title>
    <link href="/2025/11/03/Reading-notes-of-The-Principle-of-Diffusion-Models/"/>
    <url>/2025/11/03/Reading-notes-of-The-Principle-of-Diffusion-Models/</url>
    
    <content type="html"><![CDATA[<p>This note is to summarize key points, note down key formulae andgives some discussion for the book “The Principle of Diffusion Models”by Song et al. </p><p><a href="https://www.arxiv.org/pdf/2510.21890">arxiv</a></p><h2 id="deep-generative-modeling">1. Deep Generative Modeling</h2><p>TODO.</p><h2 id="variational-perspective-from-vaes-to-ddpms">2. VariationalPerspective: From VAEs to DDPMs</h2><h3 id="variational-autoencoder">2.1 Variational Autoencoder</h3><h4 id="model-structure">Model structure</h4><p><span class="math display">\[x\xrightarrow{\mathrm{encoder},q_{\theta}(z|x)} z\sim \mathcal{N}(0,1)\xrightarrow{\mathrm{decoder}, p_{\phi}(x|z)} \hat{x}\]</span></p><h4 id="key-formulae">Key formulae</h4><p>The learned data distribution:</p><p><span class="math display">\[p_{\phi}(x) = \mathbb{E}_{p(z)}[p_{\phi}(x|z)] = \intp_{\phi}(x|z)p(z)dz\]</span></p><p>Intractable to compute due to integral over latent variable <spanclass="math inline">\(z\)</span>.</p><p>Maximizing log-likelihood <span class="math inline">\(\iff\)</span>minimizing the KL divergence between true data distribution <spanclass="math inline">\(q(x)\)</span> and learned data distribution <spanclass="math inline">\(p_{\phi}(x)\)</span>:</p><p><span class="math display">\[\begin{aligned}\arg\max_{\phi} \mathbb{E}_{q(x)}[\log p_{\phi}(x)] &amp;\iff\arg\min_{\phi} \mathbb{E}_{q(x)}[\log q(x) - \log p_{\phi}(x)] \\&amp;\iff \arg\min_{\phi} \mathrm{KL}(q(x) || p_{\phi}(x))\end{aligned}\]</span></p><h3 id="evidence-lower-bound-elbo">Evidence Lower Bound (ELBO)</h3><p><strong>Core Idea</strong>:</p><p>We try to estimate <span class="math inline">\(p_{\phi}(x)\)</span>by Bayes’ theorem</p><p><span class="math display">\[p_{\phi}(x) = \frac{p_{\phi}(x,z)}{p_{\phi}(z|x)}\]</span></p><p>Now the posterior <span class="math inline">\(p_{\phi}(z|x)\)</span>is also intractable, so we introduce a variational distribution <spanclass="math inline">\(q_{\theta}(z|x)\)</span> to approximate it.</p><p>Proof of being lower bound of the log-likelihood <spanclass="math inline">\(\log p_{\phi}(x)\)</span>:</p><p><span class="math display">\[\begin{aligned}\log p_{\phi}(x) &amp;= \mathbb{E}_{q_{\theta}(z|x)}\left[\log\frac{p_{\phi}(x,z)}{p_{\phi}(z|x)}\right] \\&amp;= \mathbb{E}_{q_{\theta}(z|x)}\left[\log\frac{p_{\phi}(x,z)}{q_{\theta}(z|x)} \cdot\frac{q_{\theta}(z|x)}{p_{\phi}(z|x)}\right] \\&amp;= \mathbb{E}_{q_{\theta}(z|x)}\left[\log\frac{p_{\phi}(x,z)}{q_{\theta}(z|x)}\right] +\mathbb{E}_{q_{\theta}(z|x)}\left[\log\frac{q_{\theta}(z|x)}{p_{\phi}(z|x)}\right] \\&amp;= \mathbb{E}_{q_{\theta}(z|x)}\left[\log\frac{p_{\phi}(x,z)}{q_{\theta}(z|x)}\right] +\mathrm{KL}(q_{\theta}(z|x) || p_{\phi}(z|x)) \\&amp;\geq \mathbb{E}_{q_{\theta}(z|x)}\left[\log\frac{p_{\phi}(x,z)}{q_{\theta}(z|x)}\right] \\&amp;= \underbrace{\mathbb E_{q_{\theta}(z|x)}\left[\logp_{\phi}(x|z)\right]}_{\text{Reconstruction Term}} - \underbrace{\mathrm{KL}(q_{\theta}(z|x) || p(z))}_{\text{Latent Regularization}}\\&amp;= \mathcal{L}(\theta, \phi; x)\end{aligned}\]</span></p><p><strong>Quick summary</strong>:</p><p>By introducing a latent variable <spanclass="math inline">\(z\)</span> and an inaccurate posteriorapproximation <span class="math inline">\(q_{\theta}(z|x)\)</span>, wederive a lower bound of the log-likelihood <spanclass="math inline">\(\log p_{\phi}(x)\)</span>, which is calledEvidence Lower Bound (ELBO).</p><p>Interpretation 1.</p><p>It is the expectation of the <strong>Bayes-like ratio</strong> <spanclass="math inline">\(\frac{p_{\phi}(x,z)}{q_{\theta}(z|x)}\)</span> byreplacing the intractable true posterior <spanclass="math inline">\(p_{\phi}(z|x)\)</span> with the variationaldistribution <span class="math inline">\(q_{\theta}(z|x)\)</span>.</p><p>Interpretation 2.</p><p>It consists of two terms: a reconstruction term that encourages thedecoder to reconstruct <span class="math inline">\(x\)</span> from <spanclass="math inline">\(z\)</span>, and a latent regularization term thatencourages the encoder distribution <spanclass="math inline">\(q_{\theta}(z|x)\)</span> to be close to the prior<span class="math inline">\(p(z)\)</span>.</p><h3 id="reason-of-blurriness-of-standard-vae">Reason of blurriness ofStandard VAE</h3><p>The standard VAE employs Gaussian distribution for both encoder anddecoder.</p><p><span class="math display">\[\begin{aligned}q_{\theta}(z|x) &amp;= \mathcal{N}(z; \mu_{\theta}(x),\sigma_{\theta}^2(x)I)\\p_{\phi}(x|z) &amp;= \mathcal{N}(x; \mu_{\phi}(z), \sigma^2 I)\end{aligned}\]</span></p><p>The optimal decoder mean <spanclass="math inline">\(\mu_{\phi}(z)\)</span> is the conditionalexpectation <span class="math inline">\(\mathbb{E}[x|z]\)</span>, whichis the average of all possible <span class="math inline">\(x\)</span>that can be mapped to the same <span class="math inline">\(z\)</span>.This averaging effect leads to blurriness in generated samples.</p><p>Derivation of optimal decoder mean:</p><p>First note that <span class="math display">\[\begin{aligned}\mathbb{E}_{q_{\theta}(z|x)}[\log p_{\phi}(x|z)]&amp;= \mathbb{E}_{q_{\theta}(z|x)}\left[-\frac{1}{2\sigma^2}\|x -\mu_{\phi}(z)\|^2 + \text{const}\right] \\&amp;= -\frac{1}{2\sigma^2} \mathbb{E}_{q_{\theta}(z|x)}[\|x -\mu_{\phi}(z)\|^2] + \text{const} \\\end{aligned}\]</span></p><p>And take expectation over <spanclass="math inline">\(p(x)\)</span>:</p><p><span class="math display">\[\begin{aligned}\mathbb{E}_{p(x)}[\mathbb{E}_{q_{\theta}(z|x)}[\log p_{\phi}(x|z)]]&amp;= -\frac{1}{2\sigma^2}\mathbb{E}_{p(x)}[\mathbb{E}_{q_{\theta}(z|x)}[\|x - \mu_{\phi}(z)\|^2]]+ \text{const} \\&amp;= -\frac{1}{2\sigma^2}\mathbb{E}_{q_{\theta}(z)}[\mathbb{E}_{q_{\theta}(x|z)}[\|x -\mu_{\phi}(z)\|^2]] + \text{const} \\\end{aligned}\]</span></p><p>where <span class="math inline">\(q_{\theta}(x|z)= \frac{p(x)q_{\theta}(z|x)}{q_{\theta}(z)}\)</span> is the posterior distributionof <span class="math inline">\(x\)</span> given <spanclass="math inline">\(z\)</span> under the encoder.</p><p>For the inner expectation <spanclass="math inline">\(\mathbb{E}_{q_{\theta}(x|z)}[\|x -\mu_{\phi}(z)\|^2]\)</span>, it is minimized when <spanclass="math inline">\(\mu_{\phi}(z) =\mathbb{E}_{q_{\theta}(x|z)}[x]\)</span>.</p><p><strong>Think</strong>: Which part of the structure of VAE leads toblurriness most?</p><p>Encoder or Decoder?</p><p>Answer:</p><p>It depends on your perspective.</p><p>The encoder mixes different <span class="math inline">\(x\)</span>into the same <span class="math inline">\(z\)</span>, create ambiguity.The Gaussian assumption of <span class="math inline">\(z\)</span>enforces this mixing, otherwise the aggregate posterior <spanclass="math inline">\(q_{\theta}(z)=\int p(x) q_{\theta}(z|x)dx\)</span> cannot match the simple prior <spanclass="math inline">\(p(z)\)</span>.</p><p>The Gaussian decoder reconstructs the average of these ambiguous<span class="math inline">\(x\)</span>, leading to blurriness.</p><h3 id="hierarchical-vae">2.1.5 Hierarchical VAE</h3><h4 id="model-structure-1">Model structure</h4><p><span class="math display">\[x \xLeftrightarrow[\mathrm{decoder}, p_{\phi}(x|z_1)]{\mathrm{encoder},q_{\theta}(z_1|x)} z_1 \xLeftrightarrow[\mathrm{decoder},p_{\phi}(z_1|z_2)]{\mathrm{encoder}, q_{\theta}(z_2|z_1)} z_2\xleftrightarrow{}\cdots \xLeftrightarrow[\mathrm{decoder},p_{\phi}(z_{L-1}|z_L)]{\mathrm{encoder}, q_{\theta}(z_L|z_{L-1})}z_L\]</span></p><h4 id="key-formulae-1">Key formulae</h4><p>Distributions <span class="math display">\[\begin{aligned}p_{\phi}(x, z_{1:L}) &amp;= p_{\phi}(x|z_1) \prod_{i=1}^{L-1}p_{\phi}(z_i|z_{i+1}) p(z_L) \\p_{\phi}(x) &amp;= \int p_{\phi}(x, z_{1:L}) dz_{1:L}\\q_{\theta}(z_{1:L}|x) &amp;= q_{\theta}(z_1|x) \prod_{i=1}^{L-1}q_{\theta}(z_{i+1}|z_i) \\\end{aligned}\]</span></p><h4 id="elbo">ELBO</h4><p><span class="math display">\[\begin{aligned}\log p_{\phi}(x) &amp;\geq \mathbb{E}_{q_{\theta}(z_{1:L}|x)}\left[\log\frac{p_{\phi}(x, z_{1:L})}{q_{\theta}(z_{1:L}|x)}\right] \\&amp;= \mathbb{E}_{q_{\theta}(z_{1:L}|x)}\left[\log\frac{p_{\phi}(x|z_1) \prod_{i=2}^{L} p_{\phi}(z_{i-1}|z_{i})p(z_L)}{q_{\theta}(z_1|x) \prod_{i=2}^{L}q_{\theta}(z_{i}|z_{i-1})}\right] \\&amp;= E_q\left[\log p_{\phi}(x|z_1)\right] -E_q\left[\mathrm{KL}(q_{\theta}(z_L|z_{L-1}) || p(z_L))\right] -\sum_{i=2}^{L-1} E_q\left[\mathrm{KL}(q_{\theta}(z_{i}|z_{i-1}) ||p_{\phi}(z_{i}|z_{i+1}))\right] - E_q\left[\mathrm{KL}(q_{\theta}(z_1|x)|| p_{\phi}(z_1|z_2))\right] \\\end{aligned}\]</span> where <span class="math inline">\(\mathbb E_q\)</span> means<span class="math inline">\(\mathbb E_{p(x)q_{\theta}(z_{1:L}|x)}\)</span>.</p><h3 id="denoising-diffusion-probabilistic-models">2.2 DenoisingDiffusion Probabilistic Models</h3><h4 id="model-structure-2">Model structure</h4><p><span class="math display">\[x_0 \xLeftrightarrow[\mathrm{denoising},p_{\phi}(x_{0}|x_1)]{\mathrm{add \ noise}, q(x_1|x_{0})} x_1\xLeftrightarrow[\mathrm{denoising}, p_{\phi}(x_{1}|x_2)]{\mathrm{add\noise}, q(x_2|x_{1})} x_2 \xLeftrightarrow{}\cdots\xLeftrightarrow[\mathrm{denoising}, p_{\phi}(x_{T-1}|x_T)]{\mathrm{add\noise}, q(x_{T-1}|x_T)} x_T\]</span></p><p><span class="math display">\[\begin{aligned}p(x_i|x_{i-1}) &amp;= \mathcal{N}(x_i; \sqrt{1-\beta_i^2} x_{i-1},\beta_i^2 I) \\x_i &amp;= \alpha_i x_{i-1} + \beta_i \epsilon_{i}, \quad \epsilon_{i}\sim \mathcal{N}(0, I) \\p_i(x_i|x_0) &amp;= \mathcal{N}(x_i; \bar{\alpha}_i x_0,(1-\bar{\alpha}_i^2) I) , \quad \bar{\alpha}_i = \prod_{k=1}^{i}\sqrt{1-\beta_k^2} = \prod_{k=1}^{i} \alpha_k\\x_i &amp;= \bar{\alpha}_i x_0 + \sqrt{1-\bar{\alpha}^2_i} \epsilon,\quad \epsilon \sim \mathcal{N}(0, I) \\\end{aligned}\]</span></p><p>Here “=” means equality in distribution.</p><p><strong>Note</strong>: &gt; The DDPM paper uses <spanclass="math inline">\(\sqrt{1-\beta_i}\)</span> instead of <spanclass="math inline">\(\sqrt{1-\beta_i^2}\)</span>, so whenever see <spanclass="math inline">\(\beta_i\)</span> in this note, it means the one inDDPM paper squared. And the <span class="math inline">\(\bar\alpha_i\)</span> here is also different from DDPM paper bysquaring.</p><h4 id="idea">Idea</h4><p>The forward process gradually adds Gaussian noise to the data <spanclass="math inline">\(x_0\)</span> until it is nearly pure Gaussiannoise <span class="math inline">\(x_T\sim \mathcal{N}(0,I)\)</span>.</p><p>We want to learn the reverse denoising process <spanclass="math inline">\(p_{\phi}(x_{i-1}|x_i)\)</span> to recover datafrom noise.</p><p><span class="math display">\[\begin{aligned}\mathbb E_{p_i(x_i)} \left[\mathrm{KL}(p_i(x_{i-1}|x_i) ||p_{\phi}(x_{i-1}|x_i))\right] &amp;= \int p_i(x_i) \int p_i(x_{i-1}|x_i)\log \frac{p_i(x_{i-1}|x_i)}{p_{\phi}(x_{i-1}|x_i)} dx_{i-1} dx_i\\&amp;= \int \int p_i(x_i | x_{i-1}) p_i(x_{i-1}) \log\frac{p_i(x_{i}|x_{i-1})p(x_{i-1})}{p_{\phi}(x_{i-1}|x_i)p(x_i)}dx_{i-1} dx_i\\\end{aligned}\]</span></p><p>But estimate <span class="math inline">\(p_i(x_i) = \int p_i(x_i|x_0)p(x_0) dx_0\)</span> is intractable.</p><p>We turn to <span class="math display">\[p(x_{i-1}|x_i, x_0) = \frac{p_i(x_i|x_{i-1}, x_0)p_i(x_{i-1}|x_0)}{p_i(x_i|x_0)} = \frac{p_i(x_i|x_{i-1})p(x_{i-1}|x_0)}{p_i(x_i|x_0)}\]</span> which is tractable since all distributions are Gaussian.</p><p>Then we have <span class="math display">\[\begin{aligned}&amp;\mathbb E_{p_i(x_i)} [\mathrm{KL}(p_i(x_{i-1}|x_i) ||p_{\phi}(x_{i-1}|x_i))] \\= &amp;\mathbb E_{p(x_0) p_i(x_i|x_0)} [\mathrm{KL}(p_i(x_{i-1}|x_i,x_0) || p_{\phi}(x_{i-1}|x_i))] + C\end{aligned}\]</span></p><p><strong>Claim</strong>:</p><blockquote><p>The minimizer of both is <spanclass="math inline">\(p(x_{i-1}|x_i)\)</span>.</p></blockquote><p><strong>Proof</strong>: <span class="math display">\[\begin{aligned}\mathbb E_{p(x_0, x_i)} [\mathrm{KL}(p_i(x_{i-1}|x_i, x_0) ||p_{\phi}(x_{i-1}|x_i))] &amp;= \int \int \int p(x_0, x_i) p(x_{i-1}|x_i,x_0) \log \frac{p(x_{i-1}|x_i, x_0)}{p_{\phi}(x_{i-1}|x_i)} dx_{i-1}dx_i dx_0 \\&amp;= \int p(x_i) \int p(x_0|x_i) \int p(x_{i-1}|x_i, x_0) \log\frac{p(x_{i-1}|x_i, x_0)}{p_{\phi}(x_{i-1}|x_i)} dx_{i-1} dx_0 dx_i \\&amp;= \mathbb E_{p(x_i)} \left[\mathbb E_{p(x_0|x_i)}\left[\mathbbE_{p(x_{i-1}|x_i,x_0)}\left[\log\frac{p(x_{i-1}|x_i,x_0)}{p_{\phi}(x_{i-1}|x_i)}\right]\right]\right]\\&amp;= \mathbb E_{p(x_i)} \left[\mathbb E_{p(x_0|x_i)}\left[\mathbbE_{p(x_{i-1}|x_i,x_0)}\left[\log\frac{p(x_{i-1}|x_i,x_0)}{p(x_{i-1}|x_i)}\right] \right]\right] +\mathbb E_{p(x_i)} \left[\mathbb E_{p(x_0|x_i)}\left[\mathbbE_{p(x_{i-1}|x_i,x_0)}\left[\log\frac{p(x_{i-1}|x_i)}{p_{\phi}(x_{i-1}|x_i)}\right] \right]\right]\\&amp;= \mathbb E_{p(x_i)} \left[\mathbbE_{p(x_0|x_i)}\left[\mathrm{KL}(p(x_{i-1}|x_i,x_0) ||p(x_{i-1}|x_i))\right]\right] + \mathbb E_{p(x_i)}\left[\mathrm{KL}(p(x_{i-1}|x_i) || p_{\phi}(x_{i-1}|x_i))\right]\\\end{aligned}\]</span></p><p>Note the first term is independent of <spanclass="math inline">\(p_{\phi}\)</span>, so minimizing the wholeexpression is equivalent to minimizing the second term.</p><p>And the second term is minimized when <spanclass="math inline">\(p_{\phi}(x_{i-1}|x_i) = p(x_{i-1}|x_i) =\mathbb{E}_{p(x_0|x_i)}[p(x_{i-1}|x_i, x_0)]\)</span>.</p><p>The above argument shows that minimizing the KL divergence betweenmarginal distributions is mathematically identical to minimizing the KLdivergence between specific conditional distributions.</p><p>This is very powerful and we will see similar conditional techniquesagain in flow-based models.</p><h4 id="closed-form-of-px_i-1x_i-x_0">Closed form of <spanclass="math inline">\(p(x_{i-1}|x_i, x_0)\)</span>:</h4><p>By Bayes’ theorem and Markov property,</p><p><span class="math display">\[p(x_{i-1}|x_i, x_0) = \frac{p_i(x_i|x_{i-1}, x_0)p_i(x_{i-1}|x_0)}{p_i(x_i|x_0)} = \frac{p_i(x_i|x_{i-1})p(x_{i-1}|x_0)}{p_i(x_i|x_0)} \propto p_i(x_i|x_{i-1}) p_i(x_{i-1}|x_0)\]</span></p><p><span class="math display">\[\begin{aligned}p_i(x_i|x_{i-1}) &amp;= \mathcal{N}(x_i; \sqrt{1-\beta_i^2} x_{i-1},\beta_i^2 I) \propto \mathcal{N}(x_{i-1}; \frac{1}{\sqrt{1-\beta_i^2}}x_i, \frac{\beta_i^2}{1-\beta_i^2} I) \\p_i(x_{i-1}|x_0) &amp;= \mathcal{N}(x_{i-1}; \bar{\alpha}_{i-1} x_0,(1-\bar{\alpha}_{i-1}^2) I) \\\end{aligned}\]</span></p><p>Formulas of Gaussian multiplication give <spanclass="math display">\[\begin{aligned}\Sigma^{-1} &amp;= \Sigma_1^{-1} + \Sigma_2^{-1}\\\mu &amp;= \Sigma(\Sigma_1^{-1} \mu_1 + \Sigma_2^{-1} \mu_2)\end{aligned}\]</span></p><p>So we have <span class="math display">\[\begin{aligned}\sigma^2 &amp;= \left(\frac{1-\beta_i^2}{\beta_i^2} +\frac{1}{1-\bar{\alpha}_{i-1}^2}\right)^{-1} \\&amp;= \frac{\beta_i^2 (1-\bar{\alpha}_{i-1}^2)}{\beta_i^2 +(1-\beta_i^2)(1-\bar{\alpha}_{i-1}^2)}\\&amp;= \beta_i^2 \frac{1-\bar{\alpha}_{i-1}^2}{1-\bar{\alpha}_i^2} \\\mu &amp;= \sigma^2 \left(\frac{1-\beta_i^2}{\beta_i^2} \cdot\frac{1}{\sqrt{1-\beta_i^2}} x_i + \frac{1}{1-\bar{\alpha}_{i-1}^2}\cdot \bar{\alpha}_{i-1} x_0\right)\\&amp;= \frac{\sqrt{1-\beta_i^2}(1-\bar{\alpha}_{i-1}^2)}{1-\bar{\alpha}_i^2} x_i + \frac{\beta_i^2\bar{\alpha}_{i-1}}{1-\bar{\alpha}_i^2} x_0\\&amp;= \frac{\alpha_i (1-\bar{\alpha}_{i-1}^2)}{1-\bar{\alpha}_i^2} x_i+ \frac{\bar{\alpha}_{i-1}\beta_i^2}{1-\bar{\alpha}_i^2} x_0\\\end{aligned}\]</span></p><p>Thus, we have <span class="math display">\[p(x_{i-1}|x_i, x_0) = \mathcal{N}\left(x_{i-1}; \frac{\alpha_i(1-\bar{\alpha}_{i-1}^2)}{1-\bar{\alpha}_i^2} x_i +\frac{\bar{\alpha}_{i-1}\beta_i^2}{1-\bar{\alpha}_i^2} x_0, \beta_i^2\frac{1-\bar{\alpha}_{i-1}^2}{1-\bar{\alpha}_i^2} I\right)\]</span></p><p>And the loss function for training DDPM is the sum of KL divergences:<span class="math display">\[\begin{aligned}\mathcal{L}_{\mathrm{DDPM}} &amp;= \sum_{i=1}^{T} \mathbb E_{p(x_0)p_i(x_i|x_0)} [\mathrm{KL}(p(x_{i-1}|x_i, x_0) ||p_{\phi}(x_{i-1}|x_i))] \\&amp;= \sum_{i=1}^{T} \frac{1}{\sigma_i^2}\mathbb E_{p(x_0)p_i(x_i|x_0)} \left[\| \mu_i(x_i, x_0, i) - \mu_{\phi}(x_i,i)\|^2\right]+ C\\\end{aligned}\]</span> where <span class="math inline">\(\mu_i(x_i, x_0, i)\)</span>is the mean of <span class="math inline">\(p(x_{i-1}|x_i, x_0)\)</span>derived above and <span class="math inline">\(\mu_{\phi}(x_i,i)\)</span>is the predicted mean by the neural network.</p><p><strong>Reparameterization trick</strong>:</p><p>It is also common to parameterize the denoising model to predict thenoise <span class="math inline">\(\epsilon\)</span> added to <spanclass="math inline">\(x_0\)</span> instead of predicting the mean <spanclass="math inline">\(\mu_{\phi}(x_i,i)\)</span> directly since we havethe relation <span class="math display">\[x_i = \bar{\alpha}_i x_0 + \sqrt{1-\bar{\alpha}_i^2} \epsilon\]</span></p><h4 id="elbo-1">ELBO</h4><p><span class="math display">\[p_{\phi}(x_0) = \int p_{\phi}(x_0, x_{1:T}) dx_{1:T} = \intp_{\phi}(x_0|x_1) \prod_{i=2}^{T} p_{\phi}(x_{i-1}|x_i) p(x_T) dx_{1:T}\]</span></p><p>To make it ELBO, we introduce the forward process <spanclass="math inline">\(q(x_{1:T}|x_0)\)</span> as the variationaldistribution to approximate the intractable true posterior <spanclass="math inline">\(p_{\phi}(x_{1:T}|x_0)\)</span> and expand it bythe conditional posterior:</p><p><span class="math display">\[\log p_{\phi}(x_0) \geq \mathbb{E}_{q(x_{1:T}|x_0)}\left[\log\frac{p_{\phi}(x_0, x_{1:T})}{p(x_{1:T}|x_0)}\right]\]</span> Use <span class="math display">\[p(x_{1:T}|x_0) = p(x_T|x_0) \prod_{i=2}^{T} p(x_{i-1}|x_{i}, x_0)\]</span></p><p><span class="math display">\[\begin{aligned}\mathcal{L}_{\mathrm{ELBO}}&amp;= \mathbb{E}_{p(x_{1:T}|x_0)}\left[\logp_{\phi}(x_0|x_1) + \sum_{i=2}^{T} \log p_{\phi}(x_{i-1}|x_i) + \logp(x_T) - \sum_{i=1}^{T} \log q(x_i|x_{i-1}) - \log p(x_T|x_0)\right] \\&amp;= \underbrace{\mathbb{E}_{p(x_1|x_0)} \left[\logp_{\phi}(x_0|x_1)\right]}_{\mathrm{Reconstruction}} +\underbrace{\mathbb{E}_{p(x_T|x_0)} \left[\log p(x_T) - \logp(x_T|x_0)\right]}_{\mathrm{Prior}} + \underbrace{\sum_{i=2}^{T}\mathbb{E}_{p(x_{i}|x_0)} \left[\mathrm{KL}(p(x_{i-1}|x_i, x_0) ||p_{\phi}(x_{i-1}|x_i))\right]}_{\mathrm{Diffusion}} \\\end{aligned}\]</span></p><h4 id="features-of-ddpm">Features of DDPM</h4><p>Let’s consider <span class="math inline">\(x\)</span>-predictionsince it is easier to understand.</p><p>Assume we have the optimal denoising model in each step, i.e., <spanclass="math inline">\(p_{\phi}(x_{i-1}|x_i) = p(x_{i-1}|x_i)\)</span>,then we can recover <span class="math inline">\(x_0\)</span>perfectly.</p><p>But the reality is we approximate the denoising step with a Gaussiandistribution, so the optimal posterior mean <spanclass="math inline">\(\mu_i(x_i, i)\)</span> is still an average of allpossible <span class="math inline">\(x_{i-1}\)</span> that can be mappedto the same <span class="math inline">\(x_i\)</span>.</p><p>Claim:</p><p>Although we circumvent predicting <spanclass="math inline">\(x_{T-1}\)</span> directly by predicting <spanclass="math inline">\(x_0\)</span> and then computing the posteriordistribution. The optimal denoising mean <spanclass="math inline">\(\mu_i(x_i, x_0, i)\)</span> is still the averageof all possible <span class="math inline">\(x_{i-1}\)</span> that can bemapped to the same <span class="math inline">\(x_i\)</span>.</p><p><strong>Proof</strong>: <span class="math display">\[\begin{aligned}\mathbb{E}_{p(x_{t-1}|x_t)}[x_{t-1}] &amp;=\mathbb{E}_{p(x_0|x_t)}[\mathbb{E}_{p(x_{t-1}|x_t, x_0)}[x_{t-1}]] \\&amp;= \mathbb{E}_{p(x_0|x_t)}[\mu_t(x_t, x_0, t)] \\&amp;= \mathbb{E}_{p(x_0|x_t)}[a_t x_t + b_t x_0] \\&amp;= a_t x_t + b_t \mathbb{E}_{p(x_0|x_t)}[x_0] \\&amp;= \mu_t(x_t, \mathbb{E}_{p(x_0|x_t)}[x_0], t) \\\end{aligned}\]</span> where <span class="math inline">\(a_t = \frac{\alpha_t(1-\bar{\alpha}_{t-1}^2)}{1-\bar{\alpha}_t^2}\)</span> and <spanclass="math inline">\(b_t =\frac{\bar{\alpha}_{t-1}\beta_t^2}{1-\bar{\alpha}_t^2}\)</span>.</p><p>So we can just discuss it as the standard VAE/HVAE case.</p><p>A more theoretical analysis will be discussed when the<strong>differential equation perspective</strong> is introducedlater.</p><h2 id="score-based-perspective-from-ebms-to-ncsn">3. Score-BasedPerspective: From EBMs to NCSN</h2><h3 id="energy-based-models">3.1 Energy-Based Models</h3><p>EBMs define a distribution through an energy function <spanclass="math inline">\(E_{\phi}(x)\)</span>: <spanclass="math display">\[p_{\phi}(x) = \frac{\exp(-E_{\phi}(x))}{Z_{\phi}}, \quad Z_{\phi} = \int\exp(-E_{\phi}(x)) dx\]</span> where the <span class="math inline">\(Z_{\phi}\)</span> is thepartition function that normalizes the distribution.</p><p>When we lower the energy of a region, the probability of that regionincreases, and its complement regions decrease in probability.</p><blockquote><p>Probability mass is redistributed across the entire space rather thanassigned independently to each region.</p></blockquote><p><strong>Training through MLE</strong></p><p><span class="math display">\[\begin{aligned}\mathcal{L}_{\mathrm{MLE}}(\phi) &amp;= \mathbb{E}_{q(x)}[\logp_{\phi}(x)] \\&amp;= \mathbb{E}_{q(x)}[-E_{\phi}(x)] - \log Z_{\phi} \\&amp;= \mathbb{E}_{q(x)}[-E_{\phi}(x)] - \log \int \exp(-E_{\phi}(x)) dx\\\end{aligned}\]</span></p><p>Intractable due to the partition function <spanclass="math inline">\(Z_{\phi}\)</span>, which requires integrating overthe entire data space.</p><h4 id="score-function">Score function</h4><p>For a density <span class="math inline">\(p(x)\)</span>, its scorefunction is defined as the gradient of the log-density with respect to<span class="math inline">\(x\)</span>:</p><p><span class="math display">\[s(x):= \nabla_x \log p(x),\quad \mathbb R^D \to \mathbb R^D\]</span></p><figure><img src="ScoreVector.png" alt="Vector Field of Score Function" /><figcaption aria-hidden="true">Vector Field of ScoreFunction</figcaption></figure><p>Score vector field points toward regions of higher data density.</p><p>Calculate the score function of EBM is irrelevant to the partitionfunction: <span class="math display">\[\begin{aligned}s_{\phi}(x) &amp;= \nabla_x \log p_{\phi}(x) \\&amp;= \nabla_x [-E_{\phi}(x) - \log Z_{\phi}] \\&amp;= -\nabla_x E_{\phi}(x) \\\end{aligned}\]</span></p><p>And one can recover the density from the score function up to aconstant: <span class="math display">\[\log p(x) = \log p(x_0) + \int_{0}^{1} s(x_0+ t(x - x_0))^{\top} (x -x_0) dt\]</span></p><p><strong>Training EBM through Score Matching</strong></p><p><span class="math display">\[\mathcal{L}_{\mathrm{SM}}(\phi) = \mathbb{E}_{p_{data}(x)} \left[\|\nabla_x \log p_{\phi}(x) - \nabla_x \log p_{data}(x) \|_2^2\right]\]</span></p><p>And integration by parts gives an equivalent form that does notrequire <span class="math inline">\(\nabla_x \log p_{data}(x)\)</span>:<span class="math display">\[\mathcal{L}_{\mathrm{SM}}(\phi) = \mathbb{E}_{p_{data}(x)}\left[\mathrm{Tr}(\nabla_x^2 E_{\phi}(x)) + \frac{1}{2} \| \nabla_xE_{\phi}(x) \|_2^2\right]\]</span> However, computing the Hessian trace <spanclass="math inline">\(\mathrm{Tr}(\nabla_x^2 E_{\phi}(x))\)</span> iscomputationally expensive for high-dimensional data.</p><h4 id="langevin-sampling-with-score-function">Langevin Sampling withScore Function</h4><p>Without the partition function, we cannot directly sample from EBM.Instead, Langevin dynamics is used to generate samples.</p><figure><img src="LangevinDynamics.png" alt="Langevin Dynamics Sampling" /><figcaption aria-hidden="true">Langevin Dynamics Sampling</figcaption></figure><p><strong>Discrete-time Langevin dynamics</strong>: <spanclass="math display">\[\mathbf{x}_{n+1} = \mathbf{x}_n - \eta \nabla_{\mathbf{x}}E_{\phi}(\mathbf{x}_n) + \sqrt{\eta} \mathbf{\epsilon}_n, \quad\mathbf{\epsilon}_n \sim \mathcal{N}(0, I)\]</span> where <span class="math inline">\(\eta&gt;0\)</span> is thestep size.</p><p>Write in terms of score function: <span class="math display">\[\mathbf{x}_{n+1} = \mathbf{x}_n + \eta \nabla_{\mathbf{x}} \logp_{\phi}(\mathbf{x}_n) + \sqrt{\eta} \mathbf{\epsilon}_n\]</span></p><p><strong>Continuous-time Langevin dynamics</strong>: <spanclass="math display">\[d\mathbf{x}(t) = \nabla_{\mathbf{x}} \log p_{\phi}(\mathbf{x}(t)) dt +\sqrt{2} d\mathbf{w}(t)\]</span> where <span class="math inline">\(\mathbf{w}(t)\)</span> is aWiener process.</p><p><span class="math inline">\(\sqrt{2}\)</span> is used to ensure thestationary distribution is <spanclass="math inline">\(p_{\phi}(x)\)</span>. This will be explained inthe differential equation appendix.</p><p>The intution is that the score term <spanclass="math inline">\(\nabla_{\mathbf{x}} \log p_{\phi}(\mathbf{x}(t))dt\)</span> pushes the sample toward high-density regions, while thenoise term <span class="math inline">\(\sqrt{2} d\mathbf{w}(t)\)</span>adds randomness to explore the space.</p><p>But Langevin dynamics is still struggling to sample from complex datadistribution with <strong>many isolated modes</strong>, where itrequires extremely long time to traverse low-density regions betweenmodes.</p><h3 id="from-energy-based-to-score-based-generative-models">3.2 FromEnergy-Based to Score-Based Generative Models</h3><p>Once we have the score function, we can sample from the EBM usingLangevin dynamics without computing the partition function. Therefore,we turn to directly learn the score function from data, leading toscore-based generative models (SGMs).</p><p><span class="math display">\[\boxed{\mathcal{L}_{\mathrm{SM}}(\phi) = \mathbb{E}_{p_{data}(x)} \left[\|s_{\phi}(x) - s_{data}(x) \|_2^2\right]}\]</span></p><p>The tractable score matching loss is <span class="math display">\[\boxed{\mathcal{L}_{\mathrm{SM}}(\phi) = \mathbb{E}_{p_{data}(x)}\left[\mathrm{Tr}(\nabla_x s_{\phi}(x)) + \frac{1}{2} \| s_{\phi}(x)\|_2^2\right]}\]</span></p><hr /><p><strong>Proof of equivalence:</strong></p><p><span class="math display">\[\begin{aligned}\mathcal{L}_{\mathrm{SM}}(\phi) &amp;= \mathbb{E}_{p_{data}(x)} \left[\|s_{\phi}(x)\|_2^2\right] - 2 \mathbb{E}_{p_{data}(x)}\left[s_{\phi}(x)^{\top} s_{data}(x)\right] + \mathbb{E}_{p_{data}(x)}\left[\| s_{data}(x) \|_2^2\right] \\\end{aligned}\]</span></p><p>Focus on the second term: <span class="math display">\[\begin{aligned}\mathbb{E}_{p_{data}(x)} \left[s_{\phi}(x)^{\top} s_{data}(x)\right]&amp;= \int p_{data}(x) s_{\phi}(x)^{\top} \nabla_x \log p_{data}(x) dx\\&amp;= \int s_{\phi}(x)^{\top} \nabla_x p_{data}(x) dx \\&amp;= \int \sum_{i=1}^{D} s_{\phi,i}(x) \frac{\partialp_{data}(x)}{\partial x_i} dx \\&amp;= \sum_{i=1}^{D} \left[ s_{\phi,i}(x) p_{data}(x)\Big|_{x_i=-\infty}^{x_i=+\infty} - \int p_{data}(x) \frac{\partials_{\phi,i}(x)}{\partial x_i} dx \right] \\&amp;= - \mathbb{E}_{p_{data}(x)} \left[\mathrm{Tr}(\nabla_xs_{\phi}(x))\right] \\\end{aligned}\]</span></p><p>The boundary term vanishes assuming <spanclass="math inline">\(p_{data}(x)\)</span> decays to zero atinfinity.</p><p>Note the third term is independent of <spanclass="math inline">\(\phi\)</span>, so we have the equivalence. <spanclass="math inline">\(\quad\square\)</span></p><hr /><h3 id="denoising-score-matching">3.3 Denoising Score Matching</h3><p>Although the tractable score matching loss avoids computing thepartition function, it still requires calculating the Hessian trace<span class="math inline">\(\mathrm{Tr}(\nabla_x s_{\phi}(x))\)</span>,which is computationally expensive for high-dimensional data at <spanclass="math inline">\(O(D^2)\)</span> complexity.</p><h4 id="sliced-score-matching">Sliced Score Matching</h4><p>Let <span class="math inline">\(\mathbf{u}\)</span> be an isotropicrandom vector, i.e., <spanclass="math inline">\(\mathbb{E}[\mathbf{u}\mathbf{u}^{\top}] = I,\mathbb{E}[\mathbf{u}] = 0\)</span>.</p><p>Then we have <span class="math display">\[\begin{aligned}\mathrm{Tr}(A) &amp;= \mathbb{E}[\mathbf{u}^{\top} A \mathbf{u}] \\\mathbb{E}[(\mathbf{u}^{\top} s_{\phi}(x))^2] &amp;= \| s_{\phi}(x)\|_2^2 \\\end{aligned}\]</span></p><p>So the score matching loss can be rewritten as <spanclass="math display">\[\tilde{\mathcal{L}}_{\mathrm{SM}}(\phi) = \mathbb{E}_{x,u}\left[\mathbf{u}^{\top} \nabla_x s_{\phi}(x) \mathbf{u} + \frac{1}{2}(\mathbf{u}^{\top} s_{\phi}(x))^2\right]\]</span></p><p>This can be computed using the Jacobian-vector product (JVP)technique, which only requires <span class="math inline">\(O(D)\)</span>complexity.</p><p>Note the method only control the score function along randomdirections at observed data points, providing weak control in theirneighborhood.</p><h4 id="denoising-score-matching-dsm">Denoising Score Matching(DSM)</h4><p>Sliced score matching still need to compute the Jacobian-vectorproduct. And the random directions introduce variance inoptimization.</p><p>Vincent et al. (2011) proposed denoising score matching (DSM) toavoid computing the Hessian trace.</p><p>The idea is to add noise to data and learn the score function of thenoisy data distribution.</p><p><span class="math display">\[p_{\sigma}(\mathbf{\tilde x}) = \int p_{data}(\mathbf{x})p_{\sigma}(\mathbf{\tilde x}|\mathbf{x}) d\mathbf{x}\]</span></p><p><span class="math display">\[\mathcal{L}_{\mathrm{SM}}(\phi; \sigma) = \mathbb{E}_{\mathbf{\tilde{x}}\sim p_{\sigma}} \left[\| s_{\phi}(\mathbf{\tilde{x}}; \sigma) -\nabla_{\mathbf{\tilde{x}}} \log p_{\sigma}(\mathbf{\tilde{x}})\|_2^2\right]\]</span></p><p>This is equivalent to the denoising objective:</p><p><span class="math display">\[\mathcal{L}_{\mathrm{DSM}}(\phi; \sigma) = \mathbb{E}_{\mathbf x \simp_{data}(x),\mathbf{\tilde x} \sim p_{\sigma}(\cdot |x)} \left[\|s_{\phi}(\mathbf{\tilde x}) - \nabla_{\mathbf{\tilde x}} \logp_{\sigma}(\mathbf{\tilde x}|\mathbf{x}) \|_2^2\right]\]</span></p><p>Under Gaussian noise corruption <spanclass="math inline">\(p_{\sigma}(\mathbf{\tilde x}|\mathbf{x}) =\mathcal{N}(\mathbf{\tilde x}; \mathbf{x}, \sigma^2 I)\)</span>, we have<span class="math inline">\(\nabla_{\mathbf{\tilde x}} \logp_{\sigma}(\mathbf{\tilde x}|\mathbf{x}) = \frac{1}{\sigma^2}(\mathbf{x} - \mathbf{\tilde x})\)</span>.</p><p>The loss becomes</p><p><span class="math display">\[\begin{aligned}\mathcal{L}_{\mathrm{DSM}}(\phi; \sigma) &amp;= \mathbb{E}_{\mathbf x\sim p_{data}(x),\mathbf{\tilde x} \sim \mathcal{N}(\mathbf{x}, \sigma^2I)} \left[\| s_{\phi}(\mathbf{\tilde x}) - \frac{1}{\sigma^2}(\mathbf{x} - \mathbf{\tilde x}) \|_2^2\right]\\&amp;=\mathbb{E}_{\mathbf x \sim p_{data}(x),\epsilon \sim\mathcal{N}(0, I)} \left[\| s_{\phi}(\mathbf{x} + \sigma \epsilon) +\frac{\epsilon}{\sigma} \|_2^2\right]\\\end{aligned}\]</span></p><h4 id="tweedies-formula">Tweedie’s Formula</h4><p>Assume <span class="math inline">\(\mathbf{x} \sim p_{data}(\mathbfx)\)</span> and <span class="math inline">\(\mathbf{\tilde x}|\mathbf{x}\sim \mathcal{N}(\ \cdot \ ;\alpha\mathbf{x}, \sigma^2 I)\)</span>.</p><p>Then we have: <span class="math display">\[\alpha \mathbb{E}_{\mathbf x \sim p(\mathbf x | \mathbf{\tildex})}[\mathbf{x}|\mathbf{\tilde x}] = \mathbf{\tilde x} + \sigma^2\nabla_{\mathbf{\tilde x}} \log p_{\sigma}(\mathbf{\tilde x})\]</span> where the expectation is over the posterior distribution <spanclass="math inline">\(p(\mathbf x | \mathbf{\tilde x}) =\frac{p_{data}(\mathbf x) p_{\sigma}(\mathbf{\tildex}|\mathbf{x})}{p_{\sigma}(\mathbf{\tilde x})}\)</span>.</p><p>The proof is done by computing <spanclass="math inline">\(\nabla_{\mathbf{\tilde x}}\logp_{\sigma}(\mathbf{\tilde x})\)</span> using Bayes’ theorem.</p><p>Higher cumulants of the posterior can also be computed through higherderivatives of <span class="math inline">\(\logp_{\sigma}(\mathbf{\tilde x})\)</span>.</p><h4 id="higher-order-tweedies-formula">Higher Order Tweedie’sFormula</h4><p>Like <span class="math inline">\(\sigma\)</span>, assume theconditional law of <span class="math inline">\(\mathbf{\tildex}|\mathbf{x}\)</span> belongs to an exponential family with naturalparameter <span class="math inline">\(\eta\)</span>: <spanclass="math display">\[q(\mathbf{\tilde x}|\mathbf{\eta}) = q_0(\mathbf{\tilde x})\exp(\eta^{\top} \mathbf{\tilde x} - \psi(\mathbf{\eta}))\]</span></p><p>For Gaussian noise with variance <span class="math inline">\(\sigma^2\mathbf I\)</span>, <span class="math inline">\(\mathbf\eta =\frac{\mathbf x}{\sigma^2}\)</span>, <spanclass="math inline">\(q_0(\mathbf{\tilde x}) = \frac{1}{(2\pi\sigma^2)^{D/2}} \exp(-\frac{\|\mathbf{\tildex}\|^2}{2\sigma^2})\)</span>.</p><p><span class="math display">\[\psi(\mathbf{\eta}) = \log \int q_0(\mathbf{\tilde x}) \exp(\eta^{\top}\mathbf{\tilde x}) d\mathbf{\tilde x} = \frac{\sigma^2}{2}\|\mathbf{\eta}\|^2\]</span></p><p>is the log-partition function of the <spanclass="math inline">\(q(\mathbf{\tilde x}|\mathbf{\eta})\)</span>.</p><p>The observed data distribution is <span class="math display">\[p_{\eta}(\mathbf{\tilde x}) = \int p(\mathbf \eta) q(\mathbf{\tildex}|\mathbf{\eta}) d\mathbf{\eta}\]</span></p><p>Define the log-partition function of <spanclass="math inline">\(p_{\eta}(\mathbf{\tilde x})\)</span> as</p><p><span class="math display">\[\lambda(\mathbf{\tilde x}) = \log p_{\mathbf{\eta}}(\mathbf{\tilde x}) -\log q_0(\mathbf{\tilde x}).\]</span></p><p>Then the posterior of <spanclass="math inline">\(\mathbf{\eta}\)</span> given <spanclass="math inline">\(\mathbf{\tilde x}\)</span> is <spanclass="math display">\[p(\mathbf{\eta}|\mathbf{\tilde x}) = \exp(\eta^{\top} \mathbf{\tilde x}- \psi(\mathbf{\eta}) - \lambda(\mathbf{\tilde x}))p(\mathbf{\eta}).\]</span></p><p>By Bayes’ theorem, <span class="math display">\[p(\mathbf{\eta}|\mathbf{\tilde x}) = \frac{p(\mathbf{\eta})q(\mathbf{\tilde x}|\mathbf{\eta})}{p_{\eta}(\mathbf{\tilde x})} \proptop(\mathbf{\eta}) q(\mathbf{\tilde x}|\mathbf{\eta}) = p(\mathbf{\eta})q_0(\mathbf{\tilde x}) \exp(\eta^{\top} \mathbf{\tilde x} -\psi(\mathbf{\eta})) = p(\mathbf{\eta}) \exp(\eta^{\top} \mathbf{\tildex} - \psi(\mathbf{\eta}) - \lambda(\mathbf{\tilde x}))\]</span></p><p>The <span class="math inline">\(\lambda\)</span> is the log-partitionfunction of the posterior distribution.</p><p>For every <span class="math inline">\(\mathbf{\tilde x}\)</span>, wehave <span class="math display">\[\int p(\mathbf{\eta}| \mathbf{\tilde x}) d\mathbf{\eta} = \intp(\mathbf{\eta}) \exp(\eta^{\top} \mathbf{\tilde x} -\psi(\mathbf{\eta}) - \lambda(\mathbf{\tilde x})) d\mathbf{\eta} = 1\]</span></p><p>Taking derivatives with respect to <spanclass="math inline">\(\mathbf{\tilde x}\)</span> on both sides gives<span class="math display">\[\begin{aligned}0 &amp;= \int p(\mathbf{\eta}) \exp(\eta^{\top} \mathbf{\tilde x} -\psi(\mathbf{\eta}) - \lambda(\mathbf{\tilde x})) (\mathbf{\eta} -\nabla_{\mathbf{\tilde x}} \lambda(\mathbf{\tilde x})) d\mathbf{\eta} \\&amp;= \mathbb{E}_{p(\mathbf{\eta}|\mathbf{\tilde x})}[\mathbf{\eta} -\nabla_{\mathbf{\tilde x}} \lambda(\mathbf{\tilde x})] \\\end{aligned}\]</span></p><p>Thus, we have <span class="math display">\[\mathbb{E}_{p(\mathbf{\eta}|\mathbf{\tilde x})}[\mathbf{\eta}] =\nabla_{\mathbf{\tilde x}} \lambda(\mathbf{\tilde x})\]</span> Taking the second derivative gives <spanclass="math display">\[\mathrm{Cov}[\mathbf{\eta}|\mathbf{\tildex}]=\mathbb{E}_{p(\mathbf{\eta}|\mathbf{\tilde x})}[(\mathbf{\eta} -\mathbb{E}_{p(\mathbf{\eta}|\mathbf{\tildex})}[\mathbf{\eta}])(\mathbf{\eta} -\mathbb{E}_{p(\mathbf{\eta}|\mathbf{\tilde x})}[\mathbf{\eta}])^{\top}]= \nabla_{\mathbf{\tilde x}}^2 \lambda(\mathbf{\tilde x})\]</span></p><p>Higher order cumulants can be derived similarly by taking higherorder derivatives of <span class="math inline">\(\lambda(\mathbf{\tildex})\)</span>, which is a <span class="math inline">\(k\)</span>-thtensor for the <span class="math inline">\(k\)</span>-th cumulant.</p><p><span class="math display">\[\nabla_{\mathbf{\tilde x}}^k \lambda(\mathbf{\tilde x}) =\mathbb{E}_{p(\mathbf{\eta}|\mathbf{\tilde x})} \left[(\mathbf{\eta} -\mathbb{E}_{p(\mathbf{\eta}|\mathbf{\tilde x})}[\mathbf{\eta}])^{\otimesk}\right] = \kappa_k(\mathbf{\eta}|\mathbf{\tilde x})\]</span></p><h4 id="denoising-tweedie-and-sure">Denoising, Tweedie and SURE</h4><p>Stein’s Unbiased Risk Estimator (SURE) provides an unbiased estimateof the mean squared error (MSE) between a denoised estimate and the truesignal without access to the true signal.</p><p><span class="math display">\[\tilde x = x + \sigma \epsilon, \quad \epsilon \sim \mathcal{N}(0, I)\]</span> A denoiser is any weakly differentiable function <spanclass="math inline">\(\mathbf{D}: \mathbb{R}^D \to \mathbb{R}^D\)</span>that maps the noisy observation <span class="math inline">\(\tildex\)</span> to a denoised estimate <span class="math inline">\(\hat x =\mathbf{D}(\tilde x)\)</span>.</p><p>Quality of the denoiser is measured by the MSE: <spanclass="math display">\[R(\mathbf{D}; \mathbf x) = \mathbb{E}_{\tilde x | x}[\|\mathbf{D}(\tilde x) - x \|_2^2|x].\]</span></p><p>The SURE states that the following quantity is an unbiased estimatorof the MSE: <span class="math display">\[\mathrm{SURE}(\mathbf{D}; \tilde x) = \|\mathbf{D}(\tilde x) - \tildex\|_2^2 + 2 \sigma^2 \nabla_{\tilde x}\cdot \mathbf{D}(\tilde x) - D\sigma^2\]</span></p><p>For the proof, first use the identity <span class="math display">\[\mathbb{E}[\nabla_{\mathbf z} \cdot \mathbf{D}(\mathbf z)] ] =\mathbb{E}[\mathbf z^{\top} \mathbf{D}(\mathbf z)]\]</span> by integration by parts on each component.</p><p>Therefore <span class="math display">\[\begin{aligned}\mathbb{E} [(\tilde x- x)^{\top} \mathbf{D}(\tilde x)]&amp;=\sigma\mathbb{E}[\mathbf{z}^{\top} \mathbf{D}(\tilde x)] \\&amp;= \sigma \mathbb{E}[\nabla_{\mathbf z} \cdot \mathbf{D}(\mathbf x +\sigma \mathbf z)] \\&amp;= \sigma \mathbb{E}[\sigma \nabla_{\mathbf{\tilde x}} \cdot\mathbf{D}(\mathbf{\tilde x})] \\&amp;= \sigma^2 \mathbb{E}[\nabla_{\mathbf{\tilde x}} \cdot\mathbf{D}(\mathbf{\tilde x})]\end{aligned}\]</span></p><p>And then expand the MSE: <span class="math display">\[\begin{aligned}R(\mathbf{D}; \mathbf x) &amp;= \mathbb{E}_{\tilde x | x}[\|\mathbf{D}(\tilde x) - x \|_2^2|x] \\&amp;= \mathbb{E}_{\tilde x | x}[\| \mathbf{D}(\tilde x) - \tilde x +\tilde x - x \|_2^2|x] \\&amp;= \mathbb{E}_{\tilde x | x}[\| \mathbf{D}(\tilde x) - \tilde x\|_2^2 |x] + 2 \mathbb{E}_{\tilde x | x}[(\tilde x - x)^{\top}(\mathbf{D}(\tilde x)- \tilde x) |x] + \mathbb{E}_{\tilde x | x}[\|\tilde x - x \|_2^2|x] \\&amp;= \mathbb{E}_{\tilde x | x}[\| \mathbf{D}(\tilde x) - \tilde x\|_2^2 |x] + 2 \left(\sigma^2 \mathbb{E}_{\tilde x |x}[\nabla_{\mathbf{\tilde x}} \cdot \mathbf{D}(\mathbf{\tilde x}) |x] -\mathbb{E}_{\tilde x | x}[(\tilde x - x)^{\top} \tilde x |x]\right) +\mathbb{E}_{\tilde x | x}[\| \tilde x - x \|_2^2|x] \\&amp;= \mathbb{E}_{\tilde x | x}[\| \mathbf{D}(\tilde x) - \tilde x\|_2^2 |x] + 2 \sigma^2 \mathbb{E}_{\tilde x | x}[\nabla_{\mathbf{\tildex}} \cdot \mathbf{D}(\mathbf{\tilde x}) |x] - 2 \sigma^2 D + D \sigma^2\\&amp;= \mathbb{E}_{\tilde x | x}[\mathrm{SURE}(\mathbf{D}; \tilde x) |x]\end{aligned}\]</span></p><p><span class="math display">\[\mathbb{E}_{\tilde x | x}[\mathrm{SURE}(\mathbf{D}; \tilde x)]  =\mathbb{E}_{\tilde x | x}[\| \mathbf{D}(\tilde x) - x \|_2^2]\]</span></p><p><span class="math display">\[\begin{aligned}\mathbb{E}_{(\tilde x, x)}[\| \mathbf{D}(\tilde x) - x \|_2^2] &amp;=\mathbb{E}_{x} [\mathbb{E}_{\tilde x | x}[\| \mathbf{D}(\tilde x) - x\|_2^2]] \\&amp;= \mathbb{E}_{x} [\mathbb{E}_{\tilde x |x}[\mathrm{SURE}(\mathbf{D}; \tilde x)]] \\&amp;= \mathbb{E}_{\tilde x} [\mathrm{SURE}(\mathbf{D}; \tilde x)] \\\end{aligned}\]</span></p><p>On the other hand <span class="math display">\[\begin{aligned}\mathbb{E}_{(\tilde x, x)}[\| \mathbf{D}(\tilde x) - x \|_2^2] &amp;=\mathbb{E}_{\tilde x} [\mathbb{E}_{x | \tilde x}[\| \mathbf{D}(\tilde x)- x \|_2^2]] \\\end{aligned}\]</span> Pointwise minimization of the MSE gives <spanclass="math display">\[\mathbf{D}^*(\tilde x) = \mathbb{E}[x|\tilde x]\]</span> for almost every <span class="math inline">\(\tildex\)</span>.</p><p>By Tweedie’s formula, we have <span class="math display">\[\mathbf{D}^*(\tilde x) = \tilde x + \sigma^2 \nabla_{\tilde x} \logp_{\sigma}(\tilde x)\]</span></p><p>Therefore, we deduce <span class="math display">\[\frac{1}{2\sigma^4} \mathrm{SURE}(\mathbf{D}; \tilde x) =\mathrm{Tr}(\nabla_{\tilde x} s_{\phi}(\tilde x)) + \frac{1}{2} \|s_{\phi}(\tilde x) \|_2^2 + C(\sigma)\]</span></p><p>This shows the equivalence between denoising, score matching and SUREup to a constant depending on <spanclass="math inline">\(\sigma\)</span>.</p><h4 id="generalized-score-matching">Generalized Score Matching</h4><p>Omitted. TODO.</p><h3 id="noise-conditional-score-network-ncsn">3.4 Noise ConditionalScore Network (NCSN)</h3>]]></content>
    
    
    <categories>
      
      <category>Machine Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Generative Models</tag>
      
      <tag>Diffusion Models</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Flow matching, Score-based Generative Model, Schrödinger Bridge and Optimal Transport</title>
    <link href="/2025/09/18/Flow-matching-Score-based-Generative-Model-Schrodinger-Bridge-and-Optimal-Transport/"/>
    <url>/2025/09/18/Flow-matching-Score-based-Generative-Model-Schrodinger-Bridge-and-Optimal-Transport/</url>
    
    <content type="html"><![CDATA[<h2 id="reference">Reference</h2><p><a href="http://arxiv.org/abs/2006.11239">Denoising DiffusionProbabilistic Models</a></p><p><a href="https://arxiv.org/abs/2303.16852">Diffusion SchrödingerBridge Matching</a></p><p><a href="https://arxiv.org/abs/2403.14623">Simplified DiffusionSchrödinger Bridge</a></p><p><a href="https://arxiv.org/abs/2407.04495">Speed-accuracy relationsfor diffusion models: Wisdom from nonequilibrium thermodynamics andoptimal transport</a></p><p><a href="https://arxiv.org/abs/2405.14449">Adversarial SchrödingerBridge Matching</a></p><h2 id="introduction">Introduction</h2><p>Generative models resonate with two deep principles: thethermodynamics of entropy and the mathematics of optimal transport.</p><h2 id="symbols-and-preliminaries">Symbols and Preliminaries</h2><h3 id="probability-space">Probability Space</h3><ul><li><span class="math inline">\((\Omega,\mathcal F,\mathbb P)\)</span>is a probability space.</li><li><span class="math inline">\((E,\mathcal E) = (\mathbb R^d, \mathcalB(\mathbb R^d))\)</span> is the measurable space.</li><li>A random variable is a measurable map <spanclass="math inline">\(X:(\Omega,\mathcal F)\to(E,\mathcal E)\)</span>,i.e. <span class="math display">\[X^{-1}(B) \in \mathcal F, \quad \forall B\in \mathcal B(\mathbb R^d).\]</span></li><li>The distribution (pushforward measure) of <spanclass="math inline">\(X\)</span> is <span class="math display">\[\mu = \mathrm{Law}(X) = \mathbb P \circ X^{-1}, \quad \mu \in \mathcalP(\mathbb R^d).\]</span></li><li>If <span class="math inline">\(\mu\)</span> is absolutely continuousw.r.t. the Lebesgue measure <spanclass="math inline">\(\lambda\)</span>, then there exists a densityfunction <span class="math inline">\(p(x): E\to [0,+\infty)\)</span>such that <span class="math display">\[d\mu(x) = p(x)\, dx.\]</span></li><li>Notation: we write <span class="math inline">\(X\sim \mu\)</span>;if <span class="math inline">\(\mu\)</span> admits a density, we oftenabbreviate <span class="math inline">\(X\sim p(x)\)</span>.</li></ul><h3 id="stochastic-process">Stochastic Process</h3><ul><li>A <strong>stochastic process</strong> is a time-parametrized familyof random variables <span class="math display">\[\{X_t: t\in [0,T]\},\quad X_t:(\Omega, \mathcal{F}) \to (E,\mathcal{E}).\]</span></li><li>The marginal distribution at time <spanclass="math inline">\(t\)</span> is <span class="math display">\[\mu_t = \mathrm{Law}(X_t),\quad X_t\sim \mu_t.\]</span></li></ul><h4 id="standard-brownian-motion-wiener-process">Standard BrownianMotion (Wiener Process)</h4><p>A <span class="math inline">\(d\)</span>-dimensional <strong>standardBrownian motion</strong> <spanclass="math inline">\((W_t)_{t\ge0}\)</span> with respect to afiltration <span class="math inline">\((\mathcal F_t)_{t\ge0}\)</span>satisfies: 1. <span class="math inline">\(W_0 = 0\)</span> almostsurely. 2. For <span class="math inline">\(0\le s&lt;t\)</span>, theincrement <span class="math inline">\(W_t-W_s \sim \mathcalN(0,(t-s)I_d)\)</span>. 3. The increments <spanclass="math inline">\(W_t-W_s\)</span> are independent of <spanclass="math inline">\(\mathcal F_s\)</span> (independent increments). 4.The paths <span class="math inline">\(t\mapsto W_t(\omega)\)</span> arealmost surely continuous.</p><h4 id="itô-integral-and-itôs-lemma">Itô Integral and Itô’s Lemma</h4><p><strong>Quadratic Variation.</strong><br />For a continuous semimartingale <spanclass="math inline">\(X=(X_t)_{t\ge0}\)</span>, the quadratic variationis defined as <span class="math display">\[[X]_t = \lim_{\|\Pi\|\to 0} \sum_{k} (X_{t_{k+1}}-X_{t_k})^2,\]</span> where <spanclass="math inline">\(\Pi=\{0=t_0&lt;\cdots&lt;t_n=t\}\)</span> is apartition of <span class="math inline">\([0,t]\)</span> and the limit isin probability.</p><p>For one-dimensional Brownian motion <spanclass="math inline">\((W_t)\)</span>, <span class="math display">\[[W]_t = t, \quad \text{a.s.}\]</span> In higher dimensions, for <spanclass="math inline">\(W=(W^{(1)},\dots,W^{(d)})\)</span>, <spanclass="math display">\[[W^{(i)},W^{(j)}]_t =\begin{cases}t &amp; i=j,\\0 &amp; i\ne j,\end{cases}\]</span> where <span class="math inline">\([X,Y]_t\)</span> denotes thequadratic covariation.</p><hr /><p><strong>Itô Integral.</strong><br />Let <span class="math inline">\((W_t)\)</span> be a Brownian motion andlet <span class="math inline">\(\phi_t\)</span> be an adapted processwith <span class="math display">\[\mathbb E\int_0^T \|\phi_t\|^2 dt &lt; \infty.\]</span> Then the Itô integral is defined as <spanclass="math display">\[\int_0^T \phi_t\, dW_t = L^2\text{-}\lim_{\|\Pi\|\to0}\sum_k\phi_{t_k}(W_{t_{k+1}}-W_{t_k}).\]</span> It satisfies the <strong>Itô isometry</strong>: <spanclass="math display">\[\mathbb E\left[\left(\int_0^T \phi_t\, dW_t\right)^2\right]= \mathbb E\int_0^T \phi_t^2\, dt.\]</span></p><hr /><p><strong>Itô’s Lemma (Itô formula, one-dimensional).</strong><br />Suppose <span class="math inline">\(X_t\)</span> satisfies the SDE <spanclass="math display">\[dX_t = f(X_t,t)\,dt + g(X_t,t)\,dW_t,\]</span> and let <span class="math inline">\(F:\mathbbR\times[0,T]\to\mathbb R\)</span> be <spanclass="math inline">\(C^{2,1}\)</span> (twice continuouslydifferentiable in <span class="math inline">\(x\)</span> and once in<span class="math inline">\(t\)</span>). Then <spanclass="math display">\[dF(X_t,t) = \Big(\partial_t F + f\,\partial_x F + \tfrac{1}{2}g^2\,\partial_{xx}F\Big)\,dt+ g\,\partial_x F\, dW_t.\]</span></p><hr /><p><strong>Multidimensional Itô’s Lemma.</strong><br />If <span class="math inline">\(X_t\in\mathbb R^d\)</span> satisfies<span class="math display">\[dX_t = f(X_t,t)\,dt + G(X_t,t)\,dW_t, \quad G\in\mathbb R^{d\times m},\]</span> and <span class="math inline">\(F:\mathbbR^d\times[0,T]\to\mathbb R\)</span> is <spanclass="math inline">\(C^{2,1}\)</span>, then <spanclass="math display">\[dF(X_t,t) = \Big(\partial_t F+ f^\top \nabla_x F+ \tfrac12 \mathrm{Tr}\!\big(GG^\top \nabla_x^2 F\big)\Big)dt+ (\nabla_x F)^\top G\, dW_t.\]</span></p><hr /><p><strong>Remark.</strong><br />- The term <span class="math inline">\(\tfrac12 g^2\partial_{xx}F\)</span> (or <span class="math inline">\(\tfrac12\mathrm{Tr}(GG^\top\nabla^2F)\)</span> in higher dimensions) arises fromthe quadratic variation of Brownian motion, i.e. <spanclass="math inline">\([W]_t=t\)</span>.<br />- This correction term is what distinguishes Itô calculus from classicalcalculus and is fundamental in stochastic analysis.</p><h3 id="other">Other</h3><p>The Kullback–Leibler (KL) divergence between two probability measuresare: <span class="math display">\[D_{\mathrm{KL}}(\mu \|\nu)=\int_{E}\log\left(\frac{d\mu}{d\nu}(x)\right) d\mu(x),\]</span> where the <spanclass="math inline">\(\frac{d\mu}{d\nu}\)</span> is the Radon-Nikodymderivative.</p><p>For KL divergence between Gaussian distribution, we have</p><p><span class="math display">\[\begin{align*}\mathrm{KL}(\mathcal{N}({\mu}_x,{\Sigma}_x)\|\mathcal{N}({\mu}_y,{\Sigma}_y))&amp;=\frac{1}{2}\left[\log\frac{|{\Sigma}_y|}{|{\Sigma}_x|} - d +\text{tr}({\Sigma}_y^{-1}{\Sigma}_x)+ ({\mu}_y-{\mu}_x)^T {\Sigma}_y^{-1} ({\mu}_y-{\mu}_x)\right]\end{align*}\]</span></p><h2 id="diffusion-models">Diffusion Models</h2><h3 id="ddpm">DDPM</h3><p><span class="math display">\[x\sim q(x)\]</span></p><p>How to sample from <span class="math inline">\(q(x)\)</span>?</p><p>We define a markovian stochastic process <spanclass="math inline">\(x_t\)</span> as the <strong>forwardprocess</strong>, such that: <span class="math display">\[x_0=x\\q(x_{1:T}|x_0) = \prod_{t=1}^{T} q(x_t|x_{t-1}),\]</span> where <span class="math inline">\(q(x_t|x_{t-1}) =\mathcal{N}(x_t;\sqrt{1-\beta_t}x_{t-1}, \beta_t I) =\frac{1}{\sqrt{(2\pi\beta_t)^d}}\exp(-\frac{\|x_t-\sqrt{1-\beta_t}x_{t-1}\|^2}{2\beta_t})\)</span>.</p><p>We call <span class="math inline">\(\{\beta_t\}_{t=1}^T\)</span> as avariance schedule.</p><p>By induction, we know <span class="math inline">\(x_t =\sqrt{\bar\alpha_t}x_0 + \sqrt{1-\bar\alpha_t}\epsilon,\quad \epsilon\sim\mathcal{N}(0,I),\quad \bar\alpha_t=\prod_{s=1}^{t}(1-\beta_s)\)</span>,</p><p><span class="math display">\[x_t \sim \mathcal{N}(\sqrt{\bar \alpha_t}x_0, (1-\bar\alpha_t)I).\]</span></p><p>So we can sample <span class="math inline">\(x_t\)</span>directly.</p><p>Assume <span class="math inline">\(\bar\alpha_t \to 0\)</span>, wecan regard <span class="math display">\[x_T \sim N(0,I).\]</span></p><p>Now given <span class="math inline">\(x_T\)</span>, we want sample<span class="math inline">\(x_0\)</span> from <spanclass="math inline">\(x_T\)</span>.</p><p>Calculate the posterior probability density: <spanclass="math display">\[q(x_t|x_{t+1}) = \int q(x_{t-1}|x_{t},x_{0}) q(x_0|x_t) d x_0\]</span> Which is impractical to calculate since we has to integrateover <span class="math inline">\(x_0\)</span>.</p><p>Instead we consider <span class="math display">\[q(x_{t-1}|x_t,x_0) = \frac{q(x_t|x_{t-1})q(x_{t-1}|x_{0})}{q(x_t|x_0)}\]</span> So <span class="math display">\[q(x_{t-1}|x_t,x_0)\propto q(x_t|x_{t-1})q(x_{t-1}|x_{0})\]</span> is still a Gaussian distribution. <spanclass="math display">\[q(x_{t-1}|x_t,x_0) = \mathcal{N}(x_{t-1};\mu,\Sigma)\]</span> Recall <spanclass="math inline">\(\Sigma^{-1}=\Sigma_{1}^{-1}+\Sigma_{2}^{-1},\Sigma^{-1}\mu = \Sigma_1^{-1}\mu_1+\Sigma_1^{-1}\mu_2\)</span></p><p>Note that <span class="math display">\[q(x_t|x_{t-1}) = \mathcal{N}(x_t;\sqrt{1-\beta_t}x_{t-1},\beta_tI)=\mathcal{N}(x_{t-1};\frac{1}{\sqrt{1-\beta_t}}x_{t},\frac{\beta_t}{1-\beta_t}I)\]</span> <span class="math inline">\(\Sigma_1=\frac{\beta_t}{1-\beta_t}I,\mu_1=\frac{1}{\sqrt{1-\beta_t}}x_{t}\)</span>.</p><p><spanclass="math inline">\(\Sigma_2=(1-\bar\alpha_{t-1})I,\mu_2=\sqrt{\bar\alpha_{t-1}}x_0\)</span>.</p><p><spanclass="math inline">\(\Sigma_t=(\frac{1-\beta_{t}}{\beta_t}+\frac{1}{1-\bar\alpha_{t-1}})^{-1}I=\frac{1-\bar\alpha_{t-1}}{1-\bar\alpha_t}\beta_tI=\tilde\beta_t I\)</span></p><p><spanclass="math inline">\(\mu_t=\frac{1-\bar\alpha_{t-1}}{1-\bar\alpha_t}\beta_t(\frac{\sqrt{1-\beta_t}}{\beta_t}x_t+\frac{\sqrt{\bar\alpha_{t-1}}}{1-\bar\alpha_{t-1}}x_0)= \frac{\beta_t\sqrt{\bar\alpha_{t-1}}}{1-\bar\alpha_t}x_0 +\frac{(1-\bar\alpha_{t-1})\sqrt{1-\beta_t}}{1-\bar\alpha_t}x_t\)</span>.</p><p>Now, to train the <spanclass="math inline">\(p_{\theta}(x_{t-1}|x_t)\)</span>, fix the variance<span class="math inline">\(\tilde \beta_t\)</span>, let model predictthe <span class="math inline">\(\mu_t\)</span>.</p><p>we minimize the KL divergence: <span class="math display">\[L_t(\theta)=\mathrm{KL}(q(x_{t-1}|x_0,x_t)\|p_{\theta}(x_{t-1}|x_t))\propto \mathbb{E}[\|\mu_t(x_t,x_0)-\mu_{\theta}(x_t,t)\|^2]\]</span> Or let the model predict the noise <spanclass="math inline">\(\epsilon=\frac{x_t-\sqrt{\bar\alpha_t}x_0}{\sqrt{1-\bar\alpha_t}}\)</span>,<spanclass="math inline">\(x_0=\frac{1}{\sqrt{\bar\alpha_t}}(x_t-\sqrt{1-\bar\alpha_t}\epsilon)\)</span>.</p><p>And minimize <span class="math display">\[\mathbbE_{x_0,\epsilon,t}\left[\|\epsilon-\epsilon_{\theta}(x_t,t)\|^2\right].\]</span> This reparametrization setting generally yields betterperformance.</p><h3 id="score-based-generative-model-sgm">Score-based Generative Model(SGM)</h3><p>We view noise injection as an SDE and learn the<strong>score</strong> <span class="math inline">\(s_t(x) := \nabla_x\log p_t(x)\)</span> of the noisy marginal <spanclass="math inline">\((p_t)_{t\in [0,1]}\)</span>. Sampling is done byintegrating a reverse-time SDE (or its ODE counterpart), where the scoreguides the dynamics back to data.</p><h4 id="forward-sde">Forward SDE</h4><p>Let <span class="math inline">\(X_t\in \mathbb R^d\)</span> solve:<span class="math display">\[dX_t = f(X_t,t) dt + g(t) dW_t, \quad t\in [0,1],\]</span> with <span class="math inline">\(f:\mathbb R^d\times [0,1] \to\mathbb R^d\)</span>, diffusion scale <spanclass="math inline">\(g:[0,1]\to \mathbb R_+\)</span>. Denote <spanclass="math inline">\(p_t = \mathrm{Law}(X_t)\)</span>.</p><hr /><h4 id="ddpm-as-vp-sde">DDPM as VP-SDE</h4><p>Set the Variance-Preserving (VP) SDE <span class="math display">\[dX_t = -\frac{1}{2}\beta(t)X_t dt + \sqrt{\beta(t)} d W_t,\]</span> with <span class="math inline">\(\beta(t)\geq 0\)</span>integratable and <span class="math inline">\(X_0 \simp_{data}\)</span>.</p><p>This linear SDE has the explicit solution <spanclass="math display">\[X_t = \sqrt{\bar \alpha_t} X_0 + \underbrace{\int_0^t\exp\left(-\frac{1}{2}\int_s^t \beta(u) du\right)\sqrt{\beta(s)}dW_s}_{\text{zero-mean Gaussian}},\]</span> where <spanclass="math inline">\(\bar\alpha_t:=\exp\left(-\int_0^t\beta(s)ds\right)\)</span>.</p><p>Hence the marginal conditional matches DDPM.</p><p><span class="math display">\[X_t|X_0\sim \mathcal{N}(\sqrt{\bar\alpha_t}X_0, (1-\bar\alpha_t)I).\]</span></p><p><strong>Claim.</strong> The DDPM forward chain with <spanclass="math inline">\(\beta_1,\ldots,\beta_T\)</span> is anEuler-Maruyama discretization of the VP-SDE with piecewise-constant<span class="math inline">\(\beta(t)\)</span> and share the samemarginals <span class="math inline">\(q(x_t|x_0)\)</span>.</p><h4 id="variance-exploding-sde-ve-sde">Variance-Exploding SDE(VE-SDE)</h4><p>Alternatively, set <span class="math display">\[dX_t = g(t)\,dW_t, \qquad f\equiv 0,\]</span> with <spanclass="math inline">\(g(t)=\sqrt{d[\sigma^2(t)]/dt}\)</span> and <spanclass="math inline">\(\sigma(0)=0\)</span>. Then <spanclass="math display">\[X_t = X_0 + \sigma(t)Z,\quad Z\sim\mathcal N(0,I).\]</span> So <span class="math inline">\(p_t = p_0 * \mathcalN(0,\sigma(t)^2I)\)</span>, i.e. Gaussian smoothing.</p><h4 id="reverse-time-sde-and-probability-flow-ode">Reverse-time SDE andProbability Flow ODE</h4><p>The reverse SDE (Anderson, 1982) is <span class="math display">\[dX_t = \big(f(X_t,t)-g(t)^2\nabla_x\log p_t(X_t)\big)dt + g(t)\,d\barW_t,\]</span> where <span class="math inline">\(\bar W_t\)</span> is abackward Wiener process. Replacing <spanclass="math inline">\(\nabla_x\log p_t\)</span> with a learned <spanclass="math inline">\(s_\theta\)</span> gives a generative sampler.</p><p>The equivalent deterministic probability flow ODE is <spanclass="math display">\[\frac{dX_t}{dt} = f(X_t,t) - \tfrac12 g(t)^2 \nabla_x\log p_t(X_t).\]</span> This is the continuous analogue of DDIM.</p><h4 id="noise-prediction-vs.-score-prediction">Noise Predictionvs. Score Prediction</h4><p>For VP, <span class="math display">\[\nabla_{x_t}\log q(x_t|x_0) =-\frac{x_t-\sqrt{\bar\alpha_t}x_0}{1-\bar\alpha_t} =-\frac{\epsilon}{\sqrt{1-\bar\alpha_t}}.\]</span> Thus noise prediction and score prediction are equivalent:<span class="math display">\[s_\theta(x_t,t) = -\frac{\epsilon_\theta(x_t,t)}{\sqrt{1-\bar\alpha_t}}.\]</span></p><p>An alternative data predictor: <span class="math display">\[\hat x_0(x_t) =\frac{1}{\sqrt{\bar\alpha_t}}\big(x_t-\sqrt{1-\bar\alpha_t}\,\epsilon_\theta(x_t,t)\big).\]</span></p><h3 id="denoising-score-matching-dsm">Denoising Score Matching(DSM)</h3><p>Training objective: <span class="math display">\[\min_\theta\; \mathbb E_{x_0,\sigma,z}\;\Big\|s_\theta(x_0+\sigmaz,\sigma) + \tfrac{z}{\sigma}\Big\|^2.\]</span></p><p>For VP, choosing <spanclass="math inline">\(\sigma(t)=\sqrt{1-\bar\alpha_t}\)</span> reducesDSM to the DDPM noise-prediction MSE.</p><h4 id="derivation-and-intuition">Derivation and Intuition</h4><p>TODO: Weak form.</p><h2 id="flow-matching-models">Flow Matching Models</h2><p>See <ahref="https://notdesigned.github.io/2025/08/09/Flow-Matching/">FlowMatching</a></p><h2 id="schrödinger-bridge-models">Schrödinger Bridge Models</h2><h3 id="relation-with-entropy-regularized-optimal-transport">Relationwith Entropy-Regularized Optimal Transport</h3><h3 id="ipf-dsb-s-dsb">IPF (DSB, S-DSB)</h3><h3 id="imf">IMF</h3><h3 id="d-imf-adsb">D-IMF (ADSB)</h3><h3 id="cdsb">CDSB</h3><p>TODO.</p>]]></content>
    
    
    <categories>
      
      <category>Machine Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Generative Models</tag>
      
      <tag>Flow Matching</tag>
      
      <tag>Score-based Models</tag>
      
      <tag>Optimal Transport</tag>
      
      <tag>Stochastic Differential Equations</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Variational Calculus in VAE</title>
    <link href="/2025/08/25/Variational-Calculus-in-VAE/"/>
    <url>/2025/08/25/Variational-Calculus-in-VAE/</url>
    
    <content type="html"><![CDATA[<p>Given a probability distribution <spanclass="math inline">\(p(x)\)</span>, we want to find an encoder <spanclass="math inline">\(q_{\theta}(z|x)\)</span> that approximates thetrue posterior <span class="math inline">\(p(z|x)\)</span>. And weresort to minimize the Kullback-Leibler divergence between them:</p><p><span class="math display">\[\min D_{KL}\left(q_{\theta}(z|x) \| p(z|x)\right)\]</span></p><p>And <span class="math inline">\(D_{KL}\left(q_{\theta}(z|x) \|p(z|x)\right)\)</span> can be computed as:</p><p><span class="math display">\[\begin{align*}D_{KL}\left(q_{\theta}(z|x) \| p(z|x)\right) &amp;=\mathbb{E}_{q_{\theta}(z|x)}\left[\log\frac{q_{\theta}(z|x)}{p(z|x)}\right]\\&amp;=\int q_{\theta}(z|x) \log\frac{q_{\theta}(z|x)}{p(z|x)} dz\\&amp;= J[q]\\s.t. \int q_{\theta}(z|x) dz = 1\end{align*}\]</span> We introduce a Lagrange multipler <spanclass="math inline">\(\lambda\)</span>, and we can write the constrainedoptimization problem as:</p><p>$$ <span class="math display">\[\begin{align*}S[q, \lambda] &amp;= J[q] + \lambda\left(\int q_{\theta}(z|x)dz-1\right)\\&amp;= \int \left[ q(z) \log q(z) -q(z) \log p(z|x)\right] dz+\lambda\left(\int q(z) dz- 1\right)\\&amp;= \int \left[ q(z) \log q(z) -q(z) \log p(z|x) + \lambda q(z)\right] dz - \lambda\end{align*}\]</span> $$</p><p>Use the Euler-Lagrange Equation w.r.t. <spanclass="math inline">\(q\)</span> <span class="math display">\[\frac{\partial L}{\partial q} -\frac{d}{dz} \frac{\partial L}{\partial\dot q} = 0\]</span></p><p>Reduce to <span class="math display">\[\frac{\partial L}{\partial q} = 0\]</span> since <span class="math inline">\(L\)</span> is not explicitlydependent on <span class="math inline">\(\dot q\)</span>.</p><p>Therefore <span class="math display">\[\log q(z) + 1 - \log p(z|x) + \lambda  =0\]</span></p><p>We can solve the above equations and get <spanclass="math display">\[q(z) = \exp\left(\log p(z|x) - 1 - \lambda\right) = C \cdot p(z|x)\]</span> And with normalizing constraint, <span class="math display">\[\int q(z) dz = 1\]</span> we know that <span class="math inline">\(q(z) =p(z|x)\)</span>.</p><p>And the next step is to decompose <span class="math display">\[\begin{align*}D_{KL}\left(q_{\theta}(z|x) \| p(z|x)\right) &amp;=\mathbb{E}_{q_{\theta(z|x)}}\left[\log\frac{q_{\theta}(z|x)}{p(z)}\right] +\mathbb{E}_{q_{\theta(z|x)}}\left[\log \frac{p(z)}{p(z|x)}\right] \\&amp;= D_{KL}\left(q_{\theta}(z|x) \| p(z)\right) +\mathbb{E}_{q_\theta(z|x)}\left[\log p(x)\right]\\&amp;= D_{KL}\left(q_{\theta}(z|x) \| p(z)\right) + \log p(x)\end{align*}\]</span></p><p>which is equivalent to maximizing the ELBO:</p><p><span class="math display">\[\mathcal{L}(q) = - D_{KL}\left(q_{\theta}(z|x) \| p(z)\right) =\mathbb{E}_{z\sim q(z|x)}\left [ \log\frac{p(x,z)}{q_{\theta}(z|x)}\right]\]</span></p><p>And ELBO can be expressed as:</p><p><span class="math display">\[\mathcal{L}(q) = \mathbb{E}_{z\sim q(z|x)} [\log p_{\theta}(x|z)] -D_{KL}(q_{\theta}(z|x) \| p(z))\]</span> And <span class="math inline">\(p_{\theta}(x|z)\)</span>represent the decoder.</p>]]></content>
    
    
    <categories>
      
      <category>Machine Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Generative Models</tag>
      
      <tag>VAE</tag>
      
      <tag>Calculus of Variations</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Renormalization Group Flow as Optimal Transport</title>
    <link href="/2025/08/21/Renormalization-Group-Flow-as-Optimal-Transport/"/>
    <url>/2025/08/21/Renormalization-Group-Flow-as-Optimal-Transport/</url>
    
    <content type="html"><![CDATA[<h2 id="reference">Reference</h2><p><a href="https://arxiv.org/pdf/2202.11737">Renormalization Group Flowas Optimal Transport</a></p><h2 id="introduction">Introduction</h2><p>I have very little knowledge in physics, so there might be manyerrors in this post. If you see any errors, please point it out in thecomment!</p><p>Renormalization theory, to put it simply, is a theory describing thatevery theory is the optimal theory under our capability ofunderstanding, thus might be only effective to some extent.</p><h2 id="ising-model">Ising Model</h2><h3 id="the-lattice">The Lattice</h3><p>A <span class="math inline">\(d\)</span>-dimensional lattice <spanclass="math inline">\(\mathbb Z^d\)</span> is the set of all integerpoints in <span class="math inline">\(d\)</span>-dimensional Euclideanspace <span class="math inline">\(\mathbb R^d\)</span>. To make iteasier to deal with, we usually take a subset <spanclass="math inline">\(L\subseteq \mathbb Z^d\)</span> and put<strong>periodic boundary conditions</strong> on <spanclass="math inline">\(L\)</span> (like the Snake game).</p><p>Every point <span class="math inline">\(i=(i_1,\ldots,i_d)\in \mathbbZ^d\)</span> in the lattice is called a <strong>site</strong>.</p><p>For every two neighbors <span class="math inline">\(i,j\)</span>, if<span class="math inline">\(\|i-j\|_{1}=1\)</span>, we call them<strong>nearest neighbors</strong>, denoted as <spanclass="math inline">\(\langle i,j \rangle\)</span>.</p><h3 id="configuration-space">Configuration Space</h3><p>For every site <span class="math inline">\(i\in L\)</span>, we have astate <span class="math inline">\(s_i\in V\)</span>, where <spanclass="math inline">\(V=\{-1,1\}\)</span> is the set of all possiblestates of the system.</p><p>A configuration <span class="math inline">\(\sigma\)</span> is afunction <span class="math inline">\(\sigma:L\to V\)</span>, which meansevery site <span class="math inline">\(i\in L\)</span> has a state <spanclass="math inline">\(\sigma(i)\in V\)</span>.</p><p>The configuration space <span class="math inline">\(\Lambda\)</span>is the set of all possible configurations, i.e., <spanclass="math inline">\(\Lambda = \{ \sigma : L \to V \}\congV^{N}\)</span>, where <span class="math inline">\(N=|L|\)</span> is thenumber of sites in <span class="math inline">\(L\)</span>.</p><h3 id="operators">Operators</h3><p>An operator <span class="math inline">\(O\)</span> is a functional<span class="math inline">\(O:\Lambda\to \mathbb R\)</span>, which meansit takes a configuration <span class="math inline">\(\sigma\)</span> andreturns a real number.</p><p>The most important operator is the <strong>Hamiltonian</strong> <spanclass="math inline">\(H\)</span>, which describes the energy of aconfiguration <span class="math inline">\(\sigma\)</span>.</p><h4 id="symmetry">Symmetry</h4><p>We may constrain the possible configurations by some symmetry.</p><p>We say a physical system <span class="math inline">\((\mathcal S, S,\mathcal O)\)</span> with state space <spanclass="math inline">\(\mathcal S\)</span>, law of dynamics <spanclass="math inline">\(S\)</span> and observables <spanclass="math inline">\(\mathcal O\)</span> has a symmetry <spanclass="math inline">\(T:\mathcal{S} \to \mathcal{S}\)</span> if for anyobservable <span class="math inline">\(O \in \mathcal{O}\)</span>, wehave <span class="math inline">\(O(T(\sigma)) = O(\sigma)\)</span> forall <span class="math inline">\(\sigma \in \mathcal{S}\)</span>.</p><p>So we can use the symmetry to reduce the number of operators we needto consider.</p><p>For the Ising model, we mainly focus on two operators:</p><h4 id="magnetization-operator">Magnetization Operator</h4><p><span class="math display">\[O_M(\sigma) = \sum_{i\in L} h_i \sigma(i)\]</span></p><h4 id="nearest-neighbor-interaction-operator">Nearest-NeighborInteraction Operator</h4><p><span class="math display">\[O_{NN}(\sigma) = \sum_{\langle i,j\rangle} J_{i,j} \sigma(i)\sigma(j)\]</span></p><p>The Hamiltonian is defined as a linear combination over a basis <spanclass="math inline">\(\{O\}\)</span> of the linear vector space <spanclass="math inline">\(\Lambda\)</span>.</p><p>For the Ising model, <span class="math display">\[H(J,h;\sigma) = O_{NN}(\sigma) + h O_M(\sigma) = \sum_{\langlei,j\rangle} J_{i,j} \sigma(i)\sigma(j) + \sum_{i\in L}h_i\sigma(i)\]</span></p><p>where <span class="math inline">\(J,h\)</span> are coupling constantsdescribing the strength of the interaction and the external field.</p><p>When the external field is zero everywhere, <spanclass="math inline">\(h = 0\)</span>, the Ising model is symmetric underswitching the value of the spin in all the lattice sites; a nonzerofield breaks this symmetry.</p><h3 id="the-partition-function">The Partition Function</h3><p>The partition function <span class="math inline">\(Z\)</span> is afunction of the Hamiltonian <span class="math inline">\(H\)</span> andthe temperature <span class="math inline">\(T\)</span>, defined as:<span class="math display">\[Z(J,h,\beta) = \sum_{\sigma \in \Lambda} e^{-\beta H(J,h;\sigma)}\]</span></p><p>The configuration probability is given by the Boltzmann distribution<span class="math display">\[P(\sigma) = \frac{1}{Z(J,h,\beta)} e^{-\beta H(J,h;\sigma)}\]</span> where <span class="math inline">\(\beta = 1/k_{B} T\)</span>,and <span class="math inline">\(k_{B}\)</span> is the Boltzmannconstant.</p><h2 id="continuous-generalization">Continuous Generalization</h2><h3 id="from-lattice-to-continuum">From Lattice to Continuum</h3><p>Let the distance between two points approach <spanclass="math inline">\(0\)</span>, then <spanclass="math inline">\(\sigma\in \Lambda\)</span> becomes a real scalarfield <span class="math inline">\(\phi(x)\)</span> on <spanclass="math inline">\(\Omega \subset \mathbb R^d\)</span>.</p><p>The Hamiltonian <span class="math inline">\(H\)</span> becomes theaction functional <span class="math inline">\(S\)</span> as the integralof the Lagrangian density <span class="math display">\[S[\phi] = \int_{\Omega} \mathcal{L}(\phi,\partial_\mu \phi) d^d x\]</span></p><p>The partition function becomes the functional integral: <spanclass="math display">\[Z[\phi] = \int \mathcal{D}\phi \, e^{-S[\phi]}\]</span></p><h3 id="fourier-transform-convention-and-momentum-space">FourierTransform Convention and Momentum Space</h3><p><strong>Convention</strong>: We use the Fourier transform convention:<span class="math display">\[\phi(x) = \int \frac{d^d p}{(2\pi)^d} e^{ip \cdot x} \tilde{\phi}(p)\]</span> <span class="math display">\[\tilde{\phi}(p) = \int d^d x \, e^{-ip \cdot x} \phi(x)\]</span></p><p>where <span class="math inline">\(p \cdot x = p_\mu x^\mu\)</span>(Einstein summation convention), and we set <spanclass="math inline">\(\hbar = c = 1\)</span>.</p><p><strong>Key Identity</strong>: The crucial relationship betweenposition and momentum space derivatives is: <spanclass="math display">\[\frac{\partial}{\partial x^\mu} \phi(x) \leftrightarrow i p_\mu\tilde{\phi}(p)\]</span></p><p><strong>Derivation of the duality</strong>: Starting from the Fouriertransform: <span class="math display">\[\frac{\partial \phi(x)}{\partial x^\mu} = \frac{\partial}{\partialx^\mu} \int \frac{d^d p}{(2\pi)^d} e^{ip \cdot x} \tilde{\phi}(p)\]</span></p><p>Since <span class="math inline">\(\frac{\partial}{\partial x^\mu}e^{ip \cdot x} = \frac{\partial}{\partial x^\mu} e^{ip_\nu x^\nu} = ip_\mu e^{ip \cdot x}\)</span>, we get: <span class="math display">\[\frac{\partial \phi(x)}{\partial x^\mu} = \int \frac{d^d p}{(2\pi)^d} (ip_\mu) e^{ip \cdot x} \tilde{\phi}(p)\]</span></p><p>This shows that <strong>differentiation in position space becomesmultiplication by <span class="math inline">\(ip_\mu\)</span> inmomentum space</strong>.</p><h3 id="momentum-space-integrals">Momentum Space Integrals</h3><p><strong>Parseval’s theorem</strong> for scalar fields gives us: <spanclass="math display">\[\int d^d x \, \phi(x)^2 = \int \frac{d^d p}{(2\pi)^d} \tilde{\phi}(p)\tilde{\phi}(-p)\]</span></p><p><strong>For derivatives</strong>: Using the duality <spanclass="math inline">\(\partial_\mu \leftrightarrow i p_\mu\)</span>:<span class="math display">\[\int d^d x \, (\partial_\mu \phi(x))^2 = \int \frac{d^d p}{(2\pi)^d} (ip_\mu)^2 \tilde{\phi}(p) \tilde{\phi}(-p) = \int \frac{d^d p}{(2\pi)^d}p_\mu^2 \tilde{\phi}(p) \tilde{\phi}(-p)\]</span></p><p>More generally: <span class="math display">\[\int d^d x \, (\partial_\mu \phi)(\partial^\mu \phi) = \int \frac{d^dp}{(2\pi)^d} p^2 \tilde{\phi}(p) \tilde{\phi}(-p)\]</span> where <span class="math inline">\(p^2 = p_\mu p^\mu =|\vec{p}|^2\)</span> in Euclidean space.</p><h3 id="transformation-of-operators">Transformation of Operators</h3><p>Now we can transform our operators to momentum space:</p><p><strong>Magnetization operator</strong> (with external source <spanclass="math inline">\(j(x)\)</span>): <span class="math display">\[O_M[\phi] = \int d^d x \, j(x) \phi(x) \to \int \frac{d^d p}{(2\pi)^d}\tilde{j}(p) \tilde{\phi}(-p)\]</span></p><p><strong>Kinetic term</strong>: <span class="math display">\[\int d^d x \, \frac{1}{2}(\partial_\mu\phi)^2 \to \int \frac{d^dp}{(2\pi)^d} \frac{1}{2} p^2 \tilde{\phi}(p) \tilde{\phi}(-p)\]</span></p><p><strong>Mass term</strong>: <span class="math display">\[\int d^d x \, \frac{1}{2} m^2 \phi^2 \to \int \frac{d^d p}{(2\pi)^d}\frac{1}{2} m^2 \tilde{\phi}(p) \tilde{\phi}(-p)\]</span></p><h3 id="the-action-in-momentum-space">The Action in Momentum Space</h3><p>If we assume the interaction coupling constants are uniform, we canwrite the action as: <span class="math display">\[S[\phi] = \int d^d x\left[\frac{1}{2}(\partial_{\mu}\phi)^2+\frac{1}{2}m^2\phi^2+\frac{u_0}{4!}\phi^4+j(x)\phi(x)\right]\]</span></p><p><strong>In momentum space</strong>, this becomes: <spanclass="math display">\[S[\tilde{\phi}] = \int \frac{d^d p}{(2\pi)^d}\left[\frac{1}{2}(p^2+m^2)\tilde{\phi}(p)\tilde{\phi}(-p) +\tilde{j}(p)\tilde{\phi}(-p)\right ] + S_{int}[\tilde{\phi}]\]</span></p><p>where <span class="math inline">\(\tilde{j}(p)\)</span> is theFourier transform of <span class="math inline">\(j(x)\)</span> and <spanclass="math inline">\(S_{int}[\tilde{\phi}]\)</span> contains theinteraction terms.</p><h2 id="propagator">Propagator</h2><h3 id="definition">Definition</h3><p>After defining the behavior of the field <spanclass="math inline">\(\phi\)</span> <spanclass="math inline">\((S[\phi])\)</span> and its statisticaldistribution <span class="math inline">\(P[\phi]=\exp(-S[\phi])\)</span>, we shall ask the fundamental question—What isthe relation between the different point in a field.</p><p>In other words, if I know the value <spanclass="math inline">\(\phi(x)\)</span> at point <spanclass="math inline">\(x\)</span>, what can I say about the value <spanclass="math inline">\(\phi(y)\)</span> at another point <spanclass="math inline">\(y\)</span>?</p><p>We introduce the <strong>propagator</strong> (also Green’s function,two-point correlation function)</p><p><span class="math display">\[G(x,y) \equiv \langle \phi(x) \phi(y) \rangle = \frac{1}{Z} \int\mathcal{D}\phi \, \phi(x) \phi(y) e^{-S[\phi]}\]</span> as the expectation value of the product of the fields at twodifferent points.</p><p>In the momentum space, the propagator is defined as: <spanclass="math display">\[D(p) \equiv \langle \tilde{\phi}(p) \tilde{\phi}(-p) \rangle =\frac{1}{Z} \int \mathcal{D}\tilde{\phi} \, \tilde{\phi}(p)\tilde{\phi}(-p) e^{-S[\tilde{\phi}]}\]</span></p><p>The relation is given by the Fourier transform:</p><p><span class="math display">\[\begin{align*} G(x, y) &amp;= \langle \phi(x)\phi(y) \rangle \\&amp;= \left\langle \left( \int \frac{d^d p}{(2\pi)^d} e^{ip \cdot x}\tilde{\phi}(p) \right) \left( \int \frac{d^d q}{(2\pi)^d} e^{iq \cdoty} \tilde{\phi}(q) \right) \right\rangle \\&amp;= \int \frac{d^d p}{(2\pi)^d} \int \frac{d^d q}{(2\pi)^d} e^{ip\cdot x} e^{iq \cdot y} \langle \tilde{\phi}(p)\tilde{\phi}(q) \rangle\\&amp;= \int \frac{d^d p}{(2\pi)^d} \int \frac{d^d q}{(2\pi)^d} e^{ip\cdot x} e^{iq \cdot y} \left( (2\pi)^d \delta^{(d)}(p+q) D(p) \right)\\\end{align*}\]</span> Let <span class="math inline">\(q = -p\)</span>： <spanclass="math display">\[\begin{align*} G(x, y) &amp;= \int \frac{d^d p}{(2\pi)^d} e^{ip \cdot x}e^{-ip \cdot y} D(p) \\&amp;= \int \frac{d^d p}{(2\pi)^d} e^{ip \cdot (x-y)} D(p)\end{align*}\]</span></p><h3 id="the-source-field-method-as-partitional-generating-function">TheSource Field Method (as Partitional Generating Function)</h3><p>Calculate the propagator directly from the definition is verycomplicated due to the presence of interactions.</p><p>For the <strong>free field</strong> (no interactions, <spanclass="math inline">\(u_0 = 0\)</span>), the action is quadratic: <spanclass="math display">\[S_0[\tilde{\phi}] = \int \frac{d^d p}{(2\pi)^d}\frac{1}{2}(p^2+m^2)\tilde{\phi}(p)\tilde{\phi}(-p)\]</span></p><p>If we calculate the propagator directly, we get:</p><p><span class="math display">\[\begin{align*}D(p) = \frac{1}{Z} \int \mathcal{D}\tilde{\phi} \, \tilde{\phi}(p)\tilde{\phi}(-p) e^{-S_0[\tilde{\phi}]} \\= \frac{\int \mathcal{D}\tilde{\phi} \, \tilde{\phi}(p) \tilde{\phi}(-p)e^{-S_0[\tilde{\phi}]}}{\int \mathcal{D}\tilde{\phi} \,e^{-S_0[\tilde{\phi}]}}\\\end{align*}\]</span></p><p>To calculate this propagator, we need to evaluate the Gaussianintegrals involved.</p><p>But actually, we can use the technique of generating function.</p><p>Recall that, for a random variable <spanclass="math inline">\(X\)</span>, we can define its generating functionas:</p><p><span class="math display">\[G_X(t) = \mathbb{E}[e^{tX}]\]</span> And its moment <spanclass="math inline">\(\mathbb{E}[X^n]\)</span> can be obtained bydifferentiating <span class="math inline">\(G_X(t)\)</span> <spanclass="math inline">\(n\)</span> times and evaluating at <spanclass="math inline">\(t=0\)</span>:</p><p>Now we are dealing with a field <spanclass="math inline">\(\phi\)</span> and functional partition function<span class="math inline">\(Z[\phi] =\mathbb{E}[\exp(-S[\phi])]\)</span>.</p><p>Similarly, we introduce a source field <spanclass="math inline">\(J\)</span> and define the partition function withsource <span class="math inline">\(J\)</span> as: <spanclass="math display">\[Z[J] = \mathbb{E}[\exp(-S[\phi] + \int d^d x J(x) \phi(x))]\]</span> or, in momentum space <span class="math display">\[Z[\tilde J] = \mathbb{E}[\exp(-S[\tilde{\phi}] + \int d^d p \tilde J(p)\tilde{\phi}(-p))]\]</span> Take the functional derivative of <spanclass="math inline">\(Z[J]\)</span>, we obtain: <spanclass="math display">\[\frac{\delta}{\delta \tilde J(p)} Z[\tilde J] =\mathbb{E}[\tilde{\phi}(-p) \exp(-S[\tilde{\phi}] + \int d^d \tilde J(p)\tilde \phi(-p))]\]</span> <span class="math display">\[\frac{\delta}{\delta \tilde J(p)}\frac{\delta}{\delta \tilde J(-p)}Z[\tilde J] = \mathbb{E}[\tilde{\phi}(-p) \tilde{\phi}(p)\exp(-S[\tilde{\phi}] + \int d^d \tilde J(p) \tilde \phi(-p))]\]</span> And evaluate it at <span class="math inline">\(\tilde J =0\)</span>, this is exactly the definition of propagator: <spanclass="math display">\[D(p) = \frac{1}{Z} \left. \frac{\delta}{\delta \tildeJ(p)}\frac{\delta}{\delta \tilde J(-p)} Z[\tilde J] \right|_{\tilde J =0} = \left. \frac{\delta}{\delta \tilde J(p)}\frac{\delta}{\delta \tildeJ(-p)} W[\tilde J] \right|_{\tilde J = 0}\]</span> where <span class="math inline">\(W[\tilde J] = \ln Z[\tildeJ]\)</span>.</p><p>For the free field, <span class="math display">\[\begin{align*}Z_0[\tilde J] &amp;= \int \mathcal{D}\tilde{\phi} \, \exp \left[-\frac{1}{2} \int \frac{d^d p}{(2\pi)^d} (p^2 + m^2) \tilde{\phi}(p)\tilde{\phi}(-p) + \int d^d p \tilde J(p) \tilde{\phi}(-p) \right]\\&amp;=\int \mathcal{D} \tilde \phi \exp\left(-\int\frac{d^dp}{(2\pi)^d}\left[\frac{1}{2}\tilde\phi&#39;(-p)(p^2+m^2)\tilde\phi&#39;(p)+(p^2+m^2)^{-1}\tildeJ(p)\tilde J(-p)\right]\right)\\&amp;= Z_0[0] \exp \left[ \frac{1}{2} \int \frac{d^d p}{(2\pi)^d} (p^2 +m^2)^{-1} \tilde J(p) \tilde J(-p) \right]\end{align*}\]</span> where <span class="math inline">\(\tilde\phi(p) = \tilde\phi&#39;(p) + D(p)\tilde J(p)\)</span>.</p><p>This substitution is called the completing the square technique.</p><p>The propagator is: <span class="math display">\[D(p) = \frac{1}{p^2 + m^2}\]</span></p><p>This gives the familiar result that in position space, the two-pointcorrelation function is: <span class="math display">\[\langle \phi(x) \phi(y) \rangle = \int \frac{d^d p}{(2\pi)^d}\frac{e^{ip \cdot (x-y)}}{p^2 + m^2}\]</span></p><h3 id="connection-to-exact-rg">Connection to Exact RG</h3><p>This momentum space formulation naturally leads to the <strong>exactrenormalization group</strong> approach, where we systematicallyintegrate out high-momentum modes while keeping track of how theeffective action changes. The cutoff function <spanclass="math inline">\(K_\Lambda(p^2)\)</span> mentioned in the paperprovides a smooth way to suppress high-momentum modes above the scale<span class="math inline">\(\Lambda\)</span>.</p><h2 id="renormalization">Renormalization</h2><p>If we directly calculate the correlation, we will encounter infinitywhen <span class="math inline">\(d&gt;4\)</span>.</p><h3 id="introduction-1">Introduction</h3><p>TODO.</p><h3 id="polchinski-equation">Polchinski Equation</h3><p>Here is the detailed version of section <spanclass="math inline">\(2.1.1\)</span> in the paper.</p><p><span class="math display">\[Z_{\Lambda}[J] =\int \mathcal D\phi \exp (-S_{\Lambda}[\phi,J]) =\int\mathcal D\phi \, \exp \left[ -\frac{1}{2} \int \frac{d^d p}{(2\pi)^d}\left(\phi(p)\phi(-p) (p^2 + m^2) K_\Lambda(p^2)^{-1} + J(p)\phi(-p)\right) - S_{int,\Lambda} [\phi] \right]\]</span></p><p>where <span class="math inline">\(S_{int,\Lambda}[\phi]\)</span> isthe interaction term that depends on the cutoff scale <spanclass="math inline">\(\Lambda\)</span>, <spanclass="math inline">\(K_\Lambda(p^2)\)</span> is a soft cutoff function,i.e. it is <span class="math inline">\(1\)</span> for <spanclass="math inline">\(p^2 &lt; \Lambda^2\)</span> and smoothly approach<span class="math inline">\(0\)</span> for <spanclass="math inline">\(p^2 &gt; \Lambda^2\)</span>.</p><p>Then we consider a small scale <spanclass="math inline">\(\Lambda_{R}\)</span>, which is the scale at whichwe want to integrate out the high-momentum modes.</p><p>We want the probability functional shall not change under theperturbation of our scale <span class="math inline">\(\Lambda\)</span>,so the partition function can only change by multiple amount inreleventto <span class="math inline">\(\phi\)</span> and only relevent to <spanclass="math inline">\(\Lambda\)</span>. <span class="math display">\[- \Lambda \frac{d}{d \Lambda} Z_{\Lambda}[J] = C_{\Lambda}Z_{\Lambda}[J]\]</span></p><p>Calculate the LHS directly <span class="math display">\[\begin{align*}-\Lambda \frac{d}{d \Lambda} Z_{\Lambda}[J] &amp;= -\Lambda \int\mathcal D\phi \, \left[ \exp (-S_{\Lambda}[\phi,J]) \cdot\left(-\frac{dS_{\Lambda}[\phi,J]}{d\Lambda}\right) \right] \\&amp;= \int \mathcal D\phi \, \left( \Lambda\frac{dS_{\Lambda}[\phi,J]}{d\Lambda} \right) \exp(-S_{\Lambda}[\phi,J]) \\&amp;=\int \mathcal D\phi \, \left( \frac{1}{2}\int \frac{d^dp}{(2\pi)^d}  \phi(p) \phi(-p) (p^2 + m^2) \Lambda \frac{\partialK_\Lambda^{-1}(p^2)}{\partial \Lambda} + \Lambda \frac{\partialS_{int,\Lambda}[\phi]}{\partial \Lambda} \right) \exp(-S_{\Lambda}[\phi,J])\\&amp;=Z_{\Lambda}[J]\cdot\left\langle \frac{1}{2}\int \frac{d^dp}{(2\pi)^d} \phi(p) \phi(-p) (p^2 + m^2) \Lambda \frac{\partialK_\Lambda^{-1}(p^2)}{\partial \Lambda} + \Lambda \frac{\partialS_{int,\Lambda}[\phi]}{\partial \Lambda}  \right\rangle \bigg|_{J}\end{align*}\]</span></p><p>So <span class="math display">\[- \Lambda \frac{1}{Z_{\Lambda}[J]}\frac{d}{d \Lambda}Z_{\Lambda}[J]=-\Lambda \frac{d \lnZ_{\Lambda}[J]}{d\Lambda}=C_{\Lambda} = \left\langle \frac{1}{2}\int\frac{d^d p}{(2\pi)^d}\phi(p) \phi(-p) (p^2 + m^2) \Lambda\frac{\partial K_\Lambda^{-1}(p^2)}{\partial \Lambda} + \Lambda\frac{\partial S_{int,\Lambda}[\phi]}{\partial \Lambda}  \right\rangle\bigg|_{J}\]</span></p><p>By the arbitrarity of <span class="math inline">\(J\)</span>, <spanclass="math display">\[\frac{1}{2}\int \frac{d^d p}{(2\pi)^d} \left( \phi(p) \phi(-p) (p^2 +m^2) \Lambda \frac{\partial K_\Lambda^{-1}(p^2)}{\partial\Lambda}\right) + \Lambda \frac{\partial S_{int,\Lambda}[\phi]}{\partial\Lambda}  = C_{\Lambda}\]</span> And the standard form is: <span class="math display">\[\Lambda \frac{\partial S_{int,\Lambda}}{\partial\Lambda} =-\frac{dS_{int}}{d\Lambda/\Lambda} = \frac{1}{2} \int\frac{d^dp}{(2\pi)^d} \dot{C}_{\Lambda}(p^2) \left[ \frac{\delta^2S_{int}}{\delta\phi(p)\delta\phi(-p)} - \frac{\delta S_{int}}{\delta\phi(p)} \frac{\delta S_{int}}{\delta \phi(-p)} \right]\]</span></p><p>To make it match the form of the RHS, we should adjust the couplingconstants in the interaction term <spanclass="math inline">\(S_{int,\Lambda}[\phi]\)</span> accordingly.</p>]]></content>
    
    
    <categories>
      
      <category>Physics</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Optimal Transport</tag>
      
      <tag>Renormalization Group</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Flow Matching</title>
    <link href="/2025/08/09/Flow-Matching/"/>
    <url>/2025/08/09/Flow-Matching/</url>
    
    <content type="html"><![CDATA[<h2 id="reference">Reference</h2><p><a href="https://arxiv.org/pdf/2210.02747">Flow Matching ForGenerative Modeling</a></p><p><ahref="https://voletiv.github.io/docs/presentations/20200901_Mila_CNF_Vikram_Voleti.pdf">ContinuousNormalizing Flows</a></p><p><a href="https://arxiv.org/abs/2209.03003">Flow Straight andFast</a></p><h2 id="preliminaries">Preliminaries</h2><p>The objective of generative modeling is to learn the underlyingdistribution <span class="math inline">\(p_{data}(x)\)</span> of thetraining data <span class="math inline">\(x\)</span>. This is typicallyachieved by training a generative model <spanclass="math inline">\(p_{model}(x)\)</span> to approximate <spanclass="math inline">\(p_{data}(x)\)</span>, allowing for the generationof new samples from the learned distribution.</p><p>Let <span class="math inline">\(X\)</span> be a complete andseparable metric space such as <span class="math inline">\(\Omega\subseteq \mathbb R^n\)</span>. Then, the space of probability measures<span class="math inline">\(\mathcal{P}(X)\)</span> is defined as theset of all Borel probability measures on <spanclass="math inline">\(X\)</span>.</p><p>Generative models usually aims to learn a mapping from a simpledistribution (e.g., Gaussian) to the complex data distribution <spanclass="math inline">\(p_{data}(x)\)</span> by transforming samples fromthe simple distribution into samples from the data distribution.</p><h3 id="topology-on-mathcal-px">Topology on <spanclass="math inline">\(\mathcal P(X)\)</span></h3><h4 id="weak-topology">Weak Topology</h4><p>the <strong>weak topology</strong> on <spanclass="math inline">\(P(X)\)</span> is defined as the coarsest topologymaking all functionals of the form <span class="math display">\[\mu \mapsto \int_{X}f\,d\mu\]</span> continuous, where <span class="math inline">\(f\inC_{b}(X)\)</span> ranges over all the bounded continuous functions on<span class="math inline">\(X\)</span>.</p><p><strong>Convergence characterization</strong>: A sequence <spanclass="math inline">\((\mu_n)\)</span> converges to <spanclass="math inline">\(\mu\)</span> in the weak topology if and only if<span class="math display">\[\int_X f \, d\mu_n \to \int_X f \, d\mu\]</span> for all bounded continuous functions <spanclass="math inline">\(f: X \to \mathbb{R}\)</span>.</p><p>Actually, let <span class="math inline">\(\mathcal M(X)\)</span> bethe space of all signed, finite measure on <spanclass="math inline">\(X\)</span>. <spanclass="math inline">\(C_b(X)\)</span> can be seen as the dual spaceinduced by the pairing: <span class="math display">\[\langle f,\mu \rangle = \int_{X}f\,d\mu\]</span></p><p>The relation between a measure on <spanclass="math inline">\(X\)</span> and a linear functional (dual space) on<span class="math inline">\(C_b(X)\)</span>: <spanclass="math display">\[\begin{align*}\Phi:\mathcal{M}(X) &amp;\to C_b(X)^*\\\mu&amp;\mapsto [f\mapsto \int_X f\, d\mu = \langle f, \mu \rangle]\end{align*}\]</span></p><p><span class="math inline">\(\Phi\)</span> is linear andinjective.</p><p>When <span class="math inline">\(X\)</span> is compact, <spanclass="math inline">\(\Phi\)</span> is bijective by the Rieszrepresentation theorem. <span class="math inline">\(\mathcal{M}(X)\congC_b(X)^*\)</span>.</p><p>When <span class="math inline">\(X\)</span> is locally compact andseparable, <span class="math inline">\(\mathcal{M}(X)\congC_0(X)^*\)</span>, where <span class="math inline">\(C_0(X)\)</span> isthe compactly support function on <spanclass="math inline">\(X\)</span>, which vanishing on infinity.</p><p>And we have the <strong>weak</strong>* topology on the dual space of<span class="math inline">\(C_b(X)\)</span> <spanclass="math display">\[L_n\overset{w^*}{\to} L \iff L_n(f)\to L(f),\quad \forall f\in C_b(X)\]</span></p><p>So <span class="math inline">\(\Phi\)</span> induces the weak*topology on <span class="math inline">\(\mathcal M(X)\)</span> and itssubset <span class="math inline">\(P(X)\)</span>.</p><h4 id="wasserstein-metric-topology">Wasserstein Metric Topology</h4><p>When <span class="math inline">\(X\)</span> is a compact Riemannmanifold <span class="math inline">\((M,g)\)</span>, the <spanclass="math inline">\(p\)</span>-Wasserstein distance is a metric on<span class="math inline">\(\mathcal{P}(X)\)</span>, and induced atopology on <span class="math inline">\(M\)</span>. More precisely, forprobability distribution with finite <spanclass="math inline">\(p\)</span>-th moment, <spanclass="math inline">\(\mathcal{P}_p(X)\)</span>.</p><p><strong>Kantorovich-Rubinstein</strong> duality states that for any<span class="math inline">\(p\)</span>-Wasserstein distance, wehave:</p><p><span class="math display">\[W_1(\mu, \nu) = \sup_{f \in \text{Lip}_1(M)} \left( \int_M f \, d\mu -\int_M f \, d\nu \right)\]</span> where <span class="math inline">\(W_1(\mu, \nu)\)</span> isthe <span class="math inline">\(1\)</span>-Wasserstein distance.</p><p>This plays an important role in WGAN.</p><p>So <span class="math inline">\(1\)</span>-Wasserstein convergence isequivalent to convergence over all Lipschitz functions <spanclass="math inline">\(f\)</span>, which is stronger than weakconvergence (all <span class="math inline">\(C_b\)</span>functions).</p><p>Generally, we have <span class="math display">\[W_n(\mu, \nu)\leq W_m(\mu, \nu),\forall n\leq m\]</span></p><p>So <span class="math inline">\(2\)</span>-Wasserstein topology isweaker than <span class="math inline">\(1\)</span>-Wasserstein metrictopology, etc.</p><p>But if we assume all the probability measure has compact support, theconvergence is equivalent.</p><hr /><p>Let <span class="math inline">\(\mu \in \mathcal{P}(X)\)</span> bethe simple distribution, e.g. <spanclass="math inline">\(\mu=\mathcal{N}(0, I)\)</span>, and let <spanclass="math inline">\(\nu \in \mathcal{P}(X)\)</span> be the datadistribution. The goal is to learn a mapping <spanclass="math inline">\(f: X \to X\)</span> such that <spanclass="math inline">\(f_{*}\mu = \nu\)</span>, where <spanclass="math inline">\(f_{*}\mu\)</span> is the pushforward measure of<span class="math inline">\(\mu\)</span> under the mapping <spanclass="math inline">\(f\)</span> defined by <spanclass="math inline">\(f_{*}\mu(A) := \mu(f^{-1}(A))\)</span>, where<span class="math inline">\(A\in \mathcal{B}_X\)</span> is a Borelset.</p><p><span class="math display">\[\begin{align*}f_{*}: \mathcal{P}(X) &amp;\to \mathcal{P}(X) \\\mu &amp;\mapsto f_{*}\mu = \nu\end{align*}\]</span></p><p>But in reality, we only have the samples from the data distribution<span class="math inline">\(\nu\)</span>. So the best we can do is toapproximate <span class="math inline">\(\nu\)</span> by <spanclass="math display">\[\nu \approx \frac{1}{N}\sum_{i=1}^{N} \delta_{x_i}\]</span> where <span class="math inline">\(x_i\in X\)</span> is thesample data and <span class="math inline">\(\delta_{x_i}\)</span> is theDirac measure centered at <span class="math inline">\(x_i\)</span>.</p><p>This can be viewed as a semi-discrete optimal transport problem,where we aim to find a mapping <span class="math inline">\(f: X \toX\)</span> that pushes forward the simple distribution <spanclass="math inline">\(\mu\)</span> to the empirical distribution <spanclass="math inline">\(\nu\)</span>.</p><h3 id="data-manifold-hypothesis">Data Manifold Hypothesis</h3><p>We view the real distribution as a distribution over <spanclass="math inline">\(\mathcal{M}\)</span>, where <spanclass="math inline">\(\mathcal{M}\)</span> is a manifold embedded in<span class="math inline">\(\mathbb{R}^n\)</span> and <spanclass="math inline">\(n\)</span> is the dimensionality of the data. Moreprecisely, we assume that the data distribution <spanclass="math inline">\(\nu\)</span> has support on a lower-dimensionalmanifold <span class="math inline">\(\mathcal{M} \subset\mathbb{R}^n\)</span>, typically with <spanclass="math inline">\(\dim(\mathcal{M}) \ll n\)</span>. This is the datamanifold hypothesis, which posits that high-dimensional data often lieson or near a lower-dimensional manifold.</p><h3 id="theoretical-framework-vs.-practical-implementation">TheoreticalFramework vs. Practical Implementation</h3><p><strong>Theoretical assumption</strong>:</p><p>The true data distribution <spanclass="math inline">\(p_{data}(x)\)</span> is absolutely continuous withrespect to the volume measure on the data manifold <spanclass="math inline">\(\mathcal{M}\)</span>, i.e., there exists a smoothdensity function <span class="math inline">\(\rho_{data}: \mathcal{M}\to \mathbb{R}_+\)</span> such that:</p><p><span class="math display">\[d\nu = \rho_{data}(x) \, d\text{vol}_\mathcal{M}(x)\]</span></p><p><strong>Practical reality</strong>:</p><p>We only observe finite samples <spanclass="math inline">\(\{x_i\}_{i=1}^N\)</span> from the truedistribution, leading to the empirical distribution:</p><p><span class="math display">\[\hat{\nu}_N = \frac{1}{N}\sum_{i=1}^{N} \delta_{x_i}\]</span> Note that each Dirac measure <spanclass="math inline">\(\delta_{x_i}\)</span> is not absolutely continuouswith respect to the volume measure, since <spanclass="math inline">\(\{x_i\}\)</span> has zero volume but <spanclass="math inline">\(\delta_{x_i}(\{x_i\}) = 1\)</span>.</p><p>Regularization assumption: We assume that the empirical distribution<span class="math inline">\(\hat{\nu}_N\)</span> converges weakly to thetrue continuous distribution <span class="math inline">\(\nu\)</span> as<span class="math inline">\(N \to \infty\)</span>:</p><p><span class="math display">\[\hat{\nu}_N \rightharpoonup \nu \quad \text{as } N \to \infty\]</span> This weak convergence justifies treating the discreteempirical distribution as an approximation to the underlying continuousdistribution.</p><p>But we can instead use a Gaussian kernel to smooth the empiricaldistribution:</p><p><span class="math display">\[\tilde{\nu}_N = \frac{1}{N}\sum_{i=1}^{N} K_\sigma(x - x_i) \delta_{x_i}\]</span> where <span class="math inline">\(K_\sigma(x)\)</span> is aGaussian kernel with bandwidth <spanclass="math inline">\(\sigma\)</span>.</p><p>We shall see this is the choice of the flow matching.</p><h2 id="discrete-normalizing-flow">(Discrete) Normalizing Flow</h2><p>We learn a bijective mapping <span class="math inline">\(f:\mathbb{R}^n \to \mathbb{R}^n\)</span> such that the pushforward measure<span class="math inline">\(f_{*}\mu\)</span> matches the targetdistribution <span class="math inline">\(\nu\)</span>.</p><p>The Normalizing Flow method assumes that <spanclass="math inline">\(f\)</span> can be expressed as a sequence ofinvertible transformations, typically using neural networks, such thatthe Jacobian determinant can be efficiently computed.</p><p><span class="math display">\[y \sim \nu, y = f(x), x \sim \mu= \mathcal{N}(0, I)\]</span></p><p>$$ <span class="math display">\[\begin{align*}f &amp;= f_n \cdots \circ f_1\\x_1 &amp;= f_1(x) \\x_2 &amp;= f_2(x_1) \\\vdots \\y = x_n &amp;= f_n(x_{n-1})\end{align*}\]</span> $$</p><p>And each <span class="math inline">\(f_i\)</span> is a neural networkparameterized by <span class="math inline">\(\theta_i\)</span>, i.e.,<span class="math inline">\(f_i = f_i(x; \theta_i)\)</span>.</p><h4 id="loss-and-training">Loss and Training</h4><p>The objective is maximum likelihood estimation or KL divergence,equivalently.</p><p>By the change of variables formula, we have: <spanclass="math display">\[p_Y(y) = p_X(f^{-1}(y)) \left| \det \frac{\partial f^{-1}}{\partial y}\right|\]</span></p><p><span class="math display">\[\begin{align*}\mathcal L(\theta) &amp;= \frac{1}{N}\sum_{i=1}^N \log p_Y(y_i) \\&amp;= \frac{1}{N}\sum_{i=1}^N \log p_X(f^{-1}(y_i)) \left| \det\frac{\partial f^{-1}}{\partial y_i} \right|\\\end{align*}\]</span></p><h2 id="continuous-normalizing-flows">Continuous Normalizing Flows</h2><p>Let <span class="math inline">\(n\to \infty\)</span>, it isequivalent to construct a vector field (and corresponding ODE) and takethe transform process as particle flows.</p><p>Given a time dependent transformation <span class="math display">\[\begin{align*}g: [0,1] \times \mathbb \Omega &amp;\to \mathbb \Omega\\(t,x) &amp;\mapsto g(t,x)\end{align*}\]</span></p><p>The flow <span class="math inline">\(g\)</span> induces atime-dependent one parameter group of diffeomorphisms: <spanclass="math display">\[g_t(x) = g(t,x)\]</span> And the trajectory of a particle <spanclass="math inline">\(x\)</span> is denoted as <spanclass="math inline">\(\gamma_x(t)\)</span>: <spanclass="math display">\[\gamma_x(t) = g(t,x) = g_t(x)\]</span> Note that here particle <span class="math inline">\(x\)</span>means it was initially at position <spanclass="math inline">\(x\)</span> at time <spanclass="math inline">\(t=0\)</span>.</p><p>We want a gradual transformation of the input distribution to thetarget distribution, i.e. <span class="math display">\[g_0 = \text{id}, g_1 = T\]</span> And <span class="math inline">\(T\)</span> satisfies <spanclass="math inline">\(T_* \mu = \nu\)</span>.</p><p>Fix a position <span class="math inline">\(y\)</span> and describethe speed of the particles at <span class="math inline">\(y\)</span>, wecan define the velocity field <spanclass="math inline">\(v_t(y)\)</span> as: <span class="math display">\[v_t(y) = \frac{\partial}{\partial t} g(t, x)\bigg|_{g(t,x)=y} =\frac{\partial g}{\partial t}(t, g_t^{-1}(y))\]</span></p><p>And the Eulerian description and Lagrangian description are relatedas follows: Eulerian: <span class="math display">\[\begin{cases}\frac{d}{dt}g_t(x) = v_t(g_t(x)),\\g_0(x)=x.\end{cases}\]</span> Lagrangian: <span class="math display">\[\begin{cases}\frac{d}{dt}\gamma_x(t) = v_t(\gamma_x(t)),\\\gamma_x(0)=x.\end{cases}\]</span></p><p>We shall now consider the conservation of mass, i.e., we want toensure that the flow preserves the total mass <spanclass="math inline">\(1\)</span> of the distribution.</p><p>We thus introduce a density function <spanclass="math inline">\(\rho\)</span> induced by the flow <spanclass="math display">\[\begin{align*}\rho(t,x): [0,1] \times \Omega &amp;\to \mathbb{R}\\(t,x) &amp;\mapsto \rho(t,x)\end{align*}\]</span> And denote <span class="math inline">\(\rho_t(x):\Omega \to\mathbb {R}\)</span> as <spanclass="math inline">\(\rho(t,x)\)</span>.</p><p>We require <span class="math display">\[\rho_t = (g_t)_*\rho_0\]</span> Particularly, <span class="math inline">\(\rho_0 = \mu, \rho_1= \nu\)</span>.</p><p>And we assume <span class="math inline">\(v\)</span> is tangent tothe boundary <span class="math inline">\(\partial \Omega\)</span>, andthe flow remains inside of <span class="math inline">\(\Omega\)</span>,thus <span class="math inline">\(\rho_t \in\mathcal{P}(\Omega)\)</span>.</p><p>It must satisfy the continuity equation to make the probability massconserve <span class="math display">\[\partial_t \rho_t + \nabla \cdot (v_t \rho_t) = 0\]</span></p><h3 id="proof-of-continuity-equation">Proof of continuity equation</h3><hr /><p>Let <span class="math inline">\(\phi\in C_b(\Omega)\)</span>, <spanclass="math display">\[\begin{align*}\int_{\Omega} \partial_t \phi\rho_t\, d\Omega &amp;=\int_{\Omega}\partial_t \phi\, d\rho_t\\&amp;= \frac{d}{dt} \int_{\Omega} \phi(x) \, d((g_t)_* \rho_0)  \\&amp;= \frac{d}{dt} \int_{\Omega} \phi(g_t)\, d\rho_0 \\&amp;= \int_{\Omega} \nabla \phi(g_t) \cdot v_t(g_t) \, d\rho_0 \\&amp;= \int_{\Omega} \nabla \phi \cdot v_t \, d\rho_t\\&amp;= -\int_{\Omega} \phi \left(\nabla\cdot v_t\rho_t \right) d\Omega\end{align*}\]</span> By the arbitrariness of <spanclass="math inline">\(\phi\)</span>, we get the continuity equation:<span class="math display">\[\partial_t \rho_t + \nabla \cdot (v_t \rho_t) = 0\]</span></p><p>We say the <span class="math inline">\((\rho_t,v_t)\)</span> solvethe continuity in distributional sense, which is called a weak solutionor distributional solution.</p><p>From the Lagrangian description, we know the formula: <spanclass="math display">\[J_t = \left |\det \frac{\partial g_t}{\partial x} \right | = \left |\det  F_t \right|\]</span> <span class="math display">\[\rho_t(g_t)\cdot J_t = \rho_0\]</span> Take differential on both side, <span class="math display">\[J_td \rho_t(g_t) + \rho_t(g_t) dJ_t = 0\]</span> And <span class="math display">\[\frac{d J_t}{dt}=J_t\cdot \operatorname{tr}\left(F_t^{-1}\frac{dF_t}{dt}\right)\]</span></p><p><span class="math display">\[\frac{d F_t}{d t} = \frac{\partial}{\partial t}\frac{\partialg_t}{\partial x} = \frac{d}{d x} \frac{\partial g_t}{\partial t} =\frac{d}{dx} v_t(g_t) = \nabla v_t(g_t) \cdot F_t\]</span> Hence <span class="math display">\[\frac{d J_t}{d t}= J_t \cdot \operatorname{tr}\left(\nablav_t(g_t)\right) = J_t \cdot( \nabla \cdot v_t(g_t))\]</span></p><p>Take it back, and eliminate <span class="math inline">\(J_t\)</span><span class="math display">\[d\rho_t(g_t) + \rho_t(g_t) (\nabla \cdot v_t(g_t)) = 0\]</span> And <span class="math inline">\(d\rho_t(g_t) = \frac{\partial\rho_t}{\partial t}(g_t) + \nabla \rho_t(g_t) \cdot v_t(g_t)\)</span>.<span class="math display">\[\frac{\partial \rho_t}{\partial t}(g_t) + \nabla \rho_t(g_t) \cdotv_t(g_t) + \rho_t(g_t) (\nabla \cdot v_t(g_t)) = 0\]</span> <span class="math display">\[\frac{\partial \rho_t}{\partial t}(g_t) + \nabla \cdot (v_t(g_t)\rho_t(g_t)) = 0\]</span> since <span class="math inline">\(g_t\)</span> is adiffeomorphism, we have: <span class="math display">\[\frac{\partial \rho_t}{\partial t} + \nabla \cdot (v_t \rho_t) = 0\]</span></p><p>Note: If we further assume the incompressibility condition, i.e.<span class="math display">\[\frac{\partial \rho_t}{\partial t} = 0\]</span> then we have <span class="math display">\[\nabla \cdot v_t = 0\]</span></p><hr /><p>So we know <span class="math display">\[\frac{\partial \rho_t}{\partial t} (x)= -\nabla \cdot (v_t(x) \cdot\rho_t(x))\]</span></p><p>Now we focus on a particle is initially at <spanclass="math inline">\(x\)</span>, moving on trajectory <spanclass="math inline">\(x_t=\gamma_x(t)\)</span></p><p><span class="math display">\[\begin{align*}\frac{d}{dt} \rho_t(\gamma_x(t)) &amp;= \frac{\partial \rho_t}{\partialt}(\gamma_x(t)) + \nabla \rho_t(\gamma_x(t)) \cdot \frac{d}{dt}\gamma_x(t) \\&amp;= \frac{\partial \rho_t}{\partial t}(\gamma_x(t)) + \nabla\rho_t(\gamma_x(t)) \cdot v_t(\gamma_x(t)) \\&amp;= -\nabla \cdot (v_t\rho_t)(\gamma_x(t)) + \nabla\rho_t(\gamma_x(t)) \cdot v_t(\gamma_x(t))\\&amp;= -\rho_t(\gamma_x(t))\nabla \cdot v_t(\gamma_x(t))\end{align*}\]</span></p><p>So <span class="math display">\[\frac{d}{dt}\log \rho_t(\gamma_x(t)) = \frac{1}{\rho_t(\gamma_x(t))}\frac{d}{dt} \rho_t(\gamma_x(t)) = - \nabla \cdot v_t(\gamma_x(t))\]</span> Thus <span class="math display">\[\log \rho_{1}(x_1) - \log \rho_{0}(x_0)= -\int_0^1 \nabla \cdot v_t(x_t)dt\]</span></p><h3 id="loss-and-training-1">Loss and Training</h3><p>We simulate the flow by using a neural network <spanclass="math inline">\(g_\theta\)</span> with parameter <spanclass="math inline">\(\theta\)</span>.</p><p>Similar to the discrete normalizing flow, we use MLE:</p><p><span class="math display">\[\begin{align*}\mathcal L(\theta)&amp;= -\mathbb{E}_{x_1\sim \nu}\left[\log\rho_{1,\theta}(x_1)\right] \\&amp;= -\mathbb{E}_{x_1\sim\nu}\left[\log \mu(x_0)-\int_0^1 \nabla \cdotv_{t,\theta}(x_t)\right]\end{align*}\]</span></p><p>Or equivalently, minimize the KL divergence between <spanclass="math inline">\(\rho_{1,\theta}\)</span> and <spanclass="math inline">\(\nu\)</span>.</p><h2 id="flow-matching">Flow Matching</h2><p>Instead of learning <span class="math inline">\(g\)</span>, flowmatching directly learns the velocity field <spanclass="math inline">\(v_{t,\theta}(x)\)</span>.</p><p>The theoretical objective is: <span class="math display">\[\mathcal{L}_{FM}=\mathbb E_{t\sim\mathcal{U}[0,1],x_t\sim \rho_t}\|v_{t,\theta}(x_t)-v_t(x_t)\|^2\]</span> But this cannot be trained directly since we don’t know <spanclass="math inline">\(v_t\)</span>.</p><h3 id="conditional-flow-matching">Conditional Flow Matching</h3><p>conditioning on <span class="math inline">\(x_1\)</span>, CFM use theconditional distribution <spanclass="math inline">\(\rho_t(x_t|x_1)\)</span></p><p>Key theorem:</p><p>Given conditional probability path <spanclass="math inline">\(\rho_t(x_t|x_1)\)</span> satisfying the continuityequation with conditional vector field <spanclass="math inline">\(v_t(x_t| x_1)\)</span>.</p><p>Then the marginal vector field <spanclass="math inline">\(v_t\)</span>: <span class="math display">\[v_t(x)=\int v_t(x_t|x_1) \frac{\rho_t(x_t|x_1)\nu(x_1)}{\rho_t(x)}d{x_1}\]</span> satisfy the continuity equation and generate the marginalprobability path <span class="math inline">\(\rho_t(x_t)\)</span>.</p><p>And one can choose any conditional probability path as long as <spanclass="math display">\[\rho_0(x|x_1) = \mu, \rho_1(x|x_1) = \delta(x-x_1)\]</span> And the paper use Gaussian kernal as an approximation.</p><p>If the above condition is satisfied, the paper proves that optimizingthe objective of CFM is equivalent to optimizing the objective of theoriginal flow model.</p><p><span class="math display">\[\mathcal L_{CFM} =\mathbb{E}_{t\sim\mathcal{U}[0,1],x_1\sim\nu,x_t\sim\rho_t(x_t|x_1)}\left[\|v_{t,\theta}(x_t|x_1) - v_t(x_t|x_1)\|^2\right]\]</span></p><p>And <span class="math inline">\(\nabla_{\theta} \mathcal L_{CFM}=\nabla_{\theta} \mathcal L_{FM}\)</span></p><p>And you can also condition on <spanclass="math inline">\(x_0\)</span>.</p><p>or both <span class="math inline">\(x_0\)</span> and <spanclass="math inline">\(x_1\)</span>, the formula <spanclass="math inline">\(\nabla_{\theta} \mathcal L_{CFM}\)</span> is stillvalid if if the coupling <span class="math inline">\(\pi\in\Gamma(\mu,\nu)\)</span> is fixed.</p><h2 id="rectified-flow">Rectified Flow</h2><p>In the paper, <strong>rectified flow</strong> optimized the velocityfield <span class="math inline">\(v_t\)</span> by minimizing thefollowing objective:</p><p><span class="math display">\[\mathcal{L}_{RF}=\int_0^1 \mathbb{E} \left[\|(X_1-X_0)-v(X_t,t)\|^2\right]\, \text{d}t,\, \text{with}\,\, X_t=tX_1+(1-t)X_0\]</span> where <span class="math inline">\(X_0\sim \mu, X_1\sim\nu\)</span>.</p><p>First, lets translate this into the CFM language.</p><p>Conditioning on <span class="math inline">\(x_0\)</span> and <spanclass="math inline">\(x_1\)</span>, rectified flow defined adeterministic path as the interpolation between <spanclass="math inline">\(x_0\)</span> and <spanclass="math inline">\(x_1\)</span>: <span class="math display">\[\begin{align*}g_t(x|x_0,x_1) &amp;= t x_1 + (1-t)x_0\\v_t(x|x_0,x_1) &amp;= \partial_t g_t(x|x_0,x_1) = x_1 - x_0\\\rho_t(x|x_0,x_1) &amp;= (g_t(x|x_0,x_1))_* \rho_0(x|x_0,x_1)\\&amp;= (g_t(x|x_0,x_1))_* \delta(x - x_0) \\&amp;= \delta(g_t^{-1}(x|x_0,x_1) - x_0) \left| \det \frac{\partialg_t^{-1}}{\partial x} \right|\\&amp;= \delta(\frac{x-tx_1}{1-t} - x_0) \frac{1}{(1-t)^n}\\&amp;= \delta\left(\frac{x-tx_1-(1-t)x_0}{1-t}\right)\frac{1}{(1-t)^n}\\&amp;= \delta\left(x-tx_1-(1-t)x_0\right) (1-t)^{n} \frac{1}{(1-t)^n}\\&amp;= \delta\left(x-tx_1-(1-t)x_0\right)\\\end{align*}\]</span></p><p>And apply the CFM objective: <span class="math display">\[\begin{align*}\mathcal{L}_{RF} &amp;=\mathbb{E}_{t\sim\mathcal{U}[0,1],x_0\sim\mu,x_1\sim\nu,x_t\sim\rho_t(x|x_0,x_1)}\left[\|v_{t,\theta}(x_t|x_0,x_1) - v_t(x_t|x_0,x_1)\|^2\right]\\&amp;= \int_0^1\mathbb{E}_{x_0\sim\mu,x_1\sim\nu,x_t\sim\rho_t(x|x_0,x_1)}\left[\|v_{t,\theta}(x_t|x_0,x_1) - (x_1-x_0)\|^2\right]\, \text{d}t\\&amp;= \int_0^1 \mathbb{E}_{x_0\sim\mu,x_1\sim\nu} \left[v_{t,\theta}(x_t|x_0,x_1) - (x_1-x_0)\right]^2 \, \text{d}t\\\end{align*}\]</span> And this is exactly the objective of rectified flow.</p><p>Actually, the author of <ahref="https://arxiv.org/pdf/2210.02747">Flow Matching For GenerativeModeling</a> also proposed an similar objective, but they use the name“OT-Flow” instead of “rectified flow” (And Gaussian kernal). We shallsee the reason in the next section.</p><h3 id="relation-to-optimal-transport">Relation to OptimalTransport</h3><p>What does the rectified flow and conditional flow matching have to dowith optimal transport? Let’s recall the method of conditionalprobability path.</p><p>The claim is that to get the vector field <spanclass="math inline">\(v_t\)</span>, we can use the conditional vectorfield <span class="math inline">\(\rho_t(x_t|x_0,x_1)\)</span> and dosummation over all <span class="math inline">\(x_0\)</span> and <spanclass="math inline">\(x_1\)</span>. Then we shall get the vector field<span class="math inline">\(v_t\)</span> that satisfies the continuityequation and generates the marginal probability path <spanclass="math inline">\(\rho_t(x_t)\)</span>.</p><p>It is not difficult to see that summing over the conditional vectorfield will yield a valid vector field that satisfies the continuityequation and transport the distribution from <spanclass="math inline">\(\mu\)</span> to <spanclass="math inline">\(\nu\)</span>. But what is the property of suchvector field?</p><p>Let’s think about a principle in traditional physics, the<strong>principle of least action</strong>. It states that the pathtaken by a system between two states is the one for which the action isstationary (usually minimized).</p><p>In the context of traditional physics, the shortest path between twopoints is a straight line. But in quantum mechanics, the particle is nolonger deterministic as a point, but rather a wave function that can bespread out over a region of space assigning a probability amplitude toeach point. But the principle of least action still holds, and the pathtaken by the particle is the one that minimizes the action.</p><h3 id="benamou-brenier-theory">Benamou-Brenier Theory</h3><p>The Benamou-Brenier theory states that the optimal transport problemcan be reformulated as a dynamic problem, where the optimal transportplan is the one that minimizes the action functional.</p><p>In every time <span class="math inline">\(t\)</span>, we have adistribution <span class="math inline">\(\rho_t\)</span> and a velocityfield <span class="math inline">\(v_t\)</span>. The kinetic energy isgiven by the integral of the squared velocity field: <spanclass="math display">\[E(t) = \int_{\Omega} \frac{1}{2}\|v_t(x)\|^2 \rho_t(x) \, dx\]</span></p><p>The action functional is then defined as: <spanclass="math display">\[A(\rho, v) = \int_0^1 E(t) \, dt= \int_0^1 \int_{\Omega}\frac{1}{2}\|v_t(x)\|^2 \rho_t(x) \, dx \, dt\]</span></p><p>And we see the action functional is exactly the total energy of usingthe velocity field <span class="math inline">\(v_t\)</span> to transportthe distribution <span class="math inline">\(\rho_t\)</span> from <spanclass="math inline">\(\mu\)</span> to <spanclass="math inline">\(\nu\)</span>.</p><p><strong>Problem</strong> (Benamou-Brenier): Given two probabilitymeasures <span class="math inline">\(\mu\)</span> and <spanclass="math inline">\(\nu\)</span>, find the optimal transport plan thatminimizes the action functional <span class="math inline">\(A(\rho,v)\)</span> subject to some conditions. <span class="math display">\[\begin{align*}\text{min} \quad &amp; A(\rho, v) \\\text{s.t.} \quad &amp;(\rho,v)\in V(\mu,\nu)\end{align*}\]</span> <strong>Admissible pairs</strong> <spanclass="math inline">\(V(\mu,\nu)\)</span> are pairs <spanclass="math inline">\((\rho, v)\)</span> such that: - <spanclass="math inline">\(\rho_0 = \mu\)</span>, <spanclass="math inline">\(\rho_1 = \nu\)</span>, - <spanclass="math inline">\(\partial_t \rho_t + \nabla \cdot (v_t \rho_t) =0\)</span> in distributional sense, - <spanclass="math inline">\(\bigcup_{t\in[0,1]} \text{supp}(\rho_t) \subseteq\Omega\)</span>, - <span class="math inline">\(v \in L^2(d\rho_t(x)dt)\)</span>, i.e., the velocity field is square integrable with respectto the measure <span class="math inline">\(\rho_t\)</span>. - <spanclass="math inline">\(\rho \in C([0,1]; w^*\text{-}P_{ac}(\mathbb\Omega))\)</span>, i.e., the probability path is absolutely continuouswith respect to the weak* topology on the space of probabilitymeasures.</p><p><strong>Theorem</strong>: (Benamou-Brenier)</p><p>The <span class="math inline">\(2\)</span>-Wasserstein distancebetween two probability measures <spanclass="math inline">\(\mu\)</span> and <spanclass="math inline">\(\nu\)</span> can be expressed as the infimum ofthe action functional over all admissible pairs <spanclass="math inline">\((\rho, v)\)</span> <span class="math display">\[W_2^2 (\mu, \nu) = \inf_{(\rho, v) \in V(\mu, \nu)} A(\rho, v)\]</span></p><p>This means that the optimal transport plan is the one that minimizesthe action functional, which is equivalent to minimizing the totalenergy of the velocity field.</p><h4 id="variational-calculus">Variational Calculus</h4><p><span class="math display">\[\begin{align*}\min A(\rho, v) &amp;= \int_0^1 \int_{\Omega} \frac{1}{2}\|v_t(x)\|^2\rho_t(x) \, dx \, dt \\\text{s.t.} \quad &amp; \partial_t \rho_t + \nabla \cdot (v_t \rho_t) =0 \\&amp; \rho_0 = \mu, \rho_1 = \nu\end{align*}\]</span> We introduce a Lagrange multiplier <spanclass="math inline">\(\varphi_t(x)\)</span> to enforce the continuityequation constraint: <span class="math display">\[\begin{align*}\mathcal{J}(\rho, v, \phi) &amp;=\int_0^1 \int_{\Omega}\left[\frac{1}{2}\|v_t(x)\|^2 \rho_t(x)+\varphi_t(x) \left( \partial_t\rho_t(x) + \nabla \cdot (v_t(x) \rho_t(x)) \right) \right] \,dx \, dt\end{align*}\]</span></p><p><strong>Variation with respect to <spanclass="math inline">\(v\)</span></strong>: <span class="math display">\[\frac{\delta \mathcal{J}}{\delta v_t} = \int_0^1 \int_{\Omega} \left[v_t\cdot \delta v_t \rho_t + \varphi_t \nabla \cdot (\delta v_t \rho_t)\right] \, dx \, dt\]</span> Using integration by parts on the second term <spanclass="math display">\[\begin{align*}\int_{\Omega} \varphi_t \nabla \cdot (\delta v_t \rho_t) \, dx &amp;=\int_{\Omega} \nabla \cdot (\varphi_t \rho_t \delta v_t) \, dx -\int_{\Omega} \nabla \varphi_t \cdot (\delta v_t \rho_t) \, dx\\&amp;=\int_{\partial \Omega} \varphi_t \rho_t \delta v_t \cdot n \, dS -\int_{\Omega} \nabla \varphi_t \cdot (\delta v_t \rho_t) \, dx\\&amp;= - \int_{\Omega} \nabla \varphi_t \cdot (\delta v_t \rho_t) \, dx\end{align*}\]</span> where the boundary term vanishes due to the assumption that<span class="math inline">\(v\)</span> is tangent to the boundary <spanclass="math inline">\(\partial \Omega\)</span>.</p><p>we get <span class="math display">\[\frac{\delta \mathcal{J}}{\delta v_t} = \int_0^1 \int_{\Omega} \left[v_t\rho_t - \rho_t\nabla \varphi_t \right] \cdot \delta v_t  \, dx \,dt\]</span></p><p>Which forces <span class="math display">\[v_t =  \nabla \varphi_t\]</span></p><p><strong>Variation with respect to <spanclass="math inline">\(\rho\)</span></strong>: <spanclass="math display">\[\frac{\delta \mathcal{J}}{\delta \rho_t} = \int_0^1 \int_{\Omega} \left[\frac{1}{2}\|v_t(x)\|^2 + \varphi_t(x) \partial_t \delta \rho_t(x) +\varphi_t(x) \nabla \cdot (v_t(x) \delta \rho_t(x)) \right] \, dx \, dt\]</span> Similarly, using integration by parts on the second term andthe third term <span class="math display">\[\begin{align*}\int_{\Omega} \varphi_t\partial_t\delta\rho_t \, d\Omega &amp;=-\int_{\Omega} \partial_t\varphi_t\delta\rho_t\, d\Omega\\\int_{\Omega} \varphi_t \nabla \cdot (v_t \delta \rho_t)\, d\Omega&amp;= - \int_{\Omega} \nabla \varphi_t \cdot (v_t \delta \rho_t) \,d\Omega\end{align*}\]</span> Note <span class="math inline">\(\rho_0 = \mu\)</span> and<span class="math inline">\(\rho_1 = \nu\)</span> are fixed, sovariations <span class="math inline">\(\delta \rho_t\)</span> are zeroat the endpoints <span class="math inline">\((\delta \rho_0 = \delta\rho_1 = 0)\)</span>.</p><p>we get <span class="math display">\[\begin{align*}\frac{\delta \mathcal{J}}{\delta \rho_t} &amp;= \int_0^1 \int_{\Omega}\left[ \frac{1}{2}\|v_t(x)\|^2 - \partial_t\varphi_t(x) - \nabla\varphi_t(x) \cdot v_t(x) \right] \delta\rho_t(x) \, dx \, dt\end{align*}\]</span> Setting this to zero for all <spanclass="math inline">\(\delta \rho_t(x)\)</span>, we have <spanclass="math display">\[\frac{1}{2}\|v_t(x)\|^2 - \partial_t\varphi_t(x) - \nabla \varphi_t(x)\cdot v_t(x) = 0\]</span></p><p>Substituting <span class="math inline">\(v_t = \nabla\varphi_t\)</span>, we have <span class="math display">\[\partial_t \varphi_t(x) + \frac{1}{2}\|v_t(x)\|^2 = 0\]</span></p><p>To summarize, we have three equations for the action functional,which gives us the necessary conditions for optimality: <spanclass="math display">\[\begin{cases}v_t = \nabla \varphi_t, \quad \text{Velocity field as gradient ofpotential} \\\partial_t \varphi_t(x) + \frac{1}{2}\|v_t(x)\|^2 = 0, \quad\text{Hamilton-Jacobi equation} \\\partial_t \rho_t + \nabla \cdot (v_t \rho_t) = 0. \quad\text{Continuity equation for mass conservation}\end{cases}\]</span></p><p>By the <span class="math inline">\(\varphi\)</span>, we can getKantorovich potential <span class="math inline">\((\psi,\phi)\)</span>,<span class="math display">\[\psi(x):=\varphi_1(x), \quad \phi(x)=-\varphi_0(x)\]</span> as the dual potentials for the optimal transport problem.</p><p>One can prove it by integral along the optimal pair <spanclass="math inline">\((x,T(x))\)</span>, and verify the dualityoptimality. (Actually, this maybe give a proof to the Benamou-Breniertheorem)</p><h3 id="economic-interpretation">Economic interpretation</h3><p>The optimal transport problem can be interpreted in an economiccontext, where we want to transport resources (e.g., goods, people) fromone location to another while minimizing the cost of transportation.</p><p>(Monge’s formulation) <span class="math display">\[\begin{align*}\min \int_X c(x, T(x)) \, d\mu(x) \\\text{s.t.} \quad T_* \mu = \nu\end{align*}\]</span> where <span class="math inline">\(c(x, T(x))\)</span> is thecost of transporting from location <spanclass="math inline">\(x\)</span> to location <spanclass="math inline">\(T(x)\)</span>, and <spanclass="math inline">\(\mu\)</span> and <spanclass="math inline">\(\nu\)</span> are the source and targetdistributions, respectively.</p><p>(Kantorovich’s formulation) <span class="math display">\[\begin{align*}\min \int_{X \times Y} c(x, y) \, d\gamma(x, y) \\\text{s.t.} \quad \gamma \in \Pi(\mu, \nu)\end{align*}\]</span> where <span class="math inline">\(\gamma\)</span> is a jointdistribution over the source and target locations, and <spanclass="math inline">\(\Pi(\mu, \nu)\)</span> is the set of all couplingsbetween <span class="math inline">\(\mu\)</span> and <spanclass="math inline">\(\nu\)</span>.</p><p>(Kantorovich dual formulation) <span class="math display">\[\begin{align*}\max \int_X \phi(x) \, d\mu(x) + \int_Y \psi(y) \, d\nu(y) \\\text{s.t.} \quad \phi(x) + \psi(y) \leq c(x, y) \quad \forall (x, y)\in X \times Y\end{align*}\]</span></p><p>Let’s consider the economic interpretation:</p><p><span class="math inline">\(\mu\)</span> is the distribution ofresources at the source location <span class="math inline">\(X\)</span>,and <span class="math inline">\(\nu\)</span> is the target distributionof resources at the destination location <spanclass="math inline">\(Y\)</span>. The cost function <spanclass="math inline">\(c(x, y)\)</span> represents the cost oftransporting resources from location <spanclass="math inline">\(x\)</span> to location <spanclass="math inline">\(y\)</span>. And merchants are trying to maximizetheir profit by transporting resources.</p><p>Let <span class="math inline">\(\varphi(t,x)\)</span> be thepotential function that represents the value of resources at <spanclass="math inline">\(g_t(x)\in\Omega\)</span>, and in the nation <spanclass="math inline">\(X\)</span>, let <spanclass="math inline">\(\phi\)</span> denote the cost of buying resourcesat <span class="math inline">\(x\)</span> and <spanclass="math inline">\(\psi\)</span> denote the cost of selling resourcesat <span class="math inline">\(y\)</span>.</p><p>Now look back at the setting: <span class="math display">\[\phi:=-\varphi_0, \quad \psi:=\varphi_1\]</span> The potential <span class="math inline">\(\phi\)</span>represents the change of the balance of merchants buying resources atthe source location <span class="math inline">\(X\)</span>, and thepotential <span class="math inline">\(\psi\)</span> represents the gainof merchants selling resources at the destination location <spanclass="math inline">\(Y\)</span>.</p><p>So the Kantorovich dual formulation can be interpreted as maximizingthe profit of merchants by buying resources at the source location <spanclass="math inline">\(X\)</span> and selling them at the destinationlocation <span class="math inline">\(Y\)</span>.</p><p>And the no arbitrage condition <span class="math inline">\(\phi(x) +\psi(y) \leq c(x, y)\)</span> ensures that the profit from buying andselling resources is not greater than the cost of transportation,preventing arbitrage opportunities.</p><p>Look back at the optimality conditions, we have: 1. The velocityfield <span class="math inline">\(v_t = \nabla \varphi_t\)</span>represents the optimal transportation direction of resources, which isthe gradient of the potential function <spanclass="math inline">\(\varphi_t\)</span>.</p><p>This is quite intuitive.</p><ol start="2" type="1"><li>The Hamilton-Jacobi equation <span class="math inline">\(\partial_t\varphi_t(x) + \frac{1}{2}\|v_t(x)\|^2 = 0\)</span> describes theevolution of the potential function over time, where the term <spanclass="math inline">\(\frac{1}{2}\|v_t(x)\|^2\)</span> can beinterpreted as the cost of transportation.</li></ol><p>But here is a problem, the cost of transportation is formulated asthe integral of the instantaneous cost over time, but in the originaloptimal transport problem, the cost is formulated as independent ofpath, i.e., the cost is only dependent on the source and targetlocations, not on the path taken.</p><p>To make the two formulations consistent, we impose a constraint:</p><p>Let <span class="math display">\[C[\gamma]=\int_0^1 c(\dot\gamma_t)\,dt\]</span></p><p>be the cost of transportation along the path <spanclass="math inline">\(\gamma\)</span>, if <span class="math display">\[c(x,y) = \inf\left\{C[(\gamma_t)_{t\in[0,1]}]\right\} \quad \text{forall } (x,y)\in X\times Y\]</span> then the two formulations give the same optimal transportplan.</p><p>A common situation is <span class="math display">\[c(x,y):= c(y-x) = \|x-y\|^k\]</span> where <span class="math inline">\(k\geq 1\)</span> is aninteger.</p><p>When <span class="math inline">\(k=1\)</span>, the cost function islinear and not strictly convex, and the optimal transport plan is notunique. Any linear parameterization of the line segment connecting <spanclass="math inline">\(x\)</span> and <spanclass="math inline">\(y\)</span> will yield the same cost.</p><p>When <span class="math inline">\(k\geq 2\)</span>, the cost functionis strictly convex, and the optimal transport plan is unique. Theoptimal path is the straight line connecting <spanclass="math inline">\(x\)</span> and <spanclass="math inline">\(y\)</span> with velocity parameterized by the arclength.</p><hr /><p><strong>Lemma</strong>: If <span class="math inline">\(c\)</span> isa convex function over <span class="math inline">\(\mathbb R^n\)</span>,then <span class="math display">\[\inf\left\{\int_0^1 c(\dot\gamma_t) dt; \gamma_0=x,\gamma_1=y\right\} =c(y-x)\]</span> Moreover, if <span class="math inline">\(c\)</span> isstrictly convex, then the infimum is achieved by a unique path <spanclass="math inline">\(\gamma_t = tx + (1-t)y\)</span>.</p><p><strong>Proof</strong></p><p>Use the Jensen’s inequality of integral.</p><hr /><p>So for the <span class="math inline">\(2\)</span>-Wassersteindistance, the cost function can be seen as the integral of velocityfield over time. <span class="math display">\[c(x,y)=\inf C_{\text{path}}=\int_0^1 c(\dot\gamma_t) dt = \int_0^1v_t(x) dt = \frac{1}{2}\|y-x\|_2^2\]</span> where the best path <spanclass="math inline">\(\gamma_t\)</span> is the straight line connecting<span class="math inline">\(x\)</span> and <spanclass="math inline">\(y\)</span> with constant velocity <spanclass="math inline">\(\dot\gamma_t=1\)</span>.</p><h3 id="why-conditioning-works">Why Conditioning Works?</h3><p>Turning back to the rectified flow, the author condition on <spanclass="math inline">\(x_0,x_1\)</span>, let <spanclass="math inline">\(\pi(x_0,x_1)\)</span> be the product coupling<span class="math inline">\(\mu(x_0)\nu(x_1)\)</span>.</p><p><span class="math display">\[\begin{align*}v_t(x_t|x_0,x_1) &amp;= x_1-x_0\\v_t(x_t) &amp;= \int \int v_t(x_t|x_0,x_1)\frac{\rho_t(x_t|x_0,x_1)\pi(x_0,x_1)}{\rho_t(x_t)} \, dx_0 \, dx_1\\&amp;= \int \int (x_1-x_0)\frac{\rho_t(x_t|x_0,x_1)\mu(x_0)\nu(x_1)}{\rho_t(x_t)} \, dx_0 \,dx_1\\\end{align*}\]</span></p><p>It is easy to see if <span class="math inline">\(\pi =\pi^*\)</span>, then the marginal vector field <spanclass="math inline">\(v_t(x_t)\)</span> is exactly the optimal transportvector field.</p><p>But rectified flow set <span class="math inline">\(\pi = \mu \otimes\nu\)</span>, so it cannot get the optimal transport vector field in 1step. But the paper proves every time of reflow matching will decreasethe transport cost, and the rectified flow will converge to the optimaltransport vector field by the speed of <spanclass="math inline">\(\mathcal{O}(1/k)\)</span>, where <spanclass="math inline">\(k\)</span> is the number of iterations.</p><p>And in the <a href="https://arxiv.org/pdf/2210.02747">Flow MatchingFor Generative Modeling</a>, the author proposed “OT-flow” isconditioning on <span class="math inline">\(x_1\)</span> only.Therefore, since the target is a Gaussian kernal and the source is aGaussian distribution, they just know the exact form of the optimaltransport vector field <span class="math display">\[v_t(x_t|x_1) = \frac{x_1 - (1-\sigma_{min})x_t}{1-(1-\sigma_{min})t}\]</span> and there is no problem of coupling since they only conditionon <span class="math inline">\(x_1\)</span>.</p><h4 id="why-reflow-works">Why Reflow Works?</h4><p>The <span class="math inline">\(1\)</span>-rectified flow iscalculated by: <span class="math display">\[\begin{align*}v_t^1(x) = \mathbb{E}_{(x_0,x_1)\sim \pi_0} \left[x_1-x_0 | x = t x_0 +(1-t) x_1\right]\end{align*}\]</span> where <span class="math inline">\(\pi_0 = \mu \otimes\nu\)</span> is the independent coupling of the source and targetdistributions. This is the expectation of line direction that pass <spanclass="math inline">\(x\)</span> at time <spanclass="math inline">\(t\)</span>.</p><p>For <span class="math inline">\(k\geq 2\)</span>, the <spanclass="math inline">\(k\)</span>-rectified flow is calculated by: <spanclass="math display">\[\begin{align*}v_t^k(x) = \mathbb{E}_{(x_0,x_1)\sim \pi_{k-1}} \left[x_1-x_0 | x = tx_0 + (1-t) x_1\right]\end{align*}\]</span> where <span class="math inline">\(\pi_{k-1} =\pi_{g^{k-1}_1} =(\text{Id}, g_1^{k-1})_* \mu\)</span> is the coupling induced by the<span class="math inline">\((k-1)\)</span>-rectified flow.</p><p><span class="math display">\[C(\pi)=\mathbb{E}_{(x_0,x_1)\sim \pi} \left[c(x_0,x_1)\right]\]</span> as the transport cost of the coupling <spanclass="math inline">\(\pi\)</span>.</p><p>We assume the cost function has the form <spanclass="math inline">\(c(x,y)=c(y-x)\)</span> and <spanclass="math inline">\(c\)</span> is convex and <spanclass="math inline">\(C^1\)</span>, we know the cost can be representedas the infimum of the path cost.</p><p>We has the following transform sequence: <spanclass="math display">\[\pi_0=\mu \otimes \nu \xrightarrow{\Phi} g^1 \xrightarrow{\Psi} \pi_1\xrightarrow{\Phi} g^2 \xrightarrow{\Psi} \cdots \xrightarrow{\Phi}g^{k} \xrightarrow{\Psi} \pi_{k}\]</span> where <span class="math inline">\(\Psi\)</span> is pushforward<span class="math inline">\(\Psi(g)=(\text{Id}, g)_*\mu\)</span>.</p><p>For <span class="math inline">\(\Phi\)</span>, given <spanclass="math inline">\(\pi\)</span>, <spanclass="math inline">\(v\)</span> is calculated by: <spanclass="math display">\[v_t(x) = \mathbb{E}_{(x_0,x_1)\sim \pi} \left[x_1-x_0 | x = t x_0 +(1-t) x_1\right]\]</span> And <span class="math inline">\(g\)</span> is determined bythe following equation: <span class="math display">\[\begin{cases}\frac{d}{dt}g_t(x) = v_t(g_t(x)) \\g_0(x)=x.\end{cases}\]</span></p><p><strong>Theorem</strong>:</p><p><span class="math display">\[C(\pi_{k+1}) \leq C(\pi_k)\, \text{for all } k\geq 1\]</span></p><p><strong>Proof</strong>: <span class="math display">\[\begin{align*}C(\pi_{k+1}) &amp;= \mathbb{E}_{(x_0,x_1)\sim \pi_{k+1}}\left[c(x_1-x_0)\right]\\&amp;=\mathbb{E}_{(x_0,x_1)\sim \pi_{k+1}} \left[c \left(\int_0^1v_t^{k+1}(g_t^{k+1}(x_0))\right) dt\right] \\&amp;\leq \mathbb{E}_{(x_0,x_1)\sim \pi_{k+1}} \left[\int_0^1c\left(v_t^{k+1}(g_t^{k+1}(x_0))\right) dt\right] \quad \text{(byJensen&#39;s inequality)}\\&amp;= \int_0^1 \mathbb{E}_{(x_0,x_1)\sim \pi_{k+1}}\left[c\left(v_t^{k+1}(g_t^{k+1}(x_0))\right)\right] dt\\&amp;= \int_0^1 \mathbb{E}_{x_t \sim \rho_t^{k+1}}\left[c\left(v_t^{k+1}(x_t)\right)\right] dt\\&amp;= \int_0^1 \mathbb{E}_{x_t \sim \rho_t^k}\left[c\left(v_t^{k+1}(x_t)\right)\right] dt\\&amp;= \mathbb{E}_{(x_0,x_1)\sim \pi_k} \left[\int_0^1c\left(v_t^{k+1}(x_t)\right) dt\right]\\&amp;= \mathbb{E}_{(x_0,x_1)\sim \pi_k} \left[\int_0^1c\left(\mathbb{E}[x_1-x_0|x_t] \right) dt\right]\\&amp;\leq \mathbb{E}_{(x_0,x_1)\sim \pi_k} \left[\int_0^1\mathbb{E}[c(x_1-x_0)|x_t] dt\right] \quad \text{(Jensen)}\\&amp;= \mathbb{E}_{(x_0,x_1)\sim \pi_k} \left[c(x_1-x_0)\right] \\&amp;= C(\pi_k)\end{align*}\]</span></p><p>Note that <span class="math inline">\(\rho_t\)</span> as the marginaldistribution of <span class="math inline">\(x_t = t x_1 +(1-t)x_0\)</span>, <span class="math inline">\(\rho_{t}&#39;\)</span> isthe distribution generated by the marginal vector field <spanclass="math inline">\(v_t^{k+1}\)</span>. They are the same since thecontinuity equation is satisfied.</p><p><strong>Proof</strong></p><p>For all <span class="math inline">\(\phi\in C_0(\Omega)\)</span>,</p><p><span class="math display">\[\begin{align*}\frac{d}{dt} \mathbb{E}_{(x_0,x_1)\sim \pi^k}[\phi(x_t)] &amp;=\mathbb{E}_{(x_0,x_1)\sim \pi^k} \left[\nabla \phi(x_t) \cdot (x_1 -x_0)\right] \\&amp;=\mathbb{E}_{x\sim \rho_t^k} \left[\nabla \phi(x)\mathbb{E}_{(x_0,x_1)\sim \pi^k}[x_1 - x_0 | x_t = x]\right] \\&amp;=\mathbb{E}_{x\sim \rho_t^k} \left[\nabla \phi(x) \cdotv_t^{k+1}(x)\right]\end{align*}\]</span></p><p>Solves the weak form of the continuity equation.</p><hr /><p>This is the key construct in the rectified flow.</p><h4 id="straightness">Straightness</h4><p>Define <span class="math display">\[S(g)=\int_0^1 \mathbb{E}_{(x_0,x_1)\sim \pi_{g_1}}\left[\|x_1-x_0-v_t(x_t)\|^2\right] dt\]</span> as the straightness of the flow <spanclass="math inline">\(g\)</span>.</p><p>Theorem 3.7 in <a href="https://arxiv.org/abs/2209.03003">FlowStraight and Fast</a> states: <span class="math display">\[\min_{k\in\{1,\cdots,K\}} S(Z^k) \leq \frac{\mathbb{E}_{(x_0,x_1)\sim\pi_0}[\|x_1-x_0\|^2]}{K}\]</span></p>]]></content>
    
    
    <categories>
      
      <category>Machine Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Generative Models</tag>
      
      <tag>Flow Matching</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Differential Manifold</title>
    <link href="/2025/08/04/Differential-Manifold/"/>
    <url>/2025/08/04/Differential-Manifold/</url>
    
    <content type="html"><![CDATA[<h2 id="references">References</h2><ul><li><p>John Lee, Introduction to Smooth Manifolds</p></li><li><p>Loring Tu, An Introduction to Manifolds</p></li><li><p>Victor Guillemin and Alan Pollack, Differential Topology</p></li></ul><p>For Chinese reader, you can refer to the websitehttp://staff.ustc.edu.cn/~wangzuoq/Courses/23F-Manifolds/#notes forChinese course note in USTC.</p><h2 id="manifolds-and-smooth-maps">Manifolds and Smooth maps</h2><h3 id="topological-manifolds">Topological Manifolds</h3><p>Let <span class="math inline">\(M\)</span> be a<strong>Hausdorff</strong> and <strong>second countable</strong>topological space that is locally homeomorphic to Euclidean space <spanclass="math inline">\(\mathbb{R}^n\)</span>. We say that <spanclass="math inline">\(M\)</span> is a <strong>manifold of dimension<span class="math inline">\(n\)</span></strong> if <spanclass="math inline">\(\forall p \in M, \exists U \subseteq M,\exists\phi: U \to \mathbb{R}^n\)</span> such that <spanclass="math inline">\(\phi\)</span> is a homeomorphism onto its imageand <span class="math inline">\(\phi(U)\)</span> is an open subset of<span class="math inline">\(\mathbb{R}^n\)</span>. Sometime we denote itas <span class="math inline">\(M^n\)</span>.</p><p>We denote <span class="math inline">\((U,\phi)\)</span> as acoordinate chart, <span class="math inline">\(\phi\)</span> as acoordinate map, and <span class="math inline">\(\phi^{-1}\)</span> as aparameterization of <span class="math inline">\(U\)</span>.</p><h3 id="atlas-and-ck-structure">Atlas and <spanclass="math inline">\(C^{k}\)</span>-structure</h3><p>An <strong>atlas</strong> on a topological manifold <spanclass="math inline">\(M\)</span> is a collection of coordinate charts<span class="math inline">\(\mathcal{A} = \{(U_\alpha,\phi_\alpha)\}_{\alpha \in I}\)</span> where <spanclass="math inline">\(\bigcup_{\alpha \in I} U_\alpha = M\)</span>.</p><p>Let <span class="math inline">\((U,\phi)\)</span> and <spanclass="math inline">\((V, \psi)\)</span> be two coordinate charts in theatlas <span class="math inline">\(\mathcal{A}\)</span>. The<strong>transition map</strong> from <span class="math inline">\((U,\phi)\)</span> to <span class="math inline">\((V, \psi)\)</span> isdefined as <span class="math inline">\(\psi \circ \phi^{-1}: \phi(U \capV) \to \psi(U \cap V)\)</span>.</p><p>We call the transition map <strong><spanclass="math inline">\(C^{k}\)</span> compatible</strong> if it is a<span class="math inline">\(C^{k}\)</span> function. And we say that theatlas <span class="math inline">\(\mathcal{A}\)</span> is a<strong><span class="math inline">\(C^{k}\)</span>-atlas</strong> if<span class="math inline">\(\forall (U, \phi), (V, \psi) \in\mathcal{A}, U\cap V \neq \emptyset\)</span>, the transition map <spanclass="math inline">\(\psi \circ \phi^{-1}\in C^{k}(\phi(U \cap V)\cap\psi(U \cap V))\)</span>.</p><p>We say <span class="math inline">\((U, \phi)\)</span> are<strong><span class="math inline">\(C^{k}\)</span>-compatible with <spanclass="math inline">\(\mathcal{A}\)</span></strong> if it is <spanclass="math inline">\(C^{k}\)</span> compatible with every chart in<span class="math inline">\(\mathcal{A}\)</span>.</p><p>Let <span class="math inline">\(\mathcal{A}\)</span> be a <spanclass="math inline">\(C^{k}\)</span>-atlas on a topological manifold<span class="math inline">\(M\)</span>. Maximal extension of <spanclass="math inline">\(\mathcal{A}\)</span>: <spanclass="math inline">\(\overline{\mathcal{A}} = \{(U, \phi) : (U, \phi)\text{ is } C^{k} \text{-compatible with } \mathcal{A}\}\)</span>. It isunique by definition and is a <spanclass="math inline">\(C^{k}\)</span>-atlas on <spanclass="math inline">\(M\)</span>.</p><p>And it is not hard to prove that if <spanclass="math inline">\(\overline{\mathcal{A}} =\overline{\mathcal{B}}\)</span> for another <spanclass="math inline">\(C^{k}\)</span>-atlas <spanclass="math inline">\(\mathcal{B}\)</span>, then <spanclass="math inline">\(\mathcal{A} \cup \mathcal{B}\)</span> is also a<span class="math inline">\(C^{k}\)</span>-atlas on <spanclass="math inline">\(M\)</span>.</p><p>Actually, Let <span class="math inline">\(\mathcal{F}\)</span> be thecollection of all <span class="math inline">\(C^{k}\)</span>-compatiblecoordinate charts on <span class="math inline">\(M\)</span>. Then <spanclass="math inline">\((\mathcal{F}, \subseteq)\)</span> is a directedset. And <span class="math inline">\(\overline{\mathcal{A}}\)</span> isthe maximal element of <spanclass="math inline">\(\mathcal{F}\)</span>.</p><p>We call <span class="math inline">\(\overline{\mathcal{A}}\)</span> amaximal <span class="math inline">\(C^{k}\)</span>-atlas or a<strong><span class="math inline">\(C^{k}\)</span>-structure</strong> on<span class="math inline">\(M\)</span>. A manifold with a <spanclass="math inline">\(C^{k}\)</span>-structure is called a <strong><spanclass="math inline">\(C^{k}\)</span>-manifold</strong>.</p><h3 id="smooth-manifolds">Smooth Manifolds</h3><p>A <strong>smooth manifold</strong> is a <spanclass="math inline">\(C^{\infty}\)</span>-manifold.</p><h3 id="coordinate-functions">Coordinate Functions</h3><p>Let <span class="math inline">\((U, \phi)\)</span> be a coordinatechart on a <span class="math inline">\(n\)</span>-manifold <spanclass="math inline">\(M\)</span>. The <strong>coordinatefunctions</strong> are the components of the coordinate map <spanclass="math inline">\(\phi: U \to \mathbb{R}^n\)</span>, denoted as<span class="math inline">\(\phi = (\phi^1, \phi^2, \ldots,\phi^n)\)</span>. We call the coordinate functions <spanclass="math inline">\(\phi^i: U \to \mathbb{R}\)</span> the<strong><span class="math inline">\(i\)</span>-th coordinatefunction</strong>.</p><p>For <span class="math inline">\(p\in U\)</span>, we write <spanclass="math inline">\(\phi(p) = (x^1, x^2, \ldots, x^n)\)</span>, where<span class="math inline">\(x^i = \phi^i(p)\)</span>.</p><p>And we call <span class="math inline">\((x^1, x^2, \ldots,x^n)\)</span> the local coordinates of <spanclass="math inline">\(p\)</span> in the chart <spanclass="math inline">\((U, \phi)\)</span>.</p><h3 id="tangent-vectors-and-tangent-spaces">Tangent Vectors and TangentSpaces</h3><h4 id="germs-of-functions">Germs of functions</h4><p>A <strong>germ of a function</strong> at a point <spanclass="math inline">\(p \in M\)</span> is an equivalence class offunctions defined on a neighborhood of <spanclass="math inline">\(p\)</span>. Two functions <spanclass="math inline">\(f\)</span> and <spanclass="math inline">\(g\)</span> are equivalent if they agree on someneighborhood of <span class="math inline">\(p\)</span>.</p><p>Formally, let <span class="math inline">\((f,U)\)</span> and <spanclass="math inline">\((g,V)\)</span> be two functions defined onneighborhoods <span class="math inline">\(U\)</span> and <spanclass="math inline">\(V\)</span> of <spanclass="math inline">\(p\)</span>. If there exists a neighborhood <spanclass="math inline">\(W \subseteq U \cap V\)</span> such that <spanclass="math inline">\(f|_W = g|_W\)</span>, then we say that the germ of<span class="math inline">\(f\)</span> at <spanclass="math inline">\(p\)</span> is equal to the germ of <spanclass="math inline">\(g\)</span> at <spanclass="math inline">\(p\)</span>, denoted as <spanclass="math inline">\((f,U) \sim (g,V)\)</span>.</p><p>A germ of a function at <span class="math inline">\(p\)</span> isdenoted as <span class="math inline">\([f]_p\)</span>, <spanclass="math inline">\([f]\)</span> or <spanclass="math inline">\(f\)</span> if the context is clear.</p><p>We denote <span class="math inline">\(C^{\infty}_p(M)\)</span> or<span class="math inline">\(C^{\infty}_p\)</span> as the set of germs ofsmooth functions at <span class="math inline">\(p \in M\)</span>.Formally, <span class="math inline">\(C^{\infty}_p(M) = \{ [f]_p : f \inC^{\infty}(U), U \text{ is a neighborhood of } p \}\)</span>.</p><p>If we write <span class="math inline">\(f \inC^{\infty}_p(M)\)</span>, it means that <spanclass="math inline">\([f]_p\in C^{\infty}_p(M)\)</span>.</p><h4 id="definition">Definition</h4><p>There is three common definitions of tangent vectors on a manifold<span class="math inline">\(M\)</span>: 1. <strong>Tangent Vector asDerivation</strong>: A tangent vector at a point <spanclass="math inline">\(p \in M\)</span> is a linear map <spanclass="math inline">\(v: C^{\infty}_p(M) \to \mathbb{R}\)</span> thatsatisfies the Leibniz rule: <span class="math inline">\(v(fg) = v(f)g(p)+ f(p)v(g)\)</span> for all <span class="math inline">\(f, g \inC^{\infty}_p(M)\)</span>. 2. <strong>Tangent Vector as Equivalence Classof Curves</strong>: A tangent vector at a point <spanclass="math inline">\(p \in M\)</span> is an equivalence class of curves<span class="math inline">\(\gamma: (-\epsilon, \epsilon) \to M\)</span>such that <span class="math inline">\(\gamma(0) = p\)</span>. 3.<strong>Tangent Vector as Derivative of Coordinate Functions</strong>: Atangent vector at a point <span class="math inline">\(p \in M\)</span>is a vector in the tangent space <spanclass="math inline">\(T_pM\)</span> defined as the set of allderivations at <span class="math inline">\(p\)</span> or equivalenceclasses of curves through <span class="math inline">\(p\)</span>.</p><p>The <strong>tangent space</strong> at a point <spanclass="math inline">\(p \in M\)</span>, denoted as <spanclass="math inline">\(T_pM\)</span>, is the set of all tangent vectorsat <span class="math inline">\(p\)</span>. It is a vector space over<span class="math inline">\(\mathbb{R}\)</span> of dimension <spanclass="math inline">\(n\)</span>, same as the dimension of the manifold<span class="math inline">\(M\)</span>.</p><p>We shall mainly focus on the first definition of tangent vectors asderivations, which is the most general and abstract definition.</p><p>The tangent space is equipped with a natural <spanclass="math inline">\(\mathbb{R}\)</span>-vector space structure.</p><h5 id="coordinate-expression">Coordinate Expression</h5><p>Let <span class="math inline">\((U,x)=(U,x^1,\ldots,x^n)\)</span> bea coordinate chart containing <spanclass="math inline">\(p\)</span>.</p><p>Consider <span class="math inline">\(\left.\frac{\partial}{\partialx^i}\right|_p: C^{\infty}_p(M)\to \mathbb{R}\)</span> act on <spanclass="math inline">\(f:U\to \mathbb R\)</span> defined as <spanclass="math display">\[\left.\frac{\partial}{\partialx^i}\right|_p([f])=\left.\frac{\partial(f\circ x^{-1})}{\partialx^i}\right |_{x(p)}\]</span></p><p>We know that <span class="math inline">\(\left\{\left.\frac{\partial}{\partialx^{1}}\right|_p,\ldots,\left.\frac{\partial}{\partialx^n}\right|_p\right \}\)</span> forms a basis of <spanclass="math inline">\(T_p M\)</span>, $T_pM = M=n $.</p><p>By definition, we can prove the Kronecker delta property: <spanclass="math display">\[ \left.\frac{\partial}{\partialx^i}\right|_p(x^j) = \frac{\partial x^j\circ x^{-1}}{\partial x^i}\bigg|_{x(p)} = \delta^j_i \]</span> where <spanclass="math inline">\(\delta^i_j\)</span> is the Kronecker delta, whichis <span class="math inline">\(1\)</span> if <spanclass="math inline">\(i=j\)</span> and <spanclass="math inline">\(0\)</span> otherwise.</p><p>So every <span class="math inline">\(X_p \in T_pM\)</span> can beexpressed as a linear combination of the basis vectors: <spanclass="math inline">\(X_p = \sum_{i=1}^n X_p^i\left.\frac{\partial}{\partial x^i}\right|_p\)</span>, where <spanclass="math inline">\(X_p^i =X_p(x^i)\in \mathbb{R}\)</span>.</p><p>The coordinate expression of a tangent vector <spanclass="math inline">\(X_p\)</span> at <spanclass="math inline">\(p\)</span> in the chart <spanclass="math inline">\((U,x)\)</span> is given by the tuple <spanclass="math inline">\((a_1, a_2, \ldots, a_n)\)</span>, where <spanclass="math inline">\(a_i\)</span> are the coefficients in the linearcombination.</p><h3 id="fiber-bundles">Fiber Bundles</h3><h4 id="definition-of-fiber-bundle">Definition of Fiber Bundle</h4><p>A <strong>fiber bundle</strong> is a structure <spanclass="math inline">\((E, M, \pi, F)\)</span> consisting of:</p><ul><li><strong>Total space</strong>: <span class="math inline">\(E\)</span>(a manifold)</li><li><strong>Base space</strong>: <span class="math inline">\(M\)</span>(a manifold)</li><li><strong>Bundle projection</strong>: <span class="math inline">\(\pi:E \to M\)</span> (a smooth surjective map)</li><li><strong>Typical fiber</strong>: <spanclass="math inline">\(F\)</span> (a manifold)</li></ul><p>such that the <strong>local triviality condition</strong> holds: foreach point <span class="math inline">\(p \in M\)</span>, there exists anopen neighborhood <span class="math inline">\(U\)</span> of <spanclass="math inline">\(p\)</span> and a diffeomorphism <spanclass="math inline">\(\phi: \pi^{-1}(U) \to U \times F\)</span>satisfying: <span class="math display">\[ \text{pr}_1 \circ \phi =\pi|_{\pi^{-1}(U)} \]</span> where <spanclass="math inline">\(\text{pr}_1: U \times F \to U\)</span> is theprojection onto the first factor.</p><p>The pair <span class="math inline">\((U, \phi)\)</span> is called a<strong>local trivialization</strong> or <strong>bundlechart</strong>.</p><p>For each <span class="math inline">\(p \in M\)</span>, the<strong>fiber</strong> over <span class="math inline">\(p\)</span> isdefined as <span class="math inline">\(F_p = \pi^{-1}(p)\)</span>. Thelocal triviality ensures that each fiber <spanclass="math inline">\(F_p\)</span> is diffeomorphic to the typical fiber<span class="math inline">\(F\)</span>.</p><p>Intuitively, a fiber bundle describes a space that ‘locally lookslike a product’ but may have global twisting, like the Möbius strip overa circle.</p><h4 id="vector-bundles">Vector Bundles</h4><p>A <strong>vector bundle</strong> is a fiber bundle <spanclass="math inline">\((E, M, \pi, \mathbb{R}^k)\)</span> where:</p><ol type="1"><li>Each fiber <span class="math inline">\(F_p = \pi^{-1}(p)\)</span>has the structure of a <spanclass="math inline">\(k\)</span>-dimensional real vector space.</li><li>The local trivializations <span class="math inline">\(\phi:\pi^{-1}(U) \to U \times \mathbb{R}^k\)</span> are<strong>linear</strong> on each fiber, meaning that for each <spanclass="math inline">\(p \in U\)</span>, the restriction <spanclass="math inline">\(\phi|_{F_p}: F_p \to \{p\} \times \mathbb{R}^k\cong \mathbb{R}^k\)</span> is a vector space isomorphism.</li></ol><p>The integer <span class="math inline">\(k\)</span> is called the<strong>rank</strong> of the vector bundle.</p><h3 id="tangent-bundle">Tangent Bundle</h3><p>The <strong>tangent bundle</strong> of a manifold <spanclass="math inline">\(M\)</span>, denoted as <spanclass="math inline">\(TM\)</span>, is the vector bundle whose totalspace is the disjoint union of all tangent spaces: <spanclass="math display">\[ TM = \bigcup_{p \in M} T_pM = \{(p, X_p) : p \inM, X_p \in T_pM\}\]</span></p><p>The <strong>bundle projection</strong> <spanclass="math inline">\(\pi: TM \to M\)</span> is defined by <spanclass="math inline">\(\pi(p, X_p) = p\)</span>.</p><p><strong>Local Trivialization</strong>: Let <spanclass="math inline">\((U, x = (x^1, \ldots, x^n))\)</span> be acoordinate chart on <span class="math inline">\(M\)</span>. The tangentbundle can be locally trivialized over <spanclass="math inline">\(U\)</span> by the map: <spanclass="math display">\[ \Phi: \pi^{-1}(U) \to U \times \mathbb{R}^n\]</span> <span class="math display">\[ \Phi(p, X_p) = \left(p, (X_p^1,\ldots, X_p^n)\right) \]</span> where <span class="math inline">\(X_p =\sum_{i=1}^n X_p^i \left.\frac{\partial}{\partial x^i}\right|_p\)</span>is the coordinate representation of the tangent vector <spanclass="math inline">\(X_p\)</span>.</p><p><strong>Manifold Structure</strong>: The tangent bundle <spanclass="math inline">\(TM\)</span> is a smooth manifold of dimension<span class="math inline">\(2n\)</span>, where <spanclass="math inline">\(n\)</span> is the dimension of <spanclass="math inline">\(M\)</span>. The coordinate charts on <spanclass="math inline">\(TM\)</span> are given by <spanclass="math inline">\((U \times \mathbb{R}^n, \Phi^{-1})\)</span> where<span class="math inline">\((U, x)\)</span> ranges over all coordinatecharts on <span class="math inline">\(M\)</span>.</p><p><strong>Transition Maps</strong>: If <span class="math inline">\((U,x)\)</span> and <span class="math inline">\((V, y)\)</span> are twooverlapping coordinate charts on <span class="math inline">\(M\)</span>,the transition map between the corresponding bundle charts is: <spanclass="math display">\[ \Phi_V \circ \Phi_U^{-1}: (U \cap V) \times\mathbb{R}^n \to (U \cap V) \times \mathbb{R}^n \]</span> <spanclass="math display">\[ (p, (v^1, \ldots, v^n)) \mapsto \left(p,\left(\sum_{i=1}^n v^i \frac{\partial y^j}{\partialx^i}\bigg|_p\right)_{j=1}^n\right) \]</span></p><p>This map is smooth, confirming that <spanclass="math inline">\(TM\)</span> has a smooth manifold structure.</p><h3 id="sections-of-fiber-bundles">Sections of Fiber Bundles</h3><h4 id="definition-of-sections">Definition of Sections</h4><p>Let <span class="math inline">\(\pi: E \to M\)</span> be a fiberbundle projection. A <strong>section</strong> of <spanclass="math inline">\(E\)</span> is a map <span class="math inline">\(s:M \to E\)</span> such that <span class="math inline">\(\pi \circ s =\text{id}_M\)</span>. In other words, for each point <spanclass="math inline">\(p \in M\)</span>, we have <spanclass="math inline">\(s(p) \in E_p\)</span> (the fiber over <spanclass="math inline">\(p\)</span>).</p><p>A section <span class="math inline">\(s\)</span> is called<strong>smooth</strong> if <span class="math inline">\(s: M \toE\)</span> is a smooth map between manifolds.</p><h4 id="local-expression-of-smoothness">Local Expression ofSmoothness</h4><p>Let <span class="math inline">\((U, \phi)\)</span> be a localtrivialization of <span class="math inline">\(E\)</span> over an openset <span class="math inline">\(U \subseteq M\)</span>, where <spanclass="math inline">\(\phi: \pi^{-1}(U) \to U \times F\)</span> for sometypical fiber <span class="math inline">\(F\)</span>. Any section <spanclass="math inline">\(s\)</span> over <spanclass="math inline">\(U\)</span> can be written as: <spanclass="math display">\[ \phi \circ s|_U: U \to U \times F \]</span>which has the form <span class="math inline">\(p \mapsto (p,f(p))\)</span> for some function <span class="math inline">\(f: U \toF\)</span>.</p><p>Sometimes we just say <span class="math inline">\(f\)</span> is asection of <span class="math inline">\(E\)</span> over <spanclass="math inline">\(U\)</span>.</p><h4 id="space-of-sections">Space of Sections</h4><p>We denote by <span class="math inline">\(\Gamma(E)\)</span> or <spanclass="math inline">\(\Gamma(M, E)\)</span> the space of all smoothsections of the fiber bundle <span class="math inline">\(E \toM\)</span>.</p><p><strong>For General Fiber Bundles</strong>: The space <spanclass="math inline">\(\Gamma(E)\)</span> has the structure of a set withpointwise operations (when they make sense on the typical fiber).</p><p><strong>For Vector Bundles</strong>: When <spanclass="math inline">\(E \to M\)</span> is a vector bundle with typicalfiber <span class="math inline">\(\mathbb{R}^k\)</span>, the space <spanclass="math inline">\(\Gamma(E)\)</span> has additional algebraicstructures:</p><ol type="1"><li><p><strong>Vector space structure</strong>: For sections <spanclass="math inline">\(s_1, s_2 \in \Gamma(E)\)</span> and scalars <spanclass="math inline">\(a, b \in \mathbb{R}\)</span>: <spanclass="math display">\[ (as_1 + bs_2)(p) = as_1(p) + bs_2(p) \in E_p\]</span></p></li><li><p><strong><span class="math inline">\(C^\infty(M)\)</span>-modulestructure</strong>: For a smooth function <span class="math inline">\(f\in C^\infty(M)\)</span> and a section <span class="math inline">\(s \in\Gamma(E)\)</span>: <span class="math display">\[ (fs)(p) = f(p) \cdots(p) \in E_p \]</span></p></li></ol><p>These structures exist because each fiber <spanclass="math inline">\(E_p\)</span> is a vector space, allowing us toperform linear operations.</p><h3 id="differential">Differential</h3><p>Let <span class="math inline">\(F: M^m \to N^n\)</span> be a smoothmap between manifolds <span class="math inline">\(M\)</span> and <spanclass="math inline">\(N\)</span>. The <strong>differential</strong> of<span class="math inline">\(F\)</span> at a point <spanclass="math inline">\(p \in M\)</span>, denoted as <spanclass="math inline">\(dF_p: T_pM \to T_{F(p)}N\)</span>, is a linear mapbetween tangent spaces induced by the pushforward of <spanclass="math inline">\(F\)</span>.</p><p>It is defined as follows: for any tangent vector <spanclass="math inline">\(X_p \in T_pM\)</span> and any smooth function<span class="math inline">\(f \in C^{\infty}_{F(p)}(N)\)</span>, <spanclass="math display">\[ dF_p(X_p)f = X_p(f \circ F) \]</span></p><p><strong>Local Coordinate Expression</strong>: Let <spanclass="math inline">\((U, x)\)</span> and <spanclass="math inline">\((V, y)\)</span> be coordinate charts around <spanclass="math inline">\(p\)</span> and <spanclass="math inline">\(F(p)\)</span> respectively, and let <spanclass="math inline">\(\tilde F = y \circ F \circ x^{-1}\)</span> be thelocal representation of <span class="math inline">\(F\)</span>. Then:<span class="math display">\[\begin{align*}dF_p(X_p) &amp;= \sum_{j=1}^n X_p(y^j\circ F) \frac{\partial}{\partialy^j} \bigg|_{F(p)}\\&amp;= \sum_{j=1}^n \sum_{i=1}^m X_p^i \frac{\partial \tildeF^j}{\partial x^i} \bigg|_{x(p)} \frac{\partial}{\partial y^j}\bigg|_{F(p)}\\&amp;= \sum_{j=1}^n \left(\sum_{i=1}^m X_p^i \frac{\partial \tildeF^j}{\partial x^i} \bigg|_{x(p)}\right) \frac{\partial}{\partial y^j}\bigg|_{F(p)}\end{align*}\]</span></p><p>In matrix form, if <span class="math inline">\(J_F(p) =\left(\frac{\partial \tilde F^j}{\partial x^i}\bigg|_{x(p)}\right)\)</span> is the Jacobian matrix of <spanclass="math inline">\(F\)</span> at <spanclass="math inline">\(p\)</span>, then: <span class="math display">\[dF_p(X_p) = J_F(p) X_p \]</span> where <spanclass="math inline">\(X_p\)</span> is considered as a column vector inlocal coordinates.</p><h3 id="vector-fields">Vector Fields</h3><p>A <strong>vector field</strong> on a manifold <spanclass="math inline">\(M\)</span> is a smooth section of the tangentbundle <span class="math inline">\(TM\)</span>. That is, a vector field<span class="math inline">\(X\)</span> is a smooth map <spanclass="math inline">\(X: M \to TM\)</span> such that <spanclass="math inline">\(\pi \circ X = \text{id}_M\)</span>, where <spanclass="math inline">\(\pi: TM \to M\)</span> is the bundleprojection.</p><p>Equivalently, a vector field assigns to each point <spanclass="math inline">\(p \in M\)</span> a tangent vector <spanclass="math inline">\(X_p \in T_pM\)</span> in a smooth manner.</p><p><strong>Local Coordinate Expression</strong>:</p><p>The partial derivative operator <spanclass="math inline">\(\frac{\partial}{\partial x^i}\)</span> can beviewed as a vector field on <span class="math inline">\(M\)</span> inthe coordinate chart <span class="math inline">\((U, x)\)</span>, whereit acts on smooth functions <span class="math inline">\(f \inC^\infty(U)\)</span> by: <span class="math display">\[ \frac{\partialf}{\partial x^i} = \left.\frac{\partial(f\circ x^{-1})}{\partialx^i}\right|_{x(p)} \]</span></p><p>where <span class="math inline">\(x^i\)</span> is the <spanclass="math inline">\(i\)</span>-th coordinate function in the chart<span class="math inline">\((U, x)\)</span>.</p><p>In a coordinate chart <span class="math inline">\((U, x = (x^1,\ldots, x^n))\)</span>, a vector field <spanclass="math inline">\(X\)</span> can be expressed as: <spanclass="math display">\[ X = \sum_{i=1}^n X^i \frac{\partial}{\partialx^i} \]</span> where <span class="math inline">\(X^i: U \to\mathbb{R}\)</span> are smooth functions called the<strong>components</strong> of the vector field <spanclass="math inline">\(X\)</span> with respect to the coordinate chart<span class="math inline">\((U, x)\)</span>.</p><p>The smoothness of the vector field <spanclass="math inline">\(X\)</span> is equivalent to the smoothness of allits component functions <span class="math inline">\(X^i\)</span>.</p><p><strong>Space of Vector Fields</strong>: We denote by <spanclass="math inline">\(\mathfrak{X}(M)\)</span> or <spanclass="math inline">\(\Gamma^{\infty}(TM)\)</span> the space of allsmooth vector fields on <span class="math inline">\(M\)</span>. Thisspace is both a vector space over <spanclass="math inline">\(\mathbb{R}\)</span> and a module over the ring<span class="math inline">\(C^\infty(M)\)</span> of smooth functions on<span class="math inline">\(M\)</span>.</p><h3 id="partition-of-unity">Partition of Unity</h3><p>Partitions of unity are one of the most powerful tools in the theoryof smooth manifolds. They provide a way to smoothly “glue together”local constructions into global ones. For example, if we can define anobject (like a function, a metric, or a differential form) on each chartof an atlas, a partition of unity allows us to combine these localobjects into a single, globally defined smooth object on the entiremanifold. The fundamental building blocks for partitions of unity aresmooth bump functions.</p><h4 id="bump-functions">Bump Functions</h4><p>A <strong>bump function</strong> on a manifold <spanclass="math inline">\(M\)</span> is a smooth function <spanclass="math inline">\(\lambda: M \to \mathbb{R}\)</span> that is equalto 1 on some specified compact set and is zero outside of a slightlylarger open set containing it. The existence of such functions is acornerstone of analysis on manifolds, distinguishing smooth manifoldsfrom, for example, analytic manifolds where such functions do notexist.</p><p>First, let’s establish their existence on <spanclass="math inline">\(\mathbb{R}^n\)</span>. Consider the function <spanclass="math inline">\(\psi: \mathbb{R} \to \mathbb{R}\)</span> definedby: <span class="math display">\[\psi(t) = \begin{cases}e^{-1/t} &amp; \text{if } t &gt; 0 \\0 &amp; \text{if } t \le 0\end{cases}\]</span> This function is famously <spanclass="math inline">\(C^\infty\)</span> on all of <spanclass="math inline">\(\mathbb{R}\)</span>, including at <spanclass="math inline">\(t=0\)</span> where all its derivatives are zero.Using this, we can construct a smooth function on <spanclass="math inline">\(\mathbb{R}^n\)</span> that is positive on an openball and zero elsewhere. For example, the function <spanclass="math inline">\(\Psi: \mathbb{R}^n \to \mathbb{R}\)</span> definedby <span class="math display">\[\Psi(x) = \begin{cases}e^{-1/(1-\|x\|^2)} &amp; \text{if } \|x\| &lt; 1 \\0 &amp; \text{if } \|x\| \ge 1\end{cases}\]</span> is a smooth function that is positive on the open unit ball<span class="math inline">\(B(0,1)\)</span> and has its supportcontained in the closed unit ball <spanclass="math inline">\(\overline{B(0,1)}\)</span>.</p><p>By scaling and translating this function, we can create a bumpfunction for any ball. More generally, we have the following crucialexistence theorem for bump functions on manifolds:</p><p><strong>Theorem (Existence of Bump Functions):</strong> Let <spanclass="math inline">\(M\)</span> be a smooth manifold, <spanclass="math inline">\(K \subset M\)</span> be a compact set, and <spanclass="math inline">\(U \subset M\)</span> be an open set containing<span class="math inline">\(K\)</span>. Then there exists a smoothfunction <span class="math inline">\(\lambda: M \to [0, 1]\)</span> suchthat: 1. <span class="math inline">\(\lambda(p) = 1\)</span> for all<span class="math inline">\(p \in K\)</span>. 2. <spanclass="math inline">\(\text{supp}(\lambda) \subset U\)</span>.</p><p>Here, the <strong>support</strong> of a function <spanclass="math inline">\(f: M \to \mathbb{R}\)</span>, denoted <spanclass="math inline">\(\text{supp}(f)\)</span>, is the closure of the setof points where <span class="math inline">\(f\)</span> is non-zero:<span class="math display">\[\text{supp}(f) = \overline{\{p \in M \midf(p) \neq 0\}}\]</span> A function with compact support is a functionwhose support is a compact set.</p><h4 id="definition-of-a-partition-of-unity">Definition of a Partition ofUnity</h4><p>Let <span class="math inline">\(M\)</span> be a smooth manifold andlet <span class="math inline">\(\mathcal{U} = \{U_\alpha\}_{\alpha \inA}\)</span> be an open cover of <span class="math inline">\(M\)</span>.A family of smooth functions <span class="math inline">\(\{\rho_\alpha:M \to [0,1]\}_{\alpha \in A}\)</span> indexed by the same set <spanclass="math inline">\(A\)</span> is called a <strong>partition of unitysubordinate to the cover <spanclass="math inline">\(\mathcal{U}\)</span></strong> if it satisfies thefollowing conditions:</p><ol type="1"><li><strong>Subordination:</strong> For each <spanclass="math inline">\(\alpha \in A\)</span>, the support of <spanclass="math inline">\(\rho_\alpha\)</span> is contained in <spanclass="math inline">\(U_\alpha\)</span>. That is, <spanclass="math inline">\(\text{supp}(\rho_\alpha) \subsetU_\alpha\)</span>.</li><li><strong>Local Finiteness:</strong> The cover of supports <spanclass="math inline">\(\{\text{supp}(\rho_\alpha)\}_{\alpha \inA}\)</span> is locally finite. This means that for every point <spanclass="math inline">\(p \in M\)</span>, there exists a neighborhood<span class="math inline">\(V\)</span> of <spanclass="math inline">\(p\)</span> that intersects only a finite number ofthe sets <spanclass="math inline">\(\text{supp}(\rho_\alpha)\)</span>.</li><li><strong>Sum to Unity:</strong> For every point <spanclass="math inline">\(p \in M\)</span>, the sum of the function valuesat that point is 1. <span class="math display">\[\sum_{\alpha \in A}\rho_\alpha(p) = 1\]</span> (The local finiteness condition ensures thatfor any <span class="math inline">\(p\)</span>, this is a finite sum ina neighborhood of <span class="math inline">\(p\)</span>, and thus thetotal sum is a well-defined smooth function.)</li></ol><h4 id="existence-of-partitions-of-unity">Existence of Partitions ofUnity</h4><p>The main theorem guarantees that such partitions of unity alwaysexist on manifolds that satisfy the standard topologicalassumptions.</p><p><strong>Theorem (Existence of Partitions of Unity):</strong> Let<span class="math inline">\(M\)</span> be a smooth manifold (which isHausdorff and second-countable by our definition). For any open cover<span class="math inline">\(\mathcal{U}\)</span> of <spanclass="math inline">\(M\)</span>, there exists a smooth partition ofunity subordinate to <spanclass="math inline">\(\mathcal{U}\)</span>.</p><p><strong>Sketch of Proof:</strong> The proof relies on the topologicalproperty of paracompactness, which is guaranteed for Hausdorff,second-countable manifolds.</p><ol type="1"><li><strong>Find a good refinement:</strong> Since <spanclass="math inline">\(M\)</span> is second-countable and locallycompact, we can find a countable, locally finite open refinement <spanclass="math inline">\(\mathcal{V} = \{V_j\}_{j=1}^\infty\)</span> of theoriginal cover <span class="math inline">\(\mathcal{U}\)</span> suchthat each <span class="math inline">\(\overline{V_j}\)</span> is compactand for each <span class="math inline">\(j\)</span>, there is some <spanclass="math inline">\(\alpha_j\)</span> with <spanclass="math inline">\(\overline{V_j} \subset U_{\alpha_j}\)</span>.</li><li><strong>Find another refinement:</strong> We can construct anotheropen cover <span class="math inline">\(\mathcal{W} =\{W_j\}_{j=1}^\infty\)</span> such that for each <spanclass="math inline">\(j\)</span>, <spanclass="math inline">\(\overline{W_j}\)</span> is compact and <spanclass="math inline">\(\overline{W_j} \subset V_j\)</span>.</li><li><strong>Construct bump functions:</strong> For each <spanclass="math inline">\(j\)</span>, since <spanclass="math inline">\(\overline{W_j}\)</span> is a compact set containedin the open set <span class="math inline">\(V_j\)</span>, we can use thebump function existence theorem to find a smooth function <spanclass="math inline">\(\psi_j: M \to [0,1]\)</span> such that <spanclass="math inline">\(\psi_j \equiv 1\)</span> on <spanclass="math inline">\(\overline{W_j}\)</span> and <spanclass="math inline">\(\text{supp}(\psi_j) \subset V_j\)</span>.</li><li><strong>Sum the bump functions:</strong> Define a function <spanclass="math inline">\(\Psi: M \to \mathbb{R}\)</span> by <spanclass="math inline">\(\Psi(p) = \sum_{j=1}^\infty \psi_j(p)\)</span>.Since the cover <span class="math inline">\(\{V_j\}\)</span> (and thusthe supports of the <span class="math inline">\(\psi_j\)</span>) islocally finite, this sum is finite in a neighborhood of any point, so<span class="math inline">\(\Psi\)</span> is a smooth function. Since<span class="math inline">\(\{W_j\}\)</span> is a cover, for any <spanclass="math inline">\(p \in M\)</span>, <span class="math inline">\(p\in W_j\)</span> for some <span class="math inline">\(j\)</span>, whichmeans <span class="math inline">\(\psi_j(p) = 1\)</span>. Therefore,<span class="math inline">\(\Psi(p) &gt; 0\)</span> for all <spanclass="math inline">\(p \in M\)</span>.</li><li><strong>Normalize:</strong> For each <spanclass="math inline">\(j\)</span>, define <spanclass="math inline">\(\rho_j(p) = \frac{\psi_j(p)}{\Psi(p)}\)</span>.This family <span class="math inline">\(\{\rho_j\}_{j=1}^\infty\)</span>is a partition of unity subordinate to the cover <spanclass="math inline">\(\mathcal{V}\)</span>, and therefore alsosubordinate to the original cover <spanclass="math inline">\(\mathcal{U}\)</span>.</li></ol><h4 id="applications-of-partitions-of-unity">Applications of Partitionsof Unity</h4><p>Partitions of unity are essential for extending local properties toglobal ones. Here are two classic applications:</p><ul><li><p><strong>Integration on Manifolds:</strong> To define the integralof a function <span class="math inline">\(f\)</span> on a manifold <spanclass="math inline">\(M\)</span>, one can use a partition of unity <spanclass="math inline">\(\{\rho_\alpha\}\)</span> subordinate to a cover ofcoordinate charts <span class="math inline">\(\{U_\alpha\}\)</span>. Theintegral is then defined as a sum of integrals over Euclidean space:<span class="math display">\[\int_M f \, dV = \sum_\alpha\int_{U_\alpha} (\rho_\alpha f) \, dV = \sum_\alpha\int_{\phi_\alpha(U_\alpha)} (\rho_\alpha f) \circ \phi_\alpha^{-1} \,dx^1 \cdots dx^n\]</span> The partition of unity ensures that each piece<span class="math inline">\((\rho_\alpha f)\)</span> is compactlysupported within a single chart, making the integral well-defined, andthat the sum captures the “whole” of <spanclass="math inline">\(f\)</span>.</p></li><li><p><strong>Existence of Riemannian Metrics:</strong> Any smoothmanifold admits a Riemannian metric. To prove this, one can define aEuclidean metric on each coordinate chart <spanclass="math inline">\((U_\alpha, \phi_\alpha)\)</span>. Using apartition of unity <span class="math inline">\(\{\rho_\alpha\}\)</span>subordinate to the chart cover, one can patch these local metricstogether into a global metric <span class="math inline">\(g\)</span> viaa weighted sum: <span class="math display">\[g = \sum_\alpha \rho_\alphag_\alpha\]</span> where <span class="math inline">\(g_\alpha\)</span> isthe pullback of the Euclidean metric from <spanclass="math inline">\(\mathbb{R}^n\)</span> to <spanclass="math inline">\(U_\alpha\)</span>. The result is a smoothlyvarying inner product on each tangent space, i.e., a Riemannianmetric.</p></li></ul><h3id="local-behavior-of-smooth-maps-submersion-immersion-embedding">LocalBehavior of Smooth Maps (Submersion, Immersion, Embedding)</h3><h3 id="homotopy">Homotopy</h3><h3 id="sards-theorem">Sard’s theorem</h3><h3 id="submanifold">Submanifold</h3><h3 id="whitney-embedding-theorem">Whitney Embedding Theorem</h3><h3 id="tubular-neighborhood-theorem">Tubular Neighborhood Theorem</h3><h3 id="manifold-with-boundary">Manifold with Boundary</h3><p>First, we denote <span class="math inline">\(\mathbb R^n_+ = \{(x^1,\ldots, x^n) \in \mathbb R^n : x^n \geq 0\}\)</span> as the half-spacein <span class="math inline">\(\mathbb R^n\)</span>.</p><p>A <strong>manifold with boundary</strong> is a topological space<span class="math inline">\(M\)</span> that is<strong>Hausdorff</strong>, <strong>second countable</strong> and <spanclass="math inline">\(\forall p\in M\)</span>, there exists aneighborhood <span class="math inline">\(U\)</span> of <spanclass="math inline">\(p\)</span> that is homeomorphic to an open subsetof <span class="math inline">\(\mathbb R^n_+\)</span>.</p><p>or equivalently, a <strong>Hausdorff</strong>, <strong>secondcountable</strong> topological space <spanclass="math inline">\(M\)</span> is a manifold with boundary if it islocally homeomorphic to <span class="math inline">\(\mathbbR^n_+\)</span> or <span class="math inline">\(\mathbb R^n\)</span>.</p><p>Let <span class="math inline">\(M\)</span> be a manifold withboundary. The <strong>boundary</strong> of <spanclass="math inline">\(M\)</span>, denoted as <spanclass="math inline">\(\partial M\)</span>, is defined as the set ofpoints in <span class="math inline">\(M\)</span> that is not locallyhomeomorphic to <span class="math inline">\(\mathbb R^n\)</span> i.e.,<span class="math display">\[ \partial M = \{ p \in M : \text{there isno neighborhood } U \cong \mathbb R^n \text{ around } p \} \]</span> and<span class="math inline">\(\text{int}(M) = M \setminus \partialM\)</span> is the interior of <spanclass="math inline">\(M\)</span>.</p><p>In other words, <span class="math inline">\(p\in \partial M\)</span>if and only if there exists a local coordinate chart <spanclass="math inline">\((U,\phi)\)</span> around <spanclass="math inline">\(p\)</span> such that <spanclass="math inline">\(\phi(U) \subseteq \mathbb R^n_+\)</span> and <spanclass="math inline">\(\phi(p) \in \partial \mathbb R^n_+= \{(x^1,\ldots, x^{n-1}, 0) : x^1, \ldots, x^{n-1} \in \mathbb R\}\)</span>.</p><p>We may denote <span class="math inline">\((M,\partial M)\)</span> fora manifold with boundary and <span class="math inline">\(M\)</span> canbe called as manifold without boundary. Sometime <spanclass="math inline">\(M\)</span> can be either situation depending onthe context.</p><p>Properties: - <span class="math inline">\(\partial M\)</span> is amanifold without boundary with dimension <spanclass="math inline">\(n-1\)</span>.</p><ul><li><p>The result of atlas on manifold with boundary issimilar.</p></li><li><p>For the tangent space of <span class="math inline">\((M,\partialM)\)</span> remains a full space of <span class="math inline">\(\mathbbR^n\)</span> at any point <span class="math inline">\(p\inM\)</span>.</p></li></ul><h2 id="transversality-and-intersection-theory">Transversality andIntersection Theory</h2>]]></content>
    
    
    <categories>
      
      <category>Mathematics</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Differential Geometry</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Topology</title>
    <link href="/2025/07/30/Topology/"/>
    <url>/2025/07/30/Topology/</url>
    
    <content type="html"><![CDATA[<h2 id="basic-concepts">Basic Concepts</h2><h3 id="topology-space">Topology Space</h3><p>A topology space is a set <span class="math inline">\(X\)</span> witha collection of open sets <spanclass="math inline">\(\mathcal{T}\)</span> that satisfy the followingproperties: 1. <span class="math inline">\(\emptyset \in\mathcal{T}\)</span> and <span class="math inline">\(X \in\mathcal{T}\)</span>. 2. if <span class="math inline">\(\{U_i\}_{i \inI} \subseteq \mathcal{T}\)</span>, then <spanclass="math inline">\(\bigcup_{i \in I} U_i \in \mathcal{T}\)</span>. 3.if <span class="math inline">\(U_1, U_2 \in \mathcal{T}\)</span>, then<span class="math inline">\(U_1 \cap U_2 \in \mathcal{T}\)</span>.</p><p>A topological space is denoted as <span class="math inline">\((X,\mathcal{T})\)</span>.</p><h3 id="basis-and-subbasis-for-a-topology">Basis and subbasis for aTopology</h3><p>A basis <span class="math inline">\(\mathcal{B}\)</span> for atopology on <span class="math inline">\(X\)</span> is a collection ofsets such that: 1. For each <span class="math inline">\(x \inX\)</span>, there exists a basis element <span class="math inline">\(B\in \mathcal{B}\)</span> such that <span class="math inline">\(x \inB\)</span>. 2. If <span class="math inline">\(B_1, B_2 \in\mathcal{B}\)</span> and <span class="math inline">\(x \in B_1 \capB_2\)</span>, then there exists a basis element <spanclass="math inline">\(B_3 \in \mathcal{B}\)</span> such that <spanclass="math inline">\(x \in B_3 \subseteq B_1 \cap B_2\)</span>.</p><p>A topology generated by a basis <spanclass="math inline">\(\mathcal{B}\)</span> is the collection of allunions of elements of <span class="math inline">\(\mathcal{B}\)</span>.Formally, <span class="math inline">\(\mathcal{T} = \{ U \subseteq X : U= \bigcup_{i \in I} B_i, B_i \in \mathcal{B}, I \text{ is an index set}\}\)</span>.</p><p>A subbasis for a topology on <span class="math inline">\(X\)</span>is a collection of sets <span class="math inline">\(\mathcal{S}\)</span>such that the collection of finite intersections of elements of <spanclass="math inline">\(\mathcal{S}\)</span> forms a basis for a topologyon <span class="math inline">\(X\)</span>. Formally, a subbasis <spanclass="math inline">\(\mathcal{S}\)</span> satisfies: 1. For each <spanclass="math inline">\(x \in X\)</span>, there exists a subbasis element<span class="math inline">\(S \in \mathcal{S}\)</span> such that <spanclass="math inline">\(x \in S\)</span>. 2. The collection of finiteintersections of elements of <spanclass="math inline">\(\mathcal{S}\)</span> generates a basis for thetopology on <span class="math inline">\(X\)</span>.</p><p>A topology generated by a subbasis <spanclass="math inline">\(\mathcal{S}\)</span> is denoted as <spanclass="math inline">\(\mathcal{T} = \{ U \subseteq X : U = \bigcup_{i\in I} \bigcap_{j=1}^{n_i} S_{ij}, S_{ij} \in \mathcal{S}, n_i \text{ isfinite}, I \text{ is an index set} \}\)</span>.</p><h3 id="countability-compactness-and-separation-axioms">Countability,Compactness and Separation Axioms</h3><p>A topological space is <strong>first countable</strong> if everypoint has a countable local base, meaning for each point <spanclass="math inline">\(x \in X\)</span>, there exists a countablecollection of open sets <spanclass="math inline">\(\{U_n\}_{n=1}^{\infty}\)</span> such that for anyopen set <span class="math inline">\(U\)</span> containing <spanclass="math inline">\(x\)</span>, there exists some <spanclass="math inline">\(n\)</span> such that <spanclass="math inline">\(U_n \subseteq U\)</span>.</p><p>A topological space is <strong>second countable</strong> if it has acountable base, meaning there exists a countable collection of open sets<span class="math inline">\(\mathcal{B}\)</span> such that every openset in the topology can be expressed as a union of sets from <spanclass="math inline">\(\mathcal{B}\)</span>.</p><p>A topological space is <strong>compact</strong> if every open coverhas a finite subcover, meaning for any collection of open sets <spanclass="math inline">\(\{U_i\}_{i \in I}\)</span> such that <spanclass="math inline">\(X = \bigcup_{i \in I} U_i\)</span>, there exists afinite subset <span class="math inline">\(J \subseteq I\)</span> suchthat <span class="math inline">\(X = \bigcup_{j \in J} U_j\)</span>.</p><p>A topological space is <span class="math inline">\(T_0\)</span>(Kolmogorov) if for any two distinct points <spanclass="math inline">\(x, y \in X\)</span>, there exists an open setcontaining one of the points but not the other. Intuitively, this meansthat a pair of point can be one-sidely distinguished by open sets forone of them.</p><p>A topological space is <span class="math inline">\(T_1\)</span>(Frechet) if for any two distinct points <span class="math inline">\(x,y \in X\)</span>, there exist open sets <spanclass="math inline">\(U_x\)</span> and <spanclass="math inline">\(U_y\)</span> such that <spanclass="math inline">\(x \in U_x\)</span> and <spanclass="math inline">\(y \notin U_x\)</span>, and <spanclass="math inline">\(y \in U_y\)</span> and <spanclass="math inline">\(x \notin U_y\)</span>. Intuitively, this meansthat points can be one-sidely separated by open sets for both ofthem.</p><p>A topological space is <span class="math inline">\(T_2\)</span>(Hausdorff) if for any two distinct points <spanclass="math inline">\(x, y \in X\)</span>, there exist disjoint opensets <span class="math inline">\(U_x\)</span> and <spanclass="math inline">\(U_y\)</span> such that <spanclass="math inline">\(x \in U_x\)</span> and <spanclass="math inline">\(y \in U_y\)</span>. This means that points can beseparated by disjoint open sets.</p><p>A topological space is <strong>regular</strong> if it is <spanclass="math inline">\(T_1\)</span> and for every point <spanclass="math inline">\(x\)</span> and closed set <spanclass="math inline">\(F\)</span> not containing <spanclass="math inline">\(x\)</span>, there exist disjoint open sets <spanclass="math inline">\(U\)</span> and <spanclass="math inline">\(V\)</span> such that <span class="math inline">\(x\in U\)</span> and <span class="math inline">\(F \subseteq V\)</span>.This means that points can be separated from closed sets by disjointopen sets.</p><p>A topological space is <strong>normal</strong> if for any twodisjoint closed sets <span class="math inline">\(A\)</span> and <spanclass="math inline">\(B\)</span>, there exist disjoint open sets <spanclass="math inline">\(U\)</span> and <spanclass="math inline">\(V\)</span> such that <span class="math inline">\(A\subseteq U\)</span> and <span class="math inline">\(B \subseteqV\)</span>. This means that closed sets can be separated by disjointopen sets.</p><p>A topological space is <span class="math inline">\(T_3\)</span>(regular Hausdorff) if it is both regular and Hausdorff. This means thatpoints can be separated from closed sets by disjoint open sets, anddistinct points can be separated by disjoint open sets. <spanclass="math inline">\(T_3\)</span> spaces are also normal.</p><p>Important properties of these separation axioms include: - <spanclass="math inline">\(T_0 \subseteq T_1 \subseteq T_2 \subseteqT_3\)</span>. - A single point set is closed in a <spanclass="math inline">\(T_1\)</span> space. - A <spanclass="math inline">\(T_2\)</span> space is normal if it is also regularsince a point is closed in a <span class="math inline">\(T_2\)</span>space.</p><h3 id="functions-and-homeomorphisms">Functions and Homeomorphisms</h3><p>A function <span class="math inline">\(f: X \to Y\)</span> betweentwo topological spaces <span class="math inline">\((X,\mathcal{T}_X)\)</span> and <span class="math inline">\((Y,\mathcal{T}_Y)\)</span> is <strong>open</strong> if for every open set<span class="math inline">\(U \in \mathcal{T}_X\)</span>, the image<span class="math inline">\(f(U)\)</span> is open in <spanclass="math inline">\(\mathcal{T}_Y\)</span>. In other words, <spanclass="math display">\[ f(U) \in \mathcal{T}_Y \text{ for all } U \in\mathcal{T}_X.\]</span></p><p>A function <span class="math inline">\(f: X \to Y\)</span> is<strong>closed</strong> if for every closed set <spanclass="math inline">\(C \in \mathcal{T}_X\)</span>, the image <spanclass="math inline">\(f(C)\)</span> is closed in <spanclass="math inline">\(\mathcal{T}_Y\)</span>. In other words, <spanclass="math display">\[ f(C) \in \mathcal{T}_Y \text{ for all } C \in\mathcal{T}_X.\]</span></p><p>A function <span class="math inline">\(f: (X, \mathcal{T}_X) \to (Y,\mathcal{T}_Y)\)</span> between topological spaces is<strong>continuous</strong> if for every open set <spanclass="math inline">\(V \in \mathcal{T}_Y\)</span>, the preimage <spanclass="math inline">\(f^{-1}(V)\)</span> is open in <spanclass="math inline">\(\mathcal{T}_X\)</span>. In other words, <spanclass="math display">\[ f^{-1}(V) \in \mathcal{T}_X \text{ for all } V\in \mathcal{T}_Y.\]</span></p><p>A function <span class="math inline">\(f: (X, \mathcal{T}_X) \to (Y,\mathcal{T}_Y)\)</span> is a <strong>homeomorphism</strong> if it is acontinuous bijection and its inverse <span class="math inline">\(f^{-1}:(Y, \mathcal{T}_Y) \to (X, \mathcal{T}_X)\)</span> is also continuous.In other words, both <span class="math inline">\(f\)</span> and <spanclass="math inline">\(f^{-1}\)</span> are continuous functions.</p><p>A homeomorphism establishes a topological equivalence between the twospaces, meaning they have the same topological properties.</p><h3 id="connectedness-and-path-connectedness">Connectedness and PathConnectedness</h3><p>A topological space is <strong>connected</strong> if it cannot beexpressed as the union of two disjoint non-empty open sets. In otherwords, there are no two open sets <span class="math inline">\(U\)</span>and <span class="math inline">\(V\)</span> such that <spanclass="math inline">\(X = U \cup V\)</span>, <spanclass="math inline">\(U \cap V = \emptyset\)</span>, and both <spanclass="math inline">\(U\)</span> and <spanclass="math inline">\(V\)</span> are non-empty.</p><p>A topological space is <strong>path connected</strong> if for any twopoints <span class="math inline">\(x, y \in X\)</span>, there exists acontinuous function (path) <span class="math inline">\(f: [0, 1] \toX\)</span> such that <span class="math inline">\(f(0) = x\)</span> and<span class="math inline">\(f(1) = y\)</span>. This means that there isa continuous path connecting any two points in the space.</p><p>A topological space is <strong>locally path connected</strong> ifevery point has a neighborhood base consisting of path connected sets.This means that for each point <span class="math inline">\(x \inX\)</span>, there exists a collection of open sets <spanclass="math inline">\(\{U_n\}_{n=1}^{\infty}\)</span> such that for each<span class="math inline">\(n\)</span>, <spanclass="math inline">\(U_n\)</span> is path connected and contains <spanclass="math inline">\(x\)</span>, and for any open set <spanclass="math inline">\(U\)</span> containing <spanclass="math inline">\(x\)</span>, there exists some <spanclass="math inline">\(n\)</span> such that <spanclass="math inline">\(U_n \subseteq U\)</span>.</p><p>A topological space is <strong>locally connected</strong> if everypoint has a neighborhood base consisting of connected sets. This meansthat for each point <span class="math inline">\(x \in X\)</span>, thereexists a collection of open sets <spanclass="math inline">\(\{U_n\}_{n=1}^{\infty}\)</span> such that for each<span class="math inline">\(n\)</span>, <spanclass="math inline">\(U_n\)</span> is connected and contains <spanclass="math inline">\(x\)</span>, and for any open set <spanclass="math inline">\(U\)</span> containing <spanclass="math inline">\(x\)</span>, there exists some <spanclass="math inline">\(n\)</span> such that <spanclass="math inline">\(U_n \subseteq U\)</span>.</p><h3 id="common-topologies">Common Topologies</h3><p>Fineness and coarseness of topologies are important concepts intopology. A topology <span class="math inline">\(\mathcal{T}_1\)</span>is finer than another topology <spanclass="math inline">\(\mathcal{T}_2\)</span> on the same set <spanclass="math inline">\(X\)</span> if every open set in <spanclass="math inline">\(\mathcal{T}_2\)</span> is also an open set in<span class="math inline">\(\mathcal{T}_1\)</span>. In other words,<span class="math inline">\(\mathcal{T}_1 \supseteq\mathcal{T}_2\)</span>. Conversely, <spanclass="math inline">\(\mathcal{T}_2\)</span> is coarser than <spanclass="math inline">\(\mathcal{T}_1\)</span> if <spanclass="math inline">\(\mathcal{T}_1\)</span> is finer than <spanclass="math inline">\(\mathcal{T}_2\)</span>.</p><p>The <strong>discrete topology</strong> on a set <spanclass="math inline">\(X\)</span> is the finest topology, where everysubset of <span class="math inline">\(X\)</span> is open. Formally,<span class="math inline">\(\mathcal{T}_{\text{discrete}} =\mathcal{P}(X)\)</span>, the power set of <spanclass="math inline">\(X\)</span>.</p><p>The <strong>indiscrete topology</strong> (or trivial topology) on aset <span class="math inline">\(X\)</span> is the coarsest topology,where only the empty set and the entire set <spanclass="math inline">\(X\)</span> are open. Formally, <spanclass="math inline">\(\mathcal{T}_{\text{indiscrete}} = \{\emptyset,X\}\)</span>.</p><p>The <strong>quotient topology</strong> on a set <spanclass="math inline">\(X\)</span> with an equivalence relation <spanclass="math inline">\(\sim\)</span> is defined as follows: 1. Thequotient set is <span class="math inline">\(X / \sim = \{ [x] : x \in X\}\)</span>, where <span class="math inline">\([x]\)</span> is theequivalence class of <span class="math inline">\(x\)</span>. 2. A subset<span class="math inline">\(U \subseteq X / \sim\)</span> is open in thequotient topology if and only if its preimage under the naturalprojection map <span class="math inline">\(\pi: X \to X / \sim\)</span>is open in <span class="math inline">\(X\)</span>. That is, <spanclass="math inline">\(U\)</span> is open if <spanclass="math inline">\(\pi^{-1}(U)\)</span> is open in <spanclass="math inline">\(X\)</span>.</p><p>In other words, the quotient topology is the coarsest topology on<span class="math inline">\(X / \sim\)</span> such that the naturalprojection map <span class="math inline">\(\pi\)</span> iscontinuous.</p><p>The <strong>product topology</strong> on a product of topologicalspaces <span class="math inline">\(\{X_i\}_{i \in I}\)</span> is definedas follows: 1. The product space is <span class="math inline">\(X =\prod_{i \in I} X_i\)</span>. 2. A subset <span class="math inline">\(U\subseteq X\)</span> is open in the product topology if it can beexpressed as a union of sets of the form <spanclass="math inline">\(\prod_{i \in I} U_i\)</span>, where <spanclass="math inline">\(U_i\)</span> is open in <spanclass="math inline">\(X_i\)</span> for each <spanclass="math inline">\(i \in I\)</span>, and <spanclass="math inline">\(U_i = X_i\)</span> for all but finitely many <spanclass="math inline">\(i\)</span>. In other words, the product topologyis generated by the basis consisting of all products of open sets, whereonly finitely many factors are not the entire space.</p><p>The <strong>box topology</strong> on a product of topological spaces<span class="math inline">\(\{X_i\}_{i \in I}\)</span> is definedsimilarly to the product topology, but without the restriction that onlyfinitely many factors can be different from the entire space: 1. The boxspace is <span class="math inline">\(X = \prod_{i \in I} X_i\)</span>.2. A subset <span class="math inline">\(U \subseteq X\)</span> is openin the box topology if it can be expressed as a union of sets of theform <span class="math inline">\(\prod_{i \in I} U_i\)</span>, where<span class="math inline">\(U_i\)</span> is open in <spanclass="math inline">\(X_i\)</span> for each <spanclass="math inline">\(i \in I\)</span>. In other words, the box topologyis generated by the basis consisting of all products of open sets, whereeach factor can be any open set in the corresponding space.</p><p>The <strong>subspace topology</strong> on a subset <spanclass="math inline">\(Y \subseteq X\)</span> of a topological space<span class="math inline">\((X, \mathcal{T})\)</span> is defined asfollows: 1. The subspace is <span class="math inline">\(Y\)</span>. 2. Asubset <span class="math inline">\(U \subseteq Y\)</span> is open in thesubspace topology if it can be expressed as <spanclass="math inline">\(U = Y \cap V\)</span> for some open set <spanclass="math inline">\(V \in X\)</span>. In other words, <spanclass="math inline">\(U\)</span> is open in the subspace topology if itis the intersection of <span class="math inline">\(Y\)</span> with anopen set in <span class="math inline">\(X\)</span>.</p>]]></content>
    
    
    <categories>
      
      <category>Mathematics</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Topology</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Differential Geometry</title>
    <link href="/2025/07/30/Differential-Geometry/"/>
    <url>/2025/07/30/Differential-Geometry/</url>
    
    <content type="html"><![CDATA[<h2 id="references">References</h2><ul><li><p>John Lee, Introduction to Smooth Manifolds</p></li><li><p>Loring Tu, An Introduction to Manifolds</p></li><li><p>Victor Guillemin and Alan Pollack, Differential Topology</p></li></ul><p>For Chinese reader, you can refer to the websitehttp://staff.ustc.edu.cn/~wangzuoq/Courses/23F-Manifolds/#notes forChinese course note in USTC.</p><h2 id="review-on-manifolds">Review on Manifolds</h2><p>Please refer to the “<ahref="https://notdesigned.github.io/2025/08/04/Differential-Manifold/">DifferentialManifold</a>” article.</p><p>For the sake of this articles, one at least need to understand thefollowing concept in that article.</p><ul><li>Topological Manifold</li><li>Smooth Manifold</li><li>Manifold with Boundary</li><li>Partition of Unity</li><li>Submersion, Immersion and Embedding</li><li>etc.</li></ul><h2 id="lie-algebra-and-lie-groups">Lie Algebra and Lie Groups</h2><h3 id="smooth-differential-operators">Smooth DifferentialOperators</h3><p>A <strong>smooth differential operator</strong> of order <spanclass="math inline">\(n\)</span> on <spanclass="math inline">\(M\)</span> is a linear map <spanclass="math inline">\(P: C^{\infty}(M) \to C^{\infty}(M)\)</span> thatcan be expressed locally as a finite sum of the form on each coordinatechart <span class="math inline">\((U,x)\)</span>: <spanclass="math display">\[P=\sum_{|j|\leq n} a_j(p) \left(\frac{\partial}{\partialx^1}\right)^{j_1}\left(\frac{\partial}{\partial x^2}\right)^{j_2} \cdots\left(\frac{\partial}{\partial x^m}\right)^{j_m}\]</span> where <span class="math inline">\(a_j(p)\)</span> are smoothfunctions on <span class="math inline">\(M\)</span>, <spanclass="math inline">\(m=\dim M\)</span>, and <spanclass="math inline">\(j=(j_1,j_2,\ldots,j_m)\)</span> is a multi-indexwith <span class="math inline">\(|j|=j_1+j_2+\ldots+j_m\)</span>.</p><p>And by definition, a smooth vector field <spanclass="math inline">\(X\)</span> on <spanclass="math inline">\(M\)</span> is a smooth differential operator oforder <span class="math inline">\(1\)</span>.</p><p>Using partition of unity, we can prove that any smooth differentialoperator can be expressed as composition of at most <spanclass="math inline">\(n\)</span> smooth vector fields.</p><p>And <span class="math inline">\(\operatorname{supp}(Pf) \subseteq\operatorname{supp}(f)\)</span> for any smooth function <spanclass="math inline">\(f\in C^{\infty}(M)\)</span>.</p><p>Peetre proves that any linear differential operator that satisfiesthe above condition is a smooth differential operator.</p><h3 id="lie-bracket-on-smooth-vector-fields">Lie Bracket on SmoothVector Fields</h3><p>Given two smooth vector fields <spanclass="math inline">\(X,Y\)</span> on a smooth manifold <spanclass="math inline">\(M\)</span>, the <strong>Lie bracket</strong> of<span class="math inline">\(X\)</span> and <spanclass="math inline">\(Y\)</span>, denoted as <spanclass="math inline">\([X,Y]\)</span>, on a local coordinate <spanclass="math inline">\((U,\phi)\)</span> it is defined as: <spanclass="math display">\[[X,Y]= X \circ Y - Y \circ X = \sum_{i=1}^m \left( X^i \frac{\partialY^j}{\partial \phi^i} - Y^i \frac{\partial X^j}{\partial \phi^i} \right)\frac{\partial}{\partial \phi^j}\]</span> note the second order derivative is diminished, it is a thesmooth vector field on <span class="math inline">\(M\)</span>.</p><p>Generally speaking, if <span class="math inline">\(X\)</span> is adifferential operator of order <span class="math inline">\(p\)</span>,<span class="math inline">\(Y\)</span> is of order <spanclass="math inline">\(q\)</span>. <span class="math inline">\(X\circY-Y\circ X\)</span> shall be a differential operator of order <spanclass="math inline">\(p+q-1\)</span>.</p><p>The Lie bracket <span class="math inline">\([\cdot,\cdot]:\mathfrak{X}(M) \times \mathfrak{X}(M) \to \mathfrak{X}(M)\)</span>satisfy the following property for any <spanclass="math inline">\(X,Y,Z\in\mathfrak{X}(M)\)</span></p><ol type="1"><li>Antisymmetry: <span class="math inline">\([X,Y]=-[Y,X]\)</span></li><li>Jacobi Identity: <span class="math inline">\([X,[Y,Z]] + [Y,[Z,X]] +[Z,[X,Y]] = 0\)</span>.</li><li><span class="math inline">\(\mathbb R\)</span>-Linearity: <spanclass="math inline">\([aX+bY,Z]=a[X,Z]+b[Y,Z],\forall a,b\in \mathbbR\)</span></li></ol><p>Note that it is not linear with respect to the second argument.</p><p>The Lie bracket gives a <strong>Lie Algebra</strong> structure on<span class="math inline">\(\mathfrak{X}(M)\)</span> <spanclass="math inline">\((\text{or } \Gamma^{\infty}(TM))\)</span>.</p><h3 id="integral-curves">Integral Curves</h3><p>Let <span class="math inline">\(X\in \mathfrak{X}(M)\)</span>, if asmooth curve <span class="math inline">\(\gamma: I \to M\)</span> issuch that</p><p><span class="math display">\[\dot \gamma(t) = X_{\gamma(t)}\]</span> for all <span class="math inline">\(t \in I\)</span>, then<span class="math inline">\(\gamma\)</span> is called an<strong>integral curve</strong> of the vector field <spanclass="math inline">\(X\)</span>.</p><p>if <span class="math inline">\(0\in I\)</span>, then <spanclass="math inline">\(\gamma(0)\)</span> is called the <strong>initialpoint</strong> of the integral curve.</p><p>Example:</p><hr /><p>For the vector field <span class="math inline">\(\widetilde{X} =\sum_{i=1}^n a^i \frac{\partial}{\partial x^i}\)</span>, the integralcurve is: <span class="math display">\[\widetilde{\gamma}(t)=(c_1 + a^1 t, c_2 + a^2 t, \ldots, c_n + a^n t)\]</span> where <span class="math inline">\(c_i = \gamma(0)^i\)</span>are the initial point of the integral curve.</p><hr /><p>Assume <span class="math inline">\(\gamma\)</span> is in a coordinatechart <span class="math inline">\((U,x)\)</span>, we write <spanclass="math inline">\(\phi(\gamma(t))=(x^1(\gamma(t)),\cdots,x^n(\gamma(t)))\)</span>.</p><p>We denote <span class="math inline">\(\gamma^i:I\to\mathbb{R}=x^i\circ \gamma\)</span> as the <spanclass="math inline">\(i\)</span>-th coordinate of <spanclass="math inline">\(\gamma\)</span> on <spanclass="math inline">\(U\)</span>.</p><p><span class="math display">\[\dot \gamma(t)=(d\gamma)_t(\frac{d}{dt})|_{t}=\sum_{i=1}^n(d\gamma)_t(\frac{d}{dt})|_{t}(x^i)\left.\frac{\partial}{\partialx^i}\right|_{\gamma(t)}=\sum_{i=1}^n\frac{d (x^i\circ\gamma)}{dt}\left.\frac{\partial}{\partialx^i}\right|_{\gamma(t)}=\sum_{i=1}^n \dot{\gamma}^i\left.\frac{\partial}{\partial x^i}\right|_{\gamma(t)}\]</span></p><p>So the equation <span class="math inline">\(\dot \gamma(t) =X_{\gamma(t)}\)</span> can be written as:</p><p><span class="math display">\[\sum_{i=1}^n \dot{\gamma}^i(t) \left.\frac{\partial}{\partialx^i}\right|_{\gamma(t)} = \sum_{i=1}^n X^i(\gamma(t))\left.\frac{\partial}{\partial x^i}\right|_{\gamma(t)}\]</span></p><p>So we have the system of ordinary differential equations: <spanclass="math display">\[\dot{\gamma}^i(t) = X^i\circx^{-1}(\gamma^1(t),\ldots,\gamma^n(t))\]</span> for <spanclass="math inline">\(i=1,2,\ldots,n\)</span>.</p><p>Conversely, every system of ordinary differential equations of theform above defines a unique integral curve <spanclass="math inline">\(\gamma\)</span> on <spanclass="math inline">\(U\)</span> of the vector field <spanclass="math inline">\(X\)</span>.</p><p>By the existence and uniqueness theorem of ordinary differentialequations on <span class="math inline">\(\mathbb R^n\)</span>, we havethe following theorem:</p><p>Assume <span class="math inline">\(X\in \mathfrak{X}(M)\)</span> is asmooth vector field on a smooth manifold <spanclass="math inline">\(M\)</span>, then for every <spanclass="math inline">\(p\in M\)</span>, there exists a neighborhood <spanclass="math inline">\(U\)</span> of <spanclass="math inline">\(p\)</span>, <spanclass="math inline">\(\epsilon&gt;0\)</span> and a smooth mapping <spanclass="math inline">\(\Gamma: (-\epsilon,\epsilon) \times U \toM\)</span> such that: 1. <spanclass="math inline">\(\Gamma(0,p)=p\)</span> for all <spanclass="math inline">\(p\in U\)</span>. 2. For fixed <spanclass="math inline">\(q\in U\)</span>, <spanclass="math inline">\(\gamma_q(t)=\Gamma(t,q)\)</span> is the integralcurve of <span class="math inline">\(X\)</span> with initial point <spanclass="math inline">\(\gamma_q(0)=q\)</span> for all <spanclass="math inline">\(t\in (-\epsilon,\epsilon)\)</span>. 3. Theintegral curve in <span class="math inline">\([2]\)</span> is unique inthe sense that if <span class="math inline">\(\sigma: I \to U\)</span>is another smooth curve with <spanclass="math inline">\(\sigma(0)=q\)</span>, then <spanclass="math inline">\(\gamma_q(t)=\sigma(t)\)</span> for all <spanclass="math inline">\(t\in (-\epsilon,\epsilon)\cap I\)</span>.</p><h3 id="reparametrization">Reparametrization</h3><p>Generally, the reparametrization of an integral curve <spanclass="math inline">\(\gamma\)</span> is not an integral curve of thevector field <span class="math inline">\(X\)</span>. But if thereparametrization is linear, the reparametrization is still an integralcurve of the vector field <span class="math inline">\(X\)</span>.</p><p>If <span class="math inline">\(\gamma:I\to M\)</span> is an integralcurve of <span class="math inline">\(X\)</span>, then: 1. Let <spanclass="math inline">\(I_{a}={t|t+a\in I}\)</span>, <spanclass="math inline">\(\gamma_a:I_a\to M\)</span> be the curve defined by<span class="math inline">\(\gamma_a(t)=\gamma(t+a)\)</span> for all<span class="math inline">\(t\in I_a\)</span>. Then <spanclass="math inline">\(\gamma_a\)</span> is also an integral curve of<span class="math inline">\(X\)</span>. 2. Let <spanclass="math inline">\(I^{a}={t|at\in I} (a\neq 0)\)</span>, then <spanclass="math inline">\(\gamma^a:I^a\to M, \gamma^a(t)=\gamma(at)\)</span>is an integral curve of <span class="math inline">\(X^a=aX\)</span></p><p>For any <span class="math inline">\(p\in M\)</span>, the integralcurve starting at <span class="math inline">\(p\)</span> has a maximalinterval of existence <span class="math inline">\(J_p\)</span>. It iseasy to prove that <span class="math inline">\(J_p\)</span> must beopen.</p><p>Denote the maximal curve as <span class="math display">\[\gamma_p:J_p\to M\]</span></p><p>We have the following property:</p><blockquote><p>For <span class="math inline">\(X\in \mathfrak{X}(M)\)</span>, <spanclass="math inline">\(\gamma_{\gamma_p(s)}(t)=\gamma_p(t+s)\)</span> if<span class="math inline">\(t,s,t+s\in J_p\)</span>.</p></blockquote><p>And <span class="math inline">\(X\in \mathfrak{X}(M)\)</span> iscalled a <strong>complete vector field</strong> if <spanclass="math inline">\(J_p=\mathbb R,\forall p\in M\)</span></p><h3 id="flow">Flow</h3><p>The <strong>flow</strong> is a mapping on set <spanclass="math inline">\(X\)</span> <span class="math display">\[\Phi: X\times \mathbb{R} \to X\]</span> such that <spanclass="math inline">\(\Phi(x,0)=x,\Phi(\Phi(x,s),t) =\Phi(x,s+t)\)</span>.</p><p>In certain situations one might also consider local flows, which aredefined only in some subset <spanclass="math inline">\(\operatorname{dom}\Phi=\mathcal{X}\)</span> is anopen subset of <span class="math inline">\(X\times\mathbbR\)</span>.</p><p>We can see that a vector field <span class="math inline">\(X\)</span>on <span class="math inline">\(M\)</span> induced a flow on <spanclass="math inline">\(M\)</span>.</p><p>We have the following theorem stating the smoothness of such flow:&gt; For any <span class="math inline">\(X\in\Gamma^{\infty}(TM)\)</span>, the flow <spanclass="math inline">\(\Phi:M\times R\to M\)</span> is smooth on itsdomain <span class="math inline">\(\operatorname{dom}\Phi=\{(p,t)|p\inM, t\in J_p\}\)</span>.</p><p><strong>Proof</strong></p><hr /><p>By the property of flow, it suffices to prove that <spanclass="math inline">\(\Phi\)</span> is smooth around any point <spanclass="math inline">\((p,0)\)</span>. And then we can transfer thesmoothness to any point <span class="math inline">\((q,t)\)</span> bythe property of flow.</p><p>By the fundamental theorem above, there exists a neighborhood <spanclass="math inline">\(U\)</span> of <spanclass="math inline">\(p\)</span> and <spanclass="math inline">\(\epsilon&gt;0\)</span> such that <spanclass="math inline">\(\Phi(q,t): U \times (-\epsilon,\epsilon)\)</span>is smooth. So <span class="math inline">\(\Phi\)</span> is smooth around<span class="math inline">\((p,0)\)</span>.</p><hr /><p>If the vector field <span class="math inline">\(X\)</span> iscomplete, then the flow <span class="math inline">\(\Phi\)</span> isdefined on the whole <span class="math inline">\(M\times \mathbbR\)</span>.</p><p>We call the flow <span class="math inline">\(\Phi\)</span> of acomplete vector field <span class="math inline">\(X\)</span> the<strong>global flow</strong> of <span class="math inline">\(X\)</span>.Otherwise, we call it a <strong>local flow</strong>.</p><h3 id="completeness-and-the-induced-diffeomorphism">Completeness andthe induced diffeomorphism</h3><p>We shall naturally ask, when is a vector field complete?</p><p>A sufficient condition is that the vector field has compactsupport.</p><p>We define the support of a vector field <spanclass="math inline">\(X\in \mathfrak{X}(M)\)</span> as: <spanclass="math display">\[\operatorname{supp}(X) = \overline{\{p\in M|X_p\neq 0\}}\]</span></p><p>We state the proof idea here:</p><ul><li>For <span class="math inline">\(X_p = 0\)</span>, the integral curveis constant, so it is defined on <span class="math inline">\(\mathbbR\)</span>.</li><li>For <span class="math inline">\(X_p \neq 0\)</span>, then theintegral curve <span class="math inline">\(\gamma_p(t)\)</span> shall bealways in <span class="math inline">\(C=\operatorname{supp}(X)\)</span>.And there exists a <span class="math inline">\(\epsilon_p\)</span> suchthat <span class="math inline">\(\Gamma(q,t)\)</span> is defined on<span class="math inline">\(U_p\times (-\epsilon_p,\epsilon_p)\)</span>.Now since <span class="math inline">\(C\)</span> is compact, we cancover <span class="math inline">\(C\)</span> with finite many <spanclass="math inline">\(U_{p_i}\)</span>, and let <spanclass="math inline">\(\epsilon = \min \epsilon_{p_i}\)</span>. Then forany <span class="math inline">\(q\in C\)</span>, <spanclass="math inline">\(\gamma_q(t)\)</span> is defined on <spanclass="math inline">\((-\epsilon,\epsilon)\)</span>. And by properreparametrization, we can extend the integral curve to the whole <spanclass="math inline">\(\mathbb R\)</span>.</li></ul><p>As a corollary, a vector field on a compact manifold is alwayscomplete.</p><h4 id="induced-diffeomorphism">Induced Diffeomorphism</h4><p>Let <span class="math inline">\(X\in \mathfrak{X}(M)\)</span> be acomplete vector field on <span class="math inline">\(M\)</span>, thenthe flow <span class="math inline">\(\Phi:M\times \mathbb R\toM\)</span> induces a one-parameter group of diffeomorphisms <spanclass="math inline">\(\{\phi_t:M\to M|t\in \mathbb R\}\)</span> definedas: <span class="math display">\[\phi_t(p) = \Phi(p,t)\]</span> And <span class="math inline">\(\phi_t\)</span> satisfies thefollowing properties: - <span class="math inline">\(\phi_0 =\text{id}_M\)</span> - <span class="math inline">\(\phi_{s+t} = \phi_s\circ \phi_t\)</span> for all <span class="math inline">\(s,t\in \mathbbR\)</span>. - Each <span class="math inline">\(\phi_t\)</span> is adiffeomorphism and <span class="math inline">\(\phi_t^{-1} =\phi_{-t}\)</span>.</p><p>That is to say, a complete vector field on <spanclass="math inline">\(M\)</span> induces a family of diffeomorphismsfrom <span class="math inline">\(M\)</span> to itself.</p><h3 id="dynamical-systems-induced-by-vector-fields">Dynamical SystemsInduced by Vector Fields</h3><p>Mathematically, a <strong>dynamical system</strong> is a triple <spanclass="math inline">\((T,X,\Phi)\)</span>, where <spanclass="math inline">\(X\)</span> is a set called the <strong>statespace</strong>, <span class="math inline">\(T\)</span> is a semi-groupof transformation parameters called the <strong>time set</strong>, and<span class="math inline">\(\Phi\)</span> is a mapping from <spanclass="math inline">\(X \times T\)</span> to <spanclass="math inline">\(X\)</span> that satisfies the properties offlow.</p><p>So a complete vector field <span class="math inline">\(X\)</span> ona smooth manifold <span class="math inline">\(M\)</span> induces adynamical system <span class="math inline">\((\mathbb R, M,\Phi)\)</span>, where <span class="math inline">\(\Phi\)</span> is theflow of <span class="math inline">\(X\)</span>.</p><h4 id="application-in-morse-theory">Application in Morse Theory</h4><p>The following theorem is a fundamental result in Morse theoryindicating the topological structure of the manifold <spanclass="math inline">\(M\)</span> is determined by the properties of thevector field <span class="math inline">\(X\)</span> around the criticalpoints.</p><p>Let <span class="math inline">\(M\)</span> be a smooth manifold, and<span class="math inline">\(f: M \to \mathbb R\)</span> be a smoothfunction on <span class="math inline">\(M\)</span>. For any <spanclass="math inline">\(a\in \mathbb R\)</span>, we define the<strong>sublevel set</strong> <span class="math inline">\(M_a = \{p\in M| f(p) \leq a\}=f^{-1}((-\infty,a))\)</span></p><blockquote><p>For <span class="math inline">\(a&lt;b\)</span>, assuming that <spanclass="math inline">\(f^{-1}([a,b])\)</span> is compact and <spanclass="math inline">\(\forall c\in [a,b]\)</span> is a regular value of<span class="math inline">\(f\)</span>, then there exists adiffeomorphism <span class="math inline">\(\varphi:M\to M\)</span> suchthat <span class="math inline">\(\varphi(M^a)=M^b\)</span> (adeformation retract, more precisely).</p></blockquote><p><strong>Proof</strong></p><p>First we embed <span class="math inline">\(M\)</span> into a Eulideanspace, so a inner product is given in every tangent space <spanclass="math inline">\(T_pM\)</span>.</p><p>We define a <strong>gradient vector field</strong> <spanclass="math inline">\(\nabla f\)</span> as following: <spanclass="math display">\[\langle \nabla f|_p,X_p\rangle = df_p(X_p)= X_p(f), \quad \forall X_p\inT_pM\]</span> We take a compact support bump function <spanclass="math inline">\(h\)</span> such that <spanclass="math inline">\(f^{-1}([a,b])\subseteq\operatorname{supp}(h)\subseteq U\)</span> and <spanclass="math inline">\(U\)</span> is a open set not containing anycritical point.</p><p>And <span class="math display">\[X:=\frac{h}{\langle \nabla f, \nabla f \rangle} \nabla f\]</span> is a compact support vector field on <spanclass="math inline">\(M\)</span>.</p><p>Let <span class="math inline">\(\varphi: M \times \mathbb{R} \toM\)</span> be the global flow generated by <spanclass="math inline">\(X\)</span>, then <span class="math display">\[\frac{d}{dt}\varphi_t^*f(p) = \frac{d}{dt} f(\varphi_t(p)) = \langle\nabla f, X \rangle = \frac{h}{\langle \nabla f, \nabla f \rangle}\langle \nabla f, \nabla f \rangle = h\]</span></p><p>So we have <span class="math inline">\(f(\varphi_t(p)) = t +f(p)\)</span> for any <span class="math inline">\(\varphi_t(p)\inf^{-1}([a,b])\)</span>.</p><p>Thus, the diffeomorphism <spanclass="math inline">\(\varphi_{b-a}\)</span> maps <spanclass="math inline">\(M^a\)</span> to <spanclass="math inline">\(M^b\)</span>.</p><h3 id="lie-derivative">Lie Derivative</h3><p>The <strong>Lie derivative</strong> of a function <spanclass="math inline">\(f\in C^{\infty}(M)\)</span> with respect to avector field <span class="math inline">\(X\in \mathfrak{X}(M)\)</span>is defined as: <span class="math display">\[\mathcal{L}_X f = X(f) = \lim_{t\to 0} \frac{f\circ \varphi_t - f}{t}\]</span> where <span class="math inline">\(\varphi_t\)</span> is theflow of <span class="math inline">\(X\)</span>.</p><p>The <strong>Lie derivative</strong> of a vector field <spanclass="math inline">\(Y\in \mathfrak{X}(M)\)</span> with respect to avector field <span class="math inline">\(X\in \mathfrak{X}(M)\)</span>is defined as: <span class="math display">\[\mathcal{L}_X Y = \frac{d}{dt}\bigg|_{t=0}(d\varphi_{-t})_{\varphi_t(p)}(Y_{\varphi_t(p)}) = \lim_{t\to 0}\frac{(d\varphi_{-t})_{\varphi_t(p)}(Y_{\varphi_t(p)}) - Y_p}{t} = [X,Y]\]</span> where <span class="math inline">\(\varphi_t\)</span> is theflow of <span class="math inline">\(X\)</span>.</p><p>The second equality is because for any <spanclass="math inline">\(f\in C^{\infty}(U)\)</span> defined around <spanclass="math inline">\(p\)</span>: <span class="math display">\[\begin{align*}(d\varphi_{-t})_{\varphi_t(p)}(Y_{\varphi_t(p)})(f) &amp;=Y_{\varphi_t(p)}(f\circ \varphi_{-t}) \\&amp;= Y(f\circ \varphi_{-t})\circ \varphi_t(p) \\&amp;= (\varphi_t^*(Y(f\circ \varphi_{-t})))(p) \\&amp;= (\varphi_t^*Y\varphi_{-t}^*f)(p)\end{align*}\]</span></p><p>And <span class="math display">\[\begin{align*}\frac{d}{dt}\bigg|_{t=0} (d\varphi_{-t})_{\varphi_t(p)}(Y_{\varphi_t(p)}) f &amp;= \frac{d}{dt}\bigg|_{t=0}(\varphi_t^*Y\varphi_{-t}^*f) \\&amp;= \frac{d}{dt}\bigg|_{t=0} \varphi_t^*Yf +Y\frac{d}{dt}\bigg|_{t=0} \varphi_{-t}^*f\\&amp;= XYf-YXf = [X,Y]f\end{align*}\]</span></p><p>The Lie derivative is a derivation on the Lie algebra <spanclass="math inline">\(\mathfrak{X}(M)\to \mathfrak{X}(M)\)</span>. Thatis to say, for any <span class="math inline">\(X,Y,Z\in\mathfrak{X}(M)\)</span> and <span class="math inline">\(a,b\in \mathbbR\)</span>, we have the Leibniz rule: <span class="math display">\[L_X([Y,Z]) = [L_X Y, Z] + [Y, L_X Z]\]</span></p><p><strong>Proof</strong></p><p>By the Jacobi identity, we have: <span class="math display">\[\begin{align*}L_X([Y,Z]) &amp;= [X,[Y,Z]] \\&amp;= - [Y,[Z,X]] - [Z,[X,Y]] \\&amp;= [ [X,Y], Z] + [Y, [X,Z]] \\&amp;= [L_X Y, Z] + [Y, L_X Z]\end{align*}\]</span></p><p>So we can see the Jacobi identity as the Leibniz rule of the Liederivative.</p><h3 id="lie-group">Lie Group</h3><p>Let <span class="math inline">\(G\)</span> be a group, we say <spanclass="math inline">\(G\)</span> is a <strong>Lie group</strong> if itis a smooth manifold and the group operation <spanclass="math inline">\(\mu:G\times G\to G, (g_1,g_2)\mapsto g_1\cdotg_2\)</span> is smooth mappings.</p><p>Examples of Lie groups include: - <spanclass="math inline">\(\mathbb{R}^n\)</span> with addition as the groupoperation. - <spanclass="math inline">\(\mathbb{R}^n\setminus\{0\}\)</span> withmultiplication as the group operation. - The general linear group <spanclass="math inline">\(\operatorname{GL}(n,\mathbb{R})\)</span>, whichconsists of all invertible <span class="math inline">\(n \timesn\)</span> matrices with real entries, with matrix multiplication as thegroup operation. - The special orthogonal group <spanclass="math inline">\(\operatorname{SO}(n)\)</span>, which consists ofall <span class="math inline">\(n \times n\)</span> orthogonal matriceswith determinant 1, with matrix multiplication as the group operation. -<span class="math inline">\(\mathbb S^1\)</span>, the circle group,which can be identified with the unit complex numbers undermultiplication. - <span class="math inline">\(\mathbb S^3\)</span>, the3-sphere, which can be identified with unit quaternions undermultiplication.</p><p>A Lie group must satisfy the following properties: - The base space<span class="math inline">\(G\)</span> (i.e. the underlying topologicalspace) is orientable. - The fundamental group <spanclass="math inline">\(\pi_1(G)\)</span> is a abelian group for aconnected Lie group <span class="math inline">\(G\)</span>. - Every Liegroup is parallelizable, i.e., its tangent bundle is trivial <spanclass="math inline">\(TM \cong G \times \mathbb{R}^n\)</span>, where<span class="math inline">\(n = \dim G\)</span>.</p><h4 id="left-and-right-multiplication">Left and RightMultiplication</h4><p>Assume <span class="math inline">\(G\)</span> is a Lie group, for any<span class="math inline">\(g\in G\)</span>, the left multiplication andright multiplication induce two mappings on <spanclass="math inline">\(G\)</span>: <span class="math display">\[L_g: G \to G, \quad L_g(h) = g\cdot h\]</span> <span class="math display">\[R_g : G \to G, \quad R_g(h) = h\cdot g\]</span></p><p>Note we can write them as the composition of the group operation andthe inclusion map: <span class="math display">\[L_g = \mu \circ l_g=(i_g, \text{id}_G), \quad R_g = \mu \circr_g=(\text{id}_G, i_g)\]</span> where <span class="math inline">\(i_g: \{e\} \to G\)</span> isthe inclusion map from the identity element <spanclass="math inline">\(e\)</span> to <spanclass="math inline">\(g\)</span>.</p><p>And <span class="math inline">\(L_g^{-1} = L_{g^{-1}}, R_g^{-1} =R_{g^{-1}}\)</span>.</p><p>Thus, <span class="math inline">\(L_g\)</span> and <spanclass="math inline">\(R_g\)</span> are diffeomorphisms on <spanclass="math inline">\(G\)</span>.</p><p>To show the usefulness of left and right multiplication, we leveragedthem to prove that every Lie group is parallelizable.</p><p><strong>Theorem</strong> Every Lie group <spanclass="math inline">\(G\)</span> is parallelizable, i.e., its tangentbundle is trivial <span class="math inline">\(TG \cong G \times\mathbb{R}^n\)</span>, where <span class="math inline">\(n = \dimG\)</span>.</p><p><strong>Proof</strong> Consider <span class="math display">\[\begin{align*}\Phi: G \times T_eG &amp;\to TG \\(g,v) &amp;\mapsto (g, (dL_g)_e(v))\end{align*}\]</span> where <span class="math inline">\(e\)</span> is the identityelement of <span class="math inline">\(G\)</span>. It is easy to seethat <span class="math inline">\(\Phi\)</span> is a bijection. And since<span class="math inline">\(L_g\)</span> is a diffeomorphism, <spanclass="math inline">\((dL_g)_e\)</span> is a linear isomorphism from<span class="math inline">\(T_eG\)</span> to <spanclass="math inline">\(T_gG\)</span>. So <spanclass="math inline">\(\Phi\)</span> is a diffeomorphism.</p><p>Use the same technique, we can also show that the inverse mapping<span class="math inline">\(g\mapsto g^{-1}\)</span> is a smoothdiffeomorphism on <span class="math inline">\(G\)</span>.</p><p>And the differential of the inverse mapping is given by: <spanclass="math display">\[(d\iota)_a X_a = -(dL_{a^{-1}})_e (dR_{a^{-1}})_a X_a\]</span></p><p>And <span class="math inline">\((d\mu)_{e,e}(X_e,Y_e) = X_e+Y_e,(d\iota)_e(X_e) = -X_e\)</span>.</p><h3 id="lie-algebra">Lie Algebra</h3><p>From the proof above, we see that the tangent space <spanclass="math inline">\(T_eG\)</span> determines the structure around anypoint <span class="math inline">\(g\in G\)</span>.</p><p>So we focus on the structure around the identity element <spanclass="math inline">\(e\)</span> of the Lie group <spanclass="math inline">\(G\)</span>.</p><p><strong>Definition</strong></p><p>Let <span class="math inline">\(V\)</span> be a vector space over<span class="math inline">\(\mathbb R\)</span> (or any field <spanclass="math inline">\(\mathbb K\)</span>) with a map <spanclass="math inline">\([\cdot,\cdot]: V \times V \to V\)</span>satisfying the following properties: - Antisymmetry: <spanclass="math inline">\([X,Y] = -[Y,X]\)</span> for all <spanclass="math inline">\(X,Y\in V\)</span>. - Linearity: <spanclass="math inline">\([aX+bY,Z]=a[X,Z]+b[Y,Z],\forall a,b\in \mathbbR\)</span>. - Jacobi Identity: <span class="math inline">\([X,[Y,Z]] +[Y,[Z,X]] + [Z,[X,Y]] = 0\)</span> for all <spanclass="math inline">\(X,Y,Z\in V\)</span>.</p><p>Then <span class="math inline">\((V,[\cdot,\cdot])\)</span> is calleda <strong>Lie algebra</strong> and the map <spanclass="math inline">\([\cdot,\cdot]\)</span> is called the <strong>Liebracket</strong>.</p><h4 id="left-invariant-vector-fields">Left Invariant Vector Fields</h4><p>Assume <span class="math inline">\(G\)</span> is a Lie group, <spanclass="math inline">\(X_e\in T_eG\)</span> is a vector in the tangentspace at the identity element <span class="math inline">\(e\)</span>. Wecan define a vector field <span class="math inline">\(X\in\mathfrak{X}(G)\)</span> as follows: <span class="math display">\[X(g) = (dL_g)_e(X_e)\]</span> This vector field is called a <strong>left invariant vectorfield</strong> because it is invariant under left multiplication, i.e.,<span class="math inline">\(dL_g X(h) = X(gh)\)</span> for all <spanclass="math inline">\(g,h\in G\)</span>.</p><p>It is obvious that any left invariant vector field is uniquelydetermined by its value at the identity element <spanclass="math inline">\(e\)</span>.</p><p>Denote the set of all left invariant vector fields on <spanclass="math inline">\(G\)</span> as <spanclass="math inline">\(\mathfrak{g} = \{X\in \mathfrak{X}(G)| X(g) =(dL_g)_e(X_e), \forall g\in G\}\)</span>.</p><p>The set <span class="math inline">\(\mathfrak{g}\)</span> is a vectorspace over <span class="math inline">\(\mathbb R\)</span> and <spanclass="math inline">\(\mathfrak{g} \simeq T_eG\)</span>, <spanclass="math inline">\(\dim \mathfrak{g} = \dim G\)</span>.</p><p>For any <span class="math inline">\(X,Y\in \mathfrak{g}, g\inG\)</span>, we can verify that: <span class="math display">\[\begin{align*}( (dL_g)_e([X,Y]_e) )f &amp;= [X,Y]_e (f\circ L_g) \\&amp;= (X(Y(f\circ L_g)) - Y(X(f\circ L_g)))(e) \\&amp;= (XYf-YXf)(g)\\&amp;= [X,Y]_g f\end{align*}\]</span> Thus <span class="math inline">\([X,Y]\)</span> is also a leftinvariant vector field.</p><p>So <span class="math inline">\(\mathfrak{g}\)</span> is a Lie algebrawith the Lie bracket induced by the Lie bracket of <spanclass="math inline">\(\mathfrak{X}(G)\)</span>, which is called<strong>the Lie Algebra on Lie group <spanclass="math inline">\(G\)</span></strong>.</p><p><strong>Example</strong></p><p><span class="math inline">\(\text{GL}(n,\mathbb{R})\)</span> is aopen subset of <span class="math inline">\(M(n,\mathbb{R})\cong \mathbbR^{n^2}\)</span></p><p>So <span class="math inline">\(\mathfrak{gl}(n,\mathbb{R})\)</span>as the tangent space of <spanclass="math inline">\(\text{GL}(n,\mathbb{R})\)</span> on <spanclass="math inline">\(e=I_n\)</span> is isomorphic to <spanclass="math inline">\(M(n,\mathbb R)\)</span>.</p><p>And <span class="math inline">\([X_A,X_B]= X_{[A,B]}\)</span> for all<span class="math inline">\(A,B\in T_{I_n} \text{GL}(n,\mathbbR)\)</span>.</p><h3 id="homomorphism-of-lie-groups-and-lie-algebras">Homomorphism of LieGroups and Lie Algebras</h3><p>There are two types of homomorphism, one is the <strong>Lie grouphomomorphism</strong> and the other is the <strong>Lie algebrahomomorphism</strong>, which are morphism between Lie group categoriesand Lie algebra categories respectively.</p><p>Let <span class="math inline">\(G,H\)</span> be Lie groups, if smoothmapping <span class="math inline">\(\phi: G\to H\)</span> is a grouphomomorphism, we call <span class="math inline">\(\phi\)</span> a Liegroup homomorphism. If <span class="math inline">\(\phi\)</span> is adiffeomorphism, we call <span class="math inline">\(\phi\)</span> a Liegroup isomorphism.</p><p>Let <span class="math inline">\(\mathfrak{g,h}\)</span> be Liealgebras, if a linear mapping <spanclass="math inline">\(L:\mathfrak{g}\to \mathfrak{h}\)</span> satisfies<span class="math inline">\(L([X,Y]) = [L(X),L(Y)]\)</span> for all<span class="math inline">\(X,Y\in \mathfrak{g}\)</span>, we call <spanclass="math inline">\(L\)</span> a Lie algebra homomorphism. If <spanclass="math inline">\(L\)</span> is invertible, we call <spanclass="math inline">\(L\)</span> a Lie algebra isomorphism. Note that if<span class="math inline">\(L\)</span> is invertible, then <spanclass="math inline">\(L^{-1}\)</span> is also a Lie algebrahomomorphism.</p><p><strong>Examples</strong></p><p>For any Lie group <span class="math inline">\(G\)</span>, theconjugation mapping <span class="math inline">\(C_g: G\to G\)</span>defined as: <span class="math display">\[C_g(h) = g h g^{-1} = L_g \circ R_{g^{-1}}(h)\]</span> is a Lie group isomorphism for any <spanclass="math inline">\(g\in G\)</span>. The inverse mapping is given by<span class="math inline">\(C_{g^{-1}}\)</span>.</p><p>For any <span class="math inline">\(X\in GL(n,\mathbb R)\)</span>,the adjoint mapping <span class="math inline">\(\operatorname{Ad}_X:\mathfrak{gl}(n,\mathbb R) \to \mathfrak{gl}(n,\mathbb R)\)</span>defined as: <span class="math display">\[\operatorname{Ad}_X(Y) = XYX^{-1}\]</span> is a Lie group isomorphism. The inverse mapping is given by<span class="math inline">\(\operatorname{Ad}_{X^{-1}}\)</span>.</p><h4 id="induced-lie-algebra-homomorphism">Induced Lie AlgebraHomomorphism</h4><p>Let <span class="math inline">\(\phi: G\to H\)</span> be a Lie grouphomomorphism, then the differential of <spanclass="math inline">\(\phi\)</span> at the identity element <spanclass="math inline">\(e\in G\)</span>, induced a mapping <spanclass="math inline">\(d\phi: \mathfrak{g} \simeq T_eG \toT_{\phi(e)}H\simeq \mathfrak{h}\)</span>, is a Lie algebra homomorphismfrom the Lie algebra <span class="math inline">\(\mathfrak{g}\)</span>of <span class="math inline">\(G\)</span> to the Lie algebra <spanclass="math inline">\(\mathfrak{h}\)</span> of <spanclass="math inline">\(H\)</span>.</p><p>To write it more explicitly, for any <span class="math inline">\(X\in\mathfrak{g}\)</span>, <span class="math inline">\(X_e\in T_eG\)</span>is the value of <span class="math inline">\(X\)</span> at <spanclass="math inline">\(e\)</span>. <span class="math display">\[\begin{align*}d\phi_e: T_eG &amp;\to T_{\phi(e)}H \\X_e &amp;\mapsto (d\phi)_e(X_e)\end{align*}\]</span> <span class="math display">\[\begin{align*}d\phi: \mathfrak{g} &amp;\to \mathfrak{h} \\X &amp;\mapsto [h \mapsto dL_h(d\phi_e(X_e))]\end{align*}\]</span></p><p>To see that <span class="math inline">\(d\phi\)</span> is a Liealgebra homomorphism, we need to show that <spanclass="math inline">\(d\phi([X,Y]) = [d\phi(X), d\phi(Y)]\)</span> forany <span class="math inline">\(X,Y\in \mathfrak{g}\)</span>.</p><p>For any <span class="math inline">\(f\in C^{\infty}(H)\)</span>, wehave: <span class="math display">\[\begin{align*}(d\phi([X,Y]))_{e_{h}} f &amp;= dL_{e_{h}}(d\phi_e([X,Y]_e)) f \\&amp;= (d\phi_e([X,Y]_e))(f) \\&amp;= [X,Y]_e (f\circ \phi) \\\end{align*}\]</span></p><p><span class="math display">\[\begin{align*}[d\phi(X), d\phi(Y)]_{e_{h}} f &amp;= (d\phi(X) d\phi(Y) f - d\phi(Y)d\phi(X) f)(e_{h}) \\&amp;= (d\phi(X)(d\phi(Y) f))(e_{h}) - (d\phi(Y)(d\phi(X) f))(e_{h}) \\&amp;= (d\phi_e(X_e))(d\phi(Y) f) - (d\phi_e(Y_e))(d\phi(X) f) \\&amp;= X_e (d\phi(Y) f \circ \phi) - Y_e (d\phi(X) f \circ \phi) \\\end{align*}\]</span></p><p>To move on, we need:</p><p><strong>Lemma</strong>:</p><p><span class="math display">\[(d\phi(X)f) \phi = X(f\circ \phi)\]</span></p><p><strong>Proof</strong>: For any <span class="math inline">\(g\inG\)</span>, we have: <span class="math display">\[\begin{align*}(d\phi(X)f) \phi(g) &amp;= d\phi(X)|_{\phi(g)} f \\&amp;= (dL_{\phi(g)})_{e_H}(d\phi_e(X_e)) f \\&amp;= d\phi_g (dL_g)_e (X_e) f \\&amp;= d\phi_g X_g f \\&amp;= X_g (f \circ \phi) \\&amp;= X(f \circ \phi) (g)\end{align*}\]</span></p><blockquote><p>For the third step, <span class="math inline">\(\phi\)</span> is agroup homomorphism, we know that <span class="math inline">\(\phi(g h) =\phi(g)\phi(h)\)</span>, so <span class="math inline">\(L_{\phi(g)}\circ \phi = \phi \circ L_g\)</span>, differentiating both sides at<span class="math inline">\(e\)</span> gives us <spanclass="math inline">\(d(L_{\phi(g)})_{e_H} \circ d\phi_e = d\phi_g \circ(dL_g)_e\)</span>.</p></blockquote><p>That is to say, the definition of the push forward <spanclass="math inline">\(d\phi(X)\)</span> by the differential of thediffeomorphism <span class="math inline">\(\phi\)</span> is equivalentto the corresponding vector field induced by the left action <spanclass="math inline">\(dL_{\phi(\cdot)}\)</span> on <spanclass="math inline">\(d\phi_e(X_e)\)</span>.</p><p>So we have: <span class="math display">\[\begin{align*}[d\phi(X), d\phi(Y)]_{e_{h}} f &amp;= X_e (d\phi(Y) f \circ \phi) - Y_e(d\phi(X) f \circ \phi) \\&amp;= X_e (Y (f\circ \phi)) - Y_e (X (f\circ \phi)) \\&amp;= [X,Y]_e (f\circ \phi) \\&amp;= (d\phi([X,Y]))_{e_{h}} f\end{align*}\]</span></p><p>The above proof can be summerized as the following:</p><p>Step 1: <span class="math inline">\(d\phi(X)\)</span> is <spanclass="math inline">\(\phi\)</span>-related to <spanclass="math inline">\(X\)</span>.</p><p>For <span class="math inline">\(X \in \mathfrak{g}\)</span>, <spanclass="math inline">\(d\phi(X) \in \mathfrak{h}\)</span> isleft-invariant, and for <span class="math inline">\(f \inC^\infty(H)\)</span>, since <span class="math inline">\(\phi(g h) =\phi(g) \phi(h)\)</span>, we have <spanclass="math inline">\(L_{\phi(g)} \circ \phi = \phi \circL_g\)</span>.</p><p>Differentiating at <span class="math inline">\(e\)</span>: <spanclass="math inline">\(d(L_{\phi(g)}){e_H} \circ d\phi_e = d\phi_g \circ(dL_g)e.\)</span> Thus: <span class="math inline">\(d\phi(X)|_{\phi(g)}f = (dL{\phi(g)})_{e_H} (d\phi_e(X_e)) f = d\phi_g ((dL_g)_e X_e) f =X_g (f \circ \phi).\)</span></p><p>So, <span class="math inline">\(d\phi(X) f \circ \phi = X (f \circ\phi)\)</span>, meaning <span class="math inline">\(X\)</span> is <spanclass="math inline">\(\phi\)</span>-related to <spanclass="math inline">\(d\phi(X)\)</span>.</p><p>Step 2: Lie Bracket Preservation</p><p>If <span class="math inline">\(X, Y \in \mathfrak{g}\)</span> are<span class="math inline">\(\phi\)</span>-related to <spanclass="math inline">\(d\phi(X), d\phi(Y) \in \mathfrak{h}\)</span>, then<span class="math inline">\([X, Y]\)</span> is <spanclass="math inline">\(\phi\)</span>-related to <spanclass="math inline">\([d\phi(X), d\phi(Y)]\)</span>.</p><p>At <span class="math inline">\(g = e\)</span>: <spanclass="math inline">\([d\phi(X), d\phi(Y)]_{e_H} = d\phi_e ([X,Y]_e).\)</span> Since both are left-invariant, <spanclass="math inline">\([d\phi(X), d\phi(Y)] = d\phi([X, Y])\)</span>.Linearity of <span class="math inline">\(d\phi\)</span> follows from<span class="math inline">\(d\phi_e\)</span>.</p><p>Thus, <span class="math inline">\(d\phi\)</span> is a Lie algebrahomomorphism.</p><h4 id="adjoint-map">Adjoint Map</h4><p>Since every element <span class="math inline">\(g\in G\)</span> givesa isomorphism on Lie group: <span class="math display">\[\begin{align*}C_g: G&amp;\to G \\x&amp;\mapsto gxg^{-1}\end{align*}\]</span> Such map induces a isomorphism on Lie algebra: <spanclass="math display">\[\text{Ad}_g = d C_g\big|_e: \mathfrak{g}\to\mathfrak{g}\]</span></p><p>So this yields a new mapping <span class="math display">\[\begin{align*}\text{Ad}: G&amp;\to \text{GL}(\mathfrak g)\\g&amp;\mapsto \text{Ad}_g\end{align*}\]</span> as a group homomorphism between <spanclass="math inline">\(G\)</span> and <spanclass="math inline">\(\text{GL}(\mathfrak g)\)</span>, which is calledthe <strong>adjoint representation</strong> of <spanclass="math inline">\(G\)</span>.</p><p>Again, take the induced mapping of <spanclass="math inline">\(\text{Ad}\)</span>, we get <spanclass="math display">\[\begin{align*}\text{ad}: \mathfrak{g} &amp;\to \text{End}(\mathfrak{g}) \\X &amp;\mapsto d \text{Ad} \big|_e (X)\end{align*}\]</span></p><p>We shall see that <span class="math inline">\(\text{ad}(X)(Y) =[X,Y]\)</span>.</p><h3 id="exponential-map">Exponential Map</h3><p>Recall that Lie algebra can be viewed as the tangent space of <spanclass="math inline">\(T_eG\)</span> of some Lie group <spanclass="math inline">\(G\)</span>. For now, we put whether such <spanclass="math inline">\(G\)</span> can always be found aside. And atangent vector <span class="math inline">\(X\in T_eG\)</span> induced aleft-invariant vector field <span class="math inline">\(X\)</span> bythe left multiplication <span class="math inline">\(L_g\)</span> on<span class="math inline">\(G\)</span>. Such vector fields could inducea flow <span class="math inline">\(\Phi^X\)</span> on <spanclass="math inline">\(G\)</span>, i.e. we can build a connection betweenthe Lie algebra and the induced flow on the group.</p><p>We shall ask the following question:</p><ul><li>Is <span class="math inline">\(\Phi^X\)</span> complete?</li><li>How does <span class="math inline">\(\Phi^X\)</span> changeregarding different choices of <spanclass="math inline">\(X\)</span>?</li></ul><p>The answer to the first question is yes.</p><p>Although <span class="math inline">\(G\)</span> is not necessarilycompact, but if <span class="math inline">\(U\in e\)</span> we locallyhave <span class="math inline">\(\Gamma: (-\epsilon, \epsilon) \times U\to G\)</span> as the fundamental theorem states, then we can apply<span class="math inline">\(L_g\)</span> to transform the local maps toanywhere <span class="math inline">\(g\)</span>, thus the integral curvecan be extended to the whole manifold.</p><p>Note <span class="math display">\[\phi^X_t(g) = g \phi_t^X(e)\]</span> where <span class="math inline">\(\phi^X_t\)</span> are thediffeomorphism induced by the flow <spanclass="math inline">\(\Phi^X\)</span> at time <spanclass="math inline">\(t\)</span>.</p><p><strong>Exercise</strong>: Prove it.</p><blockquote><p>Hint: First assume <span class="math inline">\(g\in U\)</span>, andshow that any <span class="math inline">\(g&#39;\in G\)</span> can bewritten as finite product of <span class="math inline">\(g_i\inU\)</span></p></blockquote><p>So <span class="math display">\[\phi_t^X = R_{\phi_{t}^X(e)}\]</span> And <span class="math display">\[g\phi_{s+t}^X(e) = \phi_{t}^X(\phi_{s}^X(g)) =\phi_{t}^X(g\phi_{s}^X(e)) = g\phi_{s}^X(e)\phi_{t}^X(e)\]</span> for all <span class="math inline">\(g\in G\)</span></p><p>Thus <span class="math display">\[\phi_{s+t}^X(e) = \phi_{s}^X(e)\phi_{t}^X(e)\]</span></p><p>So the mapping <span class="math display">\[\begin{align*}\rho^X: \mathbb{R} &amp;\to G \\t &amp;\mapsto \phi^X_t(e)\end{align*}\]</span> is a smooth group homomorphism.</p><p><strong>Exercise</strong>: &gt; Prove it is smooth.</p><p>So <span class="math inline">\(\rho^X\)</span> is called the<strong>one-parameter subgroup</strong> of <spanclass="math inline">\(G\)</span>.</p><p>And every <span class="math inline">\(X\in\mathfrak{g}\)</span> givesrise to a one-parameter subgroup <spanclass="math inline">\(\rho^X\)</span> of <spanclass="math inline">\(G\)</span>.</p><p>Conversely, let <span class="math inline">\(\rho\)</span> be aone-parameter subgroup of <span class="math inline">\(G\)</span>. Then<span class="math display">\[X_g := \frac{d}{dt}\bigg|_{t=0} R_{\rho(t)}g\]</span> is a left-invariant vector field on <spanclass="math inline">\(G\)</span>.</p><p><strong>Proof</strong>: <span class="math display">\[\begin{align*}dL_h(X_g) &amp;= dL_h (\frac{d}{dt}\bigg|_{t=0} R_{\rho(t)}g)\\&amp;= \frac{d}{dt}\bigg|_{t=0}(L_hR_{\rho(t)}g) \\&amp;= \frac{d}{dt}\bigg|_{t=0}(R_{\rho(t)}hg) \\&amp;= X_{hg}\end{align*}\]</span></p><p>So <span class="math inline">\(\rho^X\)</span> has a bijection with<span class="math inline">\(X\in \mathfrak{g}\)</span>, which is thethird interpretation of the Lie algebra—the infinitesimal generator ofall the one-parameter subgroup of <spanclass="math inline">\(G\)</span>.</p><p><strong>Exercise</strong> &gt; Now, prove <spanclass="math inline">\(\text{ad}(X)(Y)=[X,Y]\)</span>.</p><p>To study the second problem, we define the exponential map <spanclass="math display">\[\begin{align*}\exp: \mathfrak{g} &amp;\to G \\X &amp;\mapsto \phi^X_1(e)\end{align*}\]</span> Note <span class="math inline">\(\exp(tX) = \phi_{1}^{tX}(e) =\phi_{t}^{X}(e)\)</span>.</p><p>The exponential map has the following properties: 1.<strong>Differentiable</strong>: The exponential map is a smooth(infinitely differentiable) map.</p><ol start="2" type="1"><li><p><strong>Local Diffeomorphism</strong>: The exponential map is alocal diffeomorphism around the zero vector in the Lie algebra <spanclass="math inline">\(\mathfrak{g}\)</span>. This means that for smallenough <span class="math inline">\(X \in \mathfrak{g}\)</span>, the map<span class="math inline">\(\exp\)</span> is invertible in aneighborhood of <span class="math inline">\(0 \in\mathfrak{g}\)</span>.</p></li><li><p><span class="math inline">\(\exp(tX)\cdot \exp(sX) =\exp((t+s)X)\)</span></p></li></ol><p><strong>Examples</strong></p><p>(1). <span class="math inline">\(G=\mathbb R^*\)</span>, <spanclass="math inline">\(\mathfrak{g}=T_1 G= \{c\cdot\frac{d}{dx}\big|_{1}\}\cong \mathbb R\)</span>.</p><p>To calculate <spanclass="math inline">\(\exp(X)=\phi_{1}^X(1)\)</span>, write <spanclass="math inline">\(X= x\frac{d}{dx} \in \mathfrak{g}\)</span>.</p><p>And <span class="math display">\[\dot \gamma(t) = X_{\gamma(t)}\]</span> Note <span class="math inline">\(X_{\gamma(t)} = \gamma(t) x\frac{d}{d x^0}\)</span> <span class="math display">\[\frac{d}{dt}\gamma(t) = \gamma(t) \cdot x\]</span> yields <span class="math inline">\(\gamma(t) =\exp(tx)\)</span>.</p><p>(2). For <span class="math inline">\(G=(S^1,\cdot)\)</span>, <spanclass="math display">\[\begin{align*}\exp: i\mathbb R=T_e S^1&amp;\to S^1 \\\exp(ix) &amp;\mapsto e^{ix}\end{align*}\]</span> (3). For <span class="math inline">\(G=\text{GL}(n,\mathbbR)\)</span> <span class="math display">\[\begin{align*}\exp: \mathfrak{gl}(n,\mathbb R)&amp;\to \text{GL}(n,\mathbb R) \\A &amp;\mapsto e^{A}=\sum_{i=0}^{\infty} \frac{1}{i!}A^i\end{align*}\]</span> where <span class="math inline">\(A^0 = I\)</span>.</p><p><span class="math inline">\(\exp:\mathfrak{g} \to G\)</span> is asmooth mapping, and its differential at <spanclass="math inline">\(e\)</span> is the identity map <spanclass="math display">\[(d \exp)_0 = Id_{\mathfrak g}: T_0\mathfrak{g}\simeq \mathfrak{g} \to\mathfrak{g} \simeq T_e G\]</span></p><p><strong>Proof</strong> Consider a vector field <spanclass="math inline">\(\tilde X\)</span> manifold <spanclass="math inline">\(G\times \mathfrak{g}\)</span>, <spanclass="math inline">\(\tilde X_{(g,X)}=(X_g,0)\)</span></p><p>The integral curve just be <span class="math inline">\(\gamma(t) = (g\cdot \exp(tX), X)\)</span></p><p>The flow is smooth, thus <span class="math display">\[\begin{align*}\mathfrak{g}&amp;\to \mathbb{R}\times G\times \mathfrak g &amp;&amp;\toG\times \mathfrak g &amp;&amp;\to G\\X &amp;\mapsto (1,e,X) &amp;&amp;\mapsto (\exp(X),X) &amp;&amp;\mapsto\exp(X)\end{align*}\]</span> is smooth.</p><p>The identity property is given by the following: <spanclass="math display">\[d(exp)_0 X = \frac{d}{dt}\bigg|_{t=0} \exp(tX) =\frac{d}{dt}\bigg|_{t=0} \phi_t^X(e) = X_e\]</span></p><p>Particularly, <span class="math inline">\(\exp\)</span> is adiffeomorphism around <span class="math inline">\(0\)</span>. That is tosay, locally around <span class="math inline">\(e\)</span>, the Liegroup looks like the corresponding Lie algebra.</p><h3 id="naturality">Naturality</h3><p>Given any Lie group homomorphism <span class="math inline">\(\varphi:G\to H\)</span>, the diagram</p><p><span class="math display">\[\begin{array}{ccc}\mathfrak{g} &amp; \xrightarrow{\quad\quad d\varphi\quad\quad} &amp;\mathfrak{h} \\[2em]\Big\downarrow{\exp_G} &amp; &amp; \Big\downarrow{\exp_H} \\[2em]G &amp; \xrightarrow{\quad\quad\varphi\quad\quad} &amp; H\end{array}\]</span></p><p>commute.</p><p><strong>Exercise</strong> Proof it &gt; Hint: For any <spanclass="math inline">\(X\in \mathfrak g\)</span>, <spanclass="math inline">\(\varphi \circ \exp_{G}(tX)\)</span> represents aone-parameter groups in <span class="math inline">\(H\)</span>, what isthe infinitesimal generator?</p><p>Particularly, for mapping <span class="math inline">\(C_g:G\toG\)</span> and <span class="math inline">\(\text{Ad}: G\to\text{GL}(\mathfrak g)\)</span>, we have: 1. <spanclass="math inline">\(g(\exp(tX))g^{-1} = \exp(t\text{Ad}_g(X))\)</span>2. <span class="math inline">\(\text{Ad}(\exp(tX)) =\exp(t\text{ad}(X))\)</span></p><h3 id="baker-campbell-hausdorff-formula">Baker-Campbell-HausdorffFormula</h3><p>TODO.</p><h2 id="exterior-algebra-differential-forms">Exterior Algebra,Differential Forms</h2><p><strong>Note</strong>: we discuss multilinear functions over <spanclass="math inline">\(\mathbb{R}\)</span>-vector spaces, but the theorycan be generalized to <spanclass="math inline">\(\mathbb{C}\)</span>-vector spaces or other vectorspaces over a field.</p><h3 id="dual-space">Dual Space</h3><p>Let <span class="math inline">\(V,W\)</span> be a <spanclass="math inline">\(\mathbb{R}\)</span>-vector space, we denote all ofthe mapping from <span class="math inline">\(V\)</span> to <spanclass="math inline">\(W\)</span> as <spanclass="math inline">\(\text{Hom}(V,W)\)</span>.</p><p>The <strong>dual space</strong> of <spanclass="math inline">\(V\)</span>, denoted as <spanclass="math inline">\(V^{\vee}\)</span> or <spanclass="math inline">\(V^*\)</span>, is the vector space of all linearmaps from <span class="math inline">\(V\)</span> to <spanclass="math inline">\(\mathbb{R}\)</span>, i.e., <spanclass="math inline">\(V^* = \text{Hom}(V, \mathbb{R})\)</span>.</p><p>The elements of the dual space <spanclass="math inline">\(V^*\)</span> are called <strong>covectors</strong>or <strong>dual vectors</strong>.</p><p>The basis of the dual space <span class="math inline">\(V^*\)</span>is defined as the set of linear functionals that map each basis vectorof <span class="math inline">\(V\)</span> to <spanclass="math inline">\(\mathbb{R}\)</span>, while mapping all other basisvectors to zero.</p><p>To be specific, let <span class="math inline">\(V\)</span> be <spanclass="math inline">\(n\)</span>-dimensional vector space, if <spanclass="math inline">\(\{e_1, e_2, \ldots, e_n\}\)</span> is a basis of<span class="math inline">\(V\)</span>, then the dual basis <spanclass="math inline">\(\{\alpha^1, \alpha^2, \ldots, \alpha^n\}\)</span>of <span class="math inline">\(V^*\)</span> is defined by: <spanclass="math display">\[ \alpha^i(e_j) = \delta^i_j \]</span> where <spanclass="math inline">\(\delta^i_j\)</span> is the Kronecker delta.</p><p>To show that <span class="math inline">\(\{\alpha^1, \alpha^2,\ldots, \alpha^n\}\)</span> is a basis of <spanclass="math inline">\(V^*\)</span>, we need to show that it is linearlyindependent and spans <span class="math inline">\(V^*\)</span>.</p><p>Let <span class="math inline">\(f \in V^*\)</span> be a linearfunctional. We can express <span class="math inline">\(f\)</span> interms of the dual basis as: <span class="math display">\[ f =\sum_{i=1}^n f(e_i) \alpha^i \]</span></p><p>where <span class="math inline">\(f(e_i)\)</span> are thecoefficients of the linear functional <spanclass="math inline">\(f\)</span> in the dual basis. This shows that<span class="math inline">\(\{\alpha^1, \alpha^2, \ldots,\alpha^n\}\)</span> spans <span class="math inline">\(V^*\)</span>.</p><p>If <span class="math inline">\(\sum_{i=1}^n c_i \alpha^i = 0\)</span>for some coefficients <span class="math inline">\(c_i\)</span>, then forany basis vector <span class="math inline">\(e_j \in V\)</span>, wehave: <span class="math display">\[ \sum_{i=1}^n c_i \alpha^i(e_j) =\sum_{i=1}^n c_i \delta^i_j = c_j = 0 \]</span> for all <spanclass="math inline">\(j = 1, 2, \ldots, n\)</span>. This implies thatall coefficients <span class="math inline">\(c_i = 0\)</span>, showingthat the dual basis is linearly independent.</p><h3 id="multilinear-function">Multilinear function</h3><p>Let <span class="math inline">\(V^k\)</span> be the <spanclass="math inline">\(k\)</span>-fold Cartesian product of a vectorspace <span class="math inline">\(V\)</span>, i.e., <spanclass="math inline">\(V^k = \underbrace{V \times V \times \ldots \timesV}_{k\text{ times}}\)</span> . A <strong>multilinear function</strong>or <strong><span class="math inline">\(k\)</span>-linearfunction</strong> is a function <span class="math inline">\(f: V^k \to\mathbb{R}\)</span> that is linear in each argument separately. It is atype <span class="math inline">\((0,k)\)</span> tensor.</p><p>One can prove that all multilinear functions on <spanclass="math inline">\(V\)</span> forms a vector space, denoted as <spanclass="math inline">\(L_k(V)\)</span> or <spanclass="math inline">\(\text{Mult}(V^k; \mathbb{R})\)</span>.</p><h4 id="examples">Examples</h4><ol type="1"><li><p><strong>Dot Product</strong>: For a <spanclass="math inline">\(\mathbb{R}\)</span>-vector space <spanclass="math inline">\(V\)</span> with a standard basis <spanclass="math inline">\(\{e_1, e_2, \ldots, e_n\}\)</span>, the dotproduct is a bilinear function <span class="math inline">\(f: V^2 \to\mathbb{R}\)</span> defined as: <span class="math display">\[ f(x,y) =\sum_{i=1}^n x^i y^i \]</span> where <span class="math inline">\(x =\sum_{i=1}^n x^i e_i\)</span> and <span class="math inline">\(y =\sum_{i=1}^n y^i e_i\)</span> are vectors in <spanclass="math inline">\(V\)</span>.</p></li><li><p><strong>Determinant</strong>: The determinant is a multilinearfunction <span class="math inline">\(f: V^n \to \mathbb{R}\)</span>defined on the space of <span class="math inline">\(n \times n\)</span>matrices. It is linear in each row (or column) of the matrix.</p></li></ol><p>Let <span class="math inline">\(\sigma\in S_k\)</span> be apermutation of the indices <span class="math inline">\(\{1, 2, \ldots,k\}\)</span>. The left action of <spanclass="math inline">\(\sigma\)</span> on a multilinear function <spanclass="math inline">\(f: V^k \to \mathbb{R}\)</span> is defined as:<span class="math display">\[ \sigma f(v_1, v_2, \ldots, v_k) =f(v_{\sigma(1)}, v_{\sigma(2)}, \ldots, v_{\sigma(k)}) \]</span></p><p>A <strong>symmetric multilinear function</strong> is a multilinearfunction that is invariant under any permutation of its arguments.</p><p>Formally, <span class="math inline">\(\sigma f = f\)</span> for anypermutation <span class="math inline">\(\sigma\in S_k\)</span>.</p><p>An <strong>alternating multilinear function</strong> is a multilinearfunction that changes sign when two of its arguments are swapped, orequivalently, changes by a factor of <spanclass="math inline">\(sgn(\sigma)\)</span> when permuted by <spanclass="math inline">\(\sigma\)</span>.</p><p>Formally, <span class="math inline">\(\sigma f = \text{sgn}(\sigma)f\)</span> for any permutation <span class="math inline">\(\sigma\inS_k\)</span>, where <spanclass="math inline">\(\text{sgn}(\sigma)\)</span> is the sign of thepermutation.</p><p>All symmetric multilinear functions on <spanclass="math inline">\(V\)</span> forms a vector space, denoted as <spanclass="math inline">\(S_k(V)\)</span> or <spanclass="math inline">\(\text{Sym}(V^k; \mathbb{R})\)</span>. Allalternating multilinear functions on <spanclass="math inline">\(V\)</span> forms a vector space, denoted as <spanclass="math inline">\(A_k(V)\)</span> or <spanclass="math inline">\(\text{Alt}(V^k; \mathbb{R})\)</span>.</p><h4 id="symmetrizing-and-alternating-operators">Symmetrizing andAlternating Operators</h4><p>Let <span class="math inline">\(V\)</span> be a vector space and<span class="math inline">\(f: V^k \to \mathbb{R}\)</span> be amultilinear function. We can define the <strong>symmetrizingoperator</strong> and <strong>alternating operator</strong> on <spanclass="math inline">\(f\)</span>.</p><p>The <strong>symmetrizing operator</strong> <spanclass="math inline">\(S: L_k(V) \to S_k(V)\)</span> is defined as: <spanclass="math display">\[ S(f)(v_1, v_2, \ldots, v_k) = \frac{1}{k!}\sum_{\sigma \in S_k} \sigma f \]</span></p><p>The symmetrizing operator takes a multilinear function and produces asymmetric multilinear function by averaging over all permutations of itsarguments.</p><p>The <strong>alternating operator</strong> <spanclass="math inline">\(A: L_k(V) \to A_k(V)\)</span> is defined as: <spanclass="math display">\[A(f)(v_1, v_2, \ldots, v_k) = \frac{1}{k!} \sum_{\sigma \in S_k}\text{sgn}(\sigma) \sigma f\]</span></p><p>The alternating operator takes a multilinear function and produces analternating multilinear function by summing over all permutations of itsarguments, weighted by the sign of the permutation.</p><p>We shall only prove the alternating operator does produce analternating multilinear function. The proof for the symmetrizingoperator is trivial.</p><p><strong>Proof</strong>: Let <span class="math inline">\(f\)</span> bea multilinear function and <span class="math inline">\(A(f)\)</span> bethe result of the alternating operator. For any permutation <spanclass="math inline">\(\tau \in S_k\)</span>, we have:</p><p><span class="math display">\[\begin{align*}\tau A(f) &amp;= \frac{1}{k!} \sum_{\sigma \in S_k} \text{sgn}(\sigma)\tau \circ\sigma f \\&amp;= \frac{1}{k!} \sum_{\sigma \in S_k} \text{sgn}(\tau)\text{sgn}(\tau \circ \sigma) \tau \circ\sigma f \\&amp;= \frac{1}{k!} \sum_{\sigma&#39; \in S_k} \text{sgn}(\sigma&#39;)\sigma&#39; f \\&amp;= \text{sgn}(\tau) A(f)\end{align*}\]</span></p><p>The alternating operator <span class="math inline">\(A\)</span>satisfies the following properties: 1. <strong>Linearity</strong>: For<span class="math inline">\(f_1, f_2 \in L_k(V)\)</span> and <spanclass="math inline">\(r,s\in \mathbb{R}\)</span>, we have: <spanclass="math display">\[ A(r f_1 + s f_2) = r A(f_1) + s A(f_2) \]</span>2. <span class="math inline">\(A(f\otimes A(g)) = A(A(f)\otimes g) =A(f\otimes g)\)</span> 3. <span class="math inline">\(A\circ A =A\)</span></p><h3 id="tensor-product">Tensor Product</h3><p>Let <span class="math inline">\(f\in L_k(V)\)</span> and <spanclass="math inline">\(g\in L_l(V)\)</span> be two multilinear functionson <span class="math inline">\(V\)</span>. The <strong>tensorproduct</strong> of <span class="math inline">\(f\)</span> and <spanclass="math inline">\(g\)</span>, denoted as <spanclass="math inline">\(f \otimes g\)</span>, is a multilinear function on<span class="math inline">\(V^{k+l}\)</span> defined as: <spanclass="math display">\[ (f \otimes g)(v_1, v_2, \ldots, v_{k+l}) =f(v_1, v_2, \ldots, v_k) g(v_{k+1}, v_{k+2}, \ldots, v_{k+l})\]</span></p><p>The tensor product satisfies associativity and bilinearity. Formally,we have: 1. <strong>Associativity</strong>: <spanclass="math inline">\((f \otimes g) \otimes h = f \otimes (g \otimesh)\)</span> for any multilinear function <span class="math inline">\(h\in L_m(V)\)</span>. 2. <strong>Bilinearity</strong>: For <spanclass="math inline">\(f,f_1,f_2 \in L_k(V)\)</span> and <spanclass="math inline">\(g,g_1,g_2 \in L_l(V)\)</span>; <spanclass="math inline">\(r,s\in \mathbb R\)</span>, we have: <spanclass="math display">\[ (r f_1 + s f_2) \otimes g = r (f_1 \otimes g) +s (f_2 \otimes g) \]</span> <span class="math display">\[ f \otimes (rg_1 + s g_2) = r (f \otimes g_1) + s (f \otimes g_2) \]</span></p><p><strong>Example</strong>: Let <spanclass="math inline">\((e_1,\ldots,e_n)\)</span> be a basis of <spanclass="math inline">\(V\)</span>, <spanclass="math inline">\((\alpha^1,\ldots,\alpha^n)\)</span> be the dualbasis of <span class="math inline">\(V^*\)</span>. Then the innerproduct is a bilinear function <span class="math inline">\(f: V\times V\to \mathbb{R}\)</span> defined as: <span class="math display">\[\langle x,y\rangle = \sum_{i=1}^n x^i y^i \]</span> where <spanclass="math inline">\(x = \sum_{i=1}^n x^i e_i\)</span> and <spanclass="math inline">\(y = \sum_{i=1}^n y^i e_i\)</span> are vectors in<span class="math inline">\(V\)</span>.</p><p>Now, let <span class="math inline">\(g_{ij}=\langlee_i,e_j\rangle\)</span>, <span class="math inline">\(\langle x,y\rangle= \sum x^i y^j g_{ij} = \sum \alpha^i(x) \alpha^j(y) g_{ij} = \sum_{i,j}g_{ij} (\alpha^i \otimes \alpha^j)(x,y)\)</span>.</p><p>So we can write the inner product as a tensor product of the dualbasis: <span class="math display">\[ \langle x,y\rangle = \sum_{i,j}g_{ij} (\alpha^i \otimes \alpha^j)(x,y) \]</span></p><h3 id="wedge-product">Wedge Product</h3><p>The <strong>wedge product</strong> or <strong>exteriorproduct</strong> is an operation on alternating multilinear functionsthat produces a new alternating multilinear function. It is denoted by<span class="math inline">\(\wedge\)</span> and is defined asfollows:</p><p>For all <span class="math inline">\(f \in A_k(V)\)</span> and <spanclass="math inline">\(g \in A_l(V)\)</span>, the wedge product <spanclass="math inline">\(f \wedge g \in A_{k+l}(V)\)</span> is defined by:<span class="math display">\[(f \wedge g) = \binom{k+l}{k} A(f\otimes g)\]</span> or equivalently, <span class="math display">\[(f \wedge g)(v_1, v_2, \ldots, v_{k+l}) = \frac{1}{k!l!} \sum_{\sigma\in S_{k+l}} \text{sgn}(\sigma) f(v_{\sigma(1)}, v_{\sigma(2)}, \ldots,v_{\sigma(k)}) g(v_{\sigma(k+1)}, v_{\sigma(k+2)}, \ldots,v_{\sigma(k+l)})\]</span></p><p>To avoid division by factorials, we can define the wedge product assum over sorted permutations: <span class="math display">\[\begin{equation*}(f \wedge g)(v_1, v_2, \ldots, v_{k+l}) =  \sum_{\substack{\sigma \inS_{k+l} \\ \sigma(1) &lt; \sigma(2) &lt; \ldots &lt; \sigma(k) \\\sigma(k+1) &lt; \sigma(k+2) &lt; \ldots &lt; \sigma(k+l)}}\text{sgn}(\sigma) f(v_{\sigma(1)}, v_{\sigma(2)}, \ldots,v_{\sigma(k)}) g(v_{\sigma(k+1)}, v_{\sigma(k+2)}, \ldots,v_{\sigma(k+l)})\end{equation*}\]</span></p><p>Let <span class="math inline">\(f \in A_k(V)\)</span> and <spanclass="math inline">\(g \in A_l(V)\)</span> be alternating multilinearfunctions. The wedge product satisfies the following properties: 1.<strong>Bilinearity</strong>: It is bilinear on <spanclass="math inline">\(f\)</span> and <spanclass="math inline">\(g\)</span>. 2. <strong>Associativity</strong>: Itis associative, i.e., <span class="math inline">\((f \wedge g) \wedge h= f \wedge (g \wedge h)\)</span> for any alternating multilinearfunction <span class="math inline">\(h\)</span>. 3.<strong>Anticommutativity</strong>: It is anticommutative, i.e., <spanclass="math inline">\(f \wedge g = (-1)^{kl} g \wedge f\)</span>.Particularly, <span class="math inline">\(f \wedge f = 0\)</span> when<span class="math inline">\(k\)</span> is odd. 4. <spanclass="math inline">\(c\wedge f=cf\)</span>, <spanclass="math inline">\(\forall c\in \mathbb R\)</span>.</p><p>Let <span class="math inline">\(r\in N\)</span>, <spanclass="math inline">\(f_i\in A_{k_i}(V)\)</span> for <spanclass="math inline">\(i=1,2,\ldots,r\)</span>, the wedge product <spanclass="math inline">\(f_1 \wedge f_2 \wedge \ldots \wedge f_r\)</span>is <span class="math inline">\(\binom{k_1+\ldots+k_r}{k_1,\ldots,k_r}A(f_1\otimes f_2\otimes \ldots \otimes f_r)\)</span>, where <spanclass="math inline">\(\binom{k_1+\ldots+k_r}{k_1,\ldots,k_r}\)</span> isthe multinomial coefficient.</p><p>or equivalently, <span class="math display">\[ (f_1 \wedge f_2 \wedge\ldots \wedge f_r)(v_1, v_2, \ldots, v_{k_1+\ldots+k_r}) =\frac{1}{k_1!k_2!\ldots k_r!} \sum_{\sigma \in S_{k_1+\ldots+k_r}}\text{sgn}(\sigma) f_1(v_{\sigma(1)}, v_{\sigma(2)}, \ldots,v_{\sigma(k_1)}) f_2(v_{\sigma(k_1+1)}, v_{\sigma(k_1+2)}, \ldots,v_{\sigma(k_1+k_2)}) \ldots f_r(v_{\sigma(k_1+\ldots+k_{r-1}+1)},v_{\sigma(k_1+\ldots+k_{r-1}+2)}, \ldots, v_{\sigma(k_1+\ldots+k_r)})\]</span></p><p>This shows the associativity directly.</p><h4 id="relation-to-determinant">Relation to determinant</h4><p>Let <span class="math inline">\(\alpha^1,\ldots,\alpha^n\inV^*\)</span>, <span class="math inline">\(v_1,\ldots,v_n\in V\)</span>,then: <span class="math display">\[ (\alpha^1 \wedge \alpha^2 \wedge\ldots \wedge \alpha^n)(v_1, v_2, \ldots, v_n) =\det[(\alpha^i(v_j))_{i,j}] \]</span></p><p>This is the determinant of the matrix formed by evaluating the dualbasis vectors on the vectors <span class="math inline">\(v_1, v_2,\ldots, v_n\)</span>.</p><h3 id="graded-algebra-and-exterior-algebra">Graded Algebra and ExteriorAlgebra</h3><h4 id="graded-algebra">Graded Algebra</h4><p>Let <span class="math inline">\(\mathbb K\)</span> be a field, the<strong>graded algebra</strong> <spanclass="math inline">\((A,\times)\)</span> is a vector space <spanclass="math inline">\(A\)</span> over <spanclass="math inline">\(\mathbb K\)</span> equipped with a direct sumdecomposition <span class="math inline">\(A = \bigoplus_{k=0}^{\infty}A_k\)</span>, where each <span class="math inline">\(A_k\)</span> is avector space of degree <span class="math inline">\(k\)</span>. Themultiplication operation <span class="math inline">\(\times: A \times A\to A\)</span> satisfies: <span class="math display">\[ \times: A_k\times A_l \to A_{k+l} \]</span></p><p><span class="math inline">\((A,\times)\)</span> is called<strong>anticommutative</strong> if for all <spanclass="math inline">\(a \in A_k\)</span> and <spanclass="math inline">\(b \in A_l\)</span>, <span class="math inline">\(ab= (-1)^{kl} ba\)</span>.</p><p>A homomorphism of graded algebra is an algebra homomorphism thatpreserves the degree.</p><h4 id="exterior-algebra">Exterior Algebra</h4><p>The <strong>exterior algebra</strong> or <strong>Grassmannalgebra</strong> <span class="math inline">\((\bigwedge V,\wedge)\)</span> is a <strong>anticommutative graded algebra</strong>defined on a vector space <span class="math inline">\(V\)</span> withthe wedge product <span class="math inline">\(\wedge\)</span>, where<span class="math inline">\(\bigwedge V = \bigoplus_{k=0}^{\infty}A_k(V)\)</span>. When <span class="math inline">\(V\)</span> isfinite-dimensional, we can write <span class="math inline">\(\bigwedge V= \bigoplus_{k=0}^{\dim V} A_k(V)\)</span> or <spanclass="math inline">\(\bigwedge V = \bigoplus_{k=0}^{n} \bigwedge ^kV\)</span>, where <span class="math inline">\(n = \dim V\)</span>.</p><p>This is because <span class="math inline">\(A_k(V) = 0\)</span> for<span class="math inline">\(k &gt; \dim V\)</span>, which we shall provelater.</p><p><span class="math inline">\(\bigwedge^k V\)</span> is called the<strong><span class="math inline">\(k\)</span>-th exteriorpower</strong> of <span class="math inline">\(V\)</span>.</p><h4 id="basis-of-l_k">Basis of <spanclass="math inline">\(L_k\)</span></h4><p>Let <span class="math inline">\(I=(i_1,i_2,\ldots,i_k)\)</span> be a<span class="math inline">\(k\)</span>-indices, <spanclass="math inline">\(e_1,\ldots,e_n\)</span> and <spanclass="math inline">\(\alpha^1,\ldots,\alpha^n\)</span> are the basis ofthe vector space <span class="math inline">\(V\)</span> and its dual<span class="math inline">\(V^*\)</span> respectively. Denote <spanclass="math inline">\((e_{i_1}, e_{i_2}, \ldots, e_{i_k})\)</span> as<span class="math inline">\(e_I\)</span>, and <spanclass="math inline">\(\alpha^I = \alpha^{i_1} \wedge \alpha^{i_2} \wedge\ldots \wedge \alpha^{i_k}\)</span>. <spanclass="math inline">\(\alpha^I(e_J) = \delta^I_J\)</span>, where <spanclass="math inline">\(\delta^I_J\)</span> is the generalized Kroneckerdelta, which is <span class="math inline">\(1\)</span> if <spanclass="math inline">\(I=J\)</span> and <spanclass="math inline">\(0\)</span> otherwise.</p><p>Let <span class="math inline">\(I=(1 \leq i_1 &lt; \cdots &lt; i_k\leq n)\)</span>, we claim that <spanclass="math inline">\(\{\alpha^I\}_{I}\)</span> is a basis of <spanclass="math inline">\(A_k(V)\)</span>.</p><p>The proof is similar to the proof in the dual space section.</p><p>Corollary 1: The dimension of <spanclass="math inline">\(A_k(V)\)</span> is <spanclass="math inline">\(\binom{n}{k}\)</span>, where <spanclass="math inline">\(n = \dim V\)</span>.</p><p>Corollary 2: The dimension of <span class="math inline">\(\bigwedgeV\)</span> is <span class="math inline">\(\sum_{k=0}^{n} \binom{n}{k} =2^n\)</span>, where <span class="math inline">\(n = \dim V\)</span>.</p><p>Corollary 3: The dimension of <spanclass="math inline">\(A_k(V)\)</span> is <spanclass="math inline">\(0\)</span> for <span class="math inline">\(k &gt;\dim V\)</span>.</p><p>Proof: For <span class="math inline">\(k &gt; \dim V\)</span>, theset of indices <spanclass="math inline">\(I=(i_1,i_2,\ldots,i_k)\)</span> cannot be chosensuch that <span class="math inline">\(1 \leq i_1 &lt; i_2 &lt; \ldots&lt; i_k \leq n\)</span>, hence <span class="math inline">\(A_k(V) =0\)</span>.</p><h3 id="interior-product">Interior Product</h3><p>Let <span class="math inline">\(V\)</span> be a vector space, <spanclass="math inline">\(v\in V\)</span>, and <spanclass="math inline">\(\omega \in \bigwedge^k V\)</span>. The<strong>interior product</strong> of <spanclass="math inline">\(v\)</span> and <spanclass="math inline">\(\omega\)</span>, denoted as <spanclass="math inline">\(\iota_v \omega\)</span>, is a linear map <spanclass="math inline">\(\iota_v: \bigwedge^k V \to \bigwedge^{k-1}V\)</span> defined by: <span class="math display">\[ \iota_v(\omega) =\sum_{i=1}^k (-1)^{i-1} \alpha^i(v) \alpha^{1} \wedge \ldots \wedge\widehat{\alpha^i} \wedge \ldots \wedge \alpha^{k} \]</span></p><p>where <span class="math inline">\(\widehat{\alpha^i}\)</span> meansthat the <span class="math inline">\(i\)</span>-th term is omitted fromthe wedge product.</p><p>Let <span class="math inline">\(\alpha \in \bigwedge^k V, \beta \in\bigwedge^l V\)</span>, the interior product satisfies the followingproperties: 1. <strong>Nilpotency</strong>: <spanclass="math inline">\(\iota_v \iota_v \alpha = 0\)</span> for any <spanclass="math inline">\(v \in V\)</span>. 2. <spanclass="math inline">\(\iota_v(\alpha \wedge \beta) = \iota_v(\alpha)\wedge \beta + (-1)^k \alpha \wedge \iota_v(\beta)\)</span>. 3.<strong>Linearity</strong>: <span class="math inline">\(\iota_{av +bw}(\alpha) = a \iota_v(\alpha) + b \iota_w(\alpha)\)</span> for any<span class="math inline">\(a,b \in \mathbb{R}\)</span> and <spanclass="math inline">\(v,w \in V\)</span>.</p><p>The interior product <span class="math inline">\(\iota_v\)</span> iscalled the <strong>interior product</strong> or<strong>contraction</strong> with respect to the vector <spanclass="math inline">\(v\)</span>. It reduces the degree of the form by1.</p><p>Observe the definition, we see that it just the evaluation of thefirst argument of the alternating multilinear function <spanclass="math inline">\(\omega\)</span> at the vector <spanclass="math inline">\(v\)</span>, and then take the wedge product of theremaining arguments.</p><p><span class="math display">\[ \iota_v(\omega)(v_1, v_2, \ldots,v_{k-1}) = \omega(v, v_1, v_2, \ldots, v_{k-1}) \]</span></p><h3 id="pullback">Pullback</h3><p>Let <span class="math inline">\(L: W \to V\)</span> be a linear mapbetween vector spaces <span class="math inline">\(W\)</span> and <spanclass="math inline">\(V\)</span>. The <strong>pullback</strong> ofexterior forms under <span class="math inline">\(L\)</span>, denoted as<span class="math inline">\(L^*: \bigwedge^k V \to \bigwedge^kW\)</span>, is defined by: <span class="math display">\[L^*(\omega)(w_1, w_2, \ldots, w_k) = \omega(L(w_1), L(w_2), \ldots,L(w_k)) \]</span> for all <span class="math inline">\(w_1, w_2, \ldots,w_k \in W^*\)</span>.</p><p>The pullback satisfies the following properties: 1. <spanclass="math inline">\(L^*(\alpha \wedge \beta) = L^*(\alpha) \wedgeL^*(\beta)\)</span> 2. <span class="math inline">\(\iota_w(L^*(\alpha))= L^*(\iota_{L(w)}(\alpha))\)</span>.</p><h3 id="differential-forms">Differential Forms</h3><p>We have defined the exterior algebra <spanclass="math inline">\(\bigwedge V\)</span> and the wedge product <spanclass="math inline">\(\wedge\)</span> on an abstract vector space <spanclass="math inline">\(V\)</span>. Now we can define <strong>differentialforms</strong> on a manifold <span class="math inline">\(M\)</span>,where <span class="math inline">\(V\)</span> is the tangent space <spanclass="math inline">\(T_pM\)</span> at a point <spanclass="math inline">\(p \in M\)</span>.</p><h4 id="cotangent-space-and-cotangent-bundle">Cotangent Space andCotangent Bundle</h4><p>The <strong>cotangent space</strong> <spanclass="math inline">\(T^*_pM\)</span> at a point <spanclass="math inline">\(p \in M\)</span> is the dual space of the tangentspace <span class="math inline">\(T_pM\)</span>. It consists of alllinear functionals on <span class="math inline">\(T_pM\)</span>.Elements of the cotangent space are called <strong>covectors</strong> or<strong>differential 1-forms</strong>.</p><p>The cotangent space <span class="math inline">\(T^*_pM\)</span> is avector space of dimension <span class="math inline">\(n\)</span>, where<span class="math inline">\(n\)</span> is the dimension of the manifold<span class="math inline">\(M\)</span>.</p><p>The <strong>cotangent bundle</strong> <spanclass="math inline">\(T^*M\)</span> is the disjoint union of allcotangent spaces at each point in <spanclass="math inline">\(M\)</span>: <span class="math display">\[ T^*M =\bigcup_{p \in M} T^*_pM = \{(p, \omega_p) : p \in M, \omega_p \inT^*_pM\} \]</span></p><p>The cotangent bundle <span class="math inline">\(T^*M\)</span> is amanifold of dimension <span class="math inline">\(2n\)</span>.</p><p>Local coordinates on the cotangent bundle can be defined as <spanclass="math inline">\((x^1, x^2, \ldots, x^n, \omega_1, \omega_2,\ldots, \omega_n)\)</span>, where <span class="math inline">\((x^1, x^2,\ldots, x^n)\)</span> are local coordinates on <spanclass="math inline">\(M\)</span> and <spanclass="math inline">\((\omega_1, \omega_2, \ldots, \omega_n)\)</span>are the components of a covector in the cotangent space with respect tothe dual basis <span class="math inline">\(\{dx^1, dx^2, \ldots,dx^n\}\)</span>.</p><h4 id="differential-k-forms">Differential <spanclass="math inline">\(k\)</span>-Forms</h4><p>Let <span class="math inline">\(M\)</span> be a smooth manifold ofdimension <span class="math inline">\(n\)</span>. A <strong>differential<span class="math inline">\(k\)</span>-form</strong> on <spanclass="math inline">\(M\)</span> is a smooth section of the <spanclass="math inline">\(k\)</span>-th exterior power of the cotangentbundle, denoted as <span class="math inline">\(\bigwedge^kT^*M\)</span>.</p><p>In other words, a differential <spanclass="math inline">\(k\)</span>-form on a open subset <spanclass="math inline">\(U \subseteq M\)</span> is a mapping <spanclass="math display">\[\omega: U \to \bigwedge^k T^*M\]</span> that assigns to each point <span class="math inline">\(p \inU\)</span> an alternating multilinear function <spanclass="math inline">\(\omega(p): T_pM^k \to \mathbb{R}\)</span> and<span class="math inline">\(\omega \in A_k(T_pM)\)</span> for all <spanclass="math inline">\(p \in U\)</span>.</p><p>Let <span class="math inline">\(\{e_1 =\left.\frac{\partial}{\partial x^1}\right|_p, e_2 =\left.\frac{\partial}{\partial x^2}\right|_p, \ldots, e_n =\left.\frac{\partial}{\partial x^n}\right|_p\}\)</span> be a basis of<span class="math inline">\(T_pM\)</span>. The corresponding dual basisof <span class="math inline">\(T^*_pM\)</span> is <spanclass="math inline">\(\{dx^1, dx^2, \ldots, dx^n\}\)</span>, where <spanclass="math inline">\(dx^i(e_j) = \delta^i_j\)</span>.</p><p>A basis of <span class="math inline">\(A_k(T_pM)\)</span> is thengiven by <span class="math inline">\(\{dx^I\}_{I}\)</span>, where <spanclass="math inline">\(I = (i_1, i_2, \ldots, i_k)\)</span> is a <spanclass="math inline">\(k\)</span>-tuple of indices with <spanclass="math inline">\(1 \leq i_1 &lt; i_2 &lt; \ldots &lt; i_k \leqn\)</span>, and <span class="math inline">\(dx^I\)</span> denotes thewedge product <span class="math inline">\(dx^{i_1} \wedge dx^{i_2}\wedge \ldots \wedge dx^{i_k}\)</span>.</p><p>Therefore, a differential <span class="math inline">\(k\)</span>-form<span class="math inline">\(\omega\)</span> can be expressed in localcoordinates as: <span class="math display">\[ \omega = \sum_{I} f_I dx^I\]</span> where <span class="math inline">\(f_I: U \to\mathbb{R}\)</span> are functions on <spanclass="math inline">\(U\)</span>.</p><p>A differential <span class="math inline">\(k\)</span>-form <spanclass="math inline">\(\omega\)</span> is called <strong>smooth</strong>if all the functions <span class="math inline">\(f_I\)</span> are smoothfunctions on <span class="math inline">\(U\)</span>.</p><p>Let <spanclass="math inline">\(\Omega^k(M)=\Gamma^{\infty}(\bigwedge^kT^*M)\)</span> be the space of all smooth differential <spanclass="math inline">\(k\)</span>-forms on a manifold <spanclass="math inline">\(M\)</span>. It is a vector space over <spanclass="math inline">\(\mathbb{R}\)</span>.</p><p>The wedge product of two differential forms <spanclass="math inline">\(\omega_1 \in \Omega^k(M)\)</span> and <spanclass="math inline">\(\omega_2 \in \Omega^l(M)\)</span> is pointwisedefined as: <span class="math display">\[ (\omega_1 \wedge \omega_2)(p)= \omega_1(p) \wedge \omega_2(p) = \sum_{I,J} f_I(p) g_J(p) dx^I \wedgedx^J \]</span> where <span class="math inline">\(f_I\)</span> and <spanclass="math inline">\(g_J\)</span> are the coefficients of <spanclass="math inline">\(\omega_1\)</span> and <spanclass="math inline">\(\omega_2\)</span> in local coordinates,respectively. Note that if <span class="math inline">\(I\)</span> and<span class="math inline">\(J\)</span> are such that <spanclass="math inline">\(I \cap J \neq \emptyset\)</span>, then <spanclass="math inline">\(dx^I \wedge dx^J = 0\)</span> due to theanticommutativity of the wedge product. The wedge product ofdifferential forms is bilinear and associative, and it satisfies theanticommutativity property: <span class="math display">\[ \omega_1\wedge \omega_2 = (-1)^{kl} \omega_2 \wedge \omega_1 \]</span></p><p>Let <span class="math inline">\(\Omega^*(M) = \bigoplus_{k=0}^n\Omega^k(M)\)</span> be the space of all smooth differential forms on<span class="math inline">\(M\)</span>. It is a graded algebra withrespect to the wedge product <spanclass="math inline">\(\wedge\)</span>.</p><h4 id="coordinate-functions-and-their-differentials">CoordinateFunctions and Their Differentials</h4><p>Let <span class="math inline">\((U, x) = (U, x^1, \ldots,x^n)\)</span> be a coordinate chart on a smooth manifold <spanclass="math inline">\(M\)</span> of dimension <spanclass="math inline">\(n\)</span>. The coordinate functions <spanclass="math inline">\(x^i: U \to \mathbb{R}\)</span> are smoothfunctions that assign to each point <span class="math inline">\(p \inU\)</span> its <span class="math inline">\(i\)</span>-th coordinate<span class="math inline">\(x^i(p)\)</span>.</p><p>The <strong>coordinate 1-forms</strong> <spanclass="math inline">\(dx^i\)</span> are smooth sections of the cotangentbundle, each <span class="math inline">\(dx^i: U \to T^*M\)</span>satisfies <span class="math display">\[\pi \circ dx^i = \text{id}_U\]</span> where <span class="math inline">\(\pi: T^*M \to M\)</span> isthe bundle projection.</p><p>We have <span class="math inline">\(dx^i|_p: T_pM \to\mathbb{R}\)</span> defined by <span class="math inline">\(dx^i|_p(v) =v(x^i(p))\)</span> for any tangent vector <span class="math inline">\(v\in T_pM\)</span>. The coordinate 1-forms satisfy the dual basisproperty: <span class="math display">\[\left.dx^i\right|_p\left(\left.\frac{\partial}{\partialx^j}\right|_p\right) = \delta^i_j\]</span> making <span class="math inline">\(\{dx^1|_p, \ldots,dx^n|_p\}\)</span> the dual basis of <spanclass="math inline">\(T_p^*M\)</span> corresponding to the coordinatebasis <span class="math inline">\(\{\frac{\partial}{\partial x^1}|_p,\ldots, \frac{\partial}{\partial x^n}|_p\}\)</span> of <spanclass="math inline">\(T_pM\)</span>.</p><p>Any differential <span class="math inline">\(k\)</span>-form on <spanclass="math inline">\(U\)</span> can be uniquely expressed as <spanclass="math inline">\(\omega = \sum_{|I|=k} f_I \, dx^I\)</span>, where<span class="math inline">\(I = (i_1, \ldots, i_k)\)</span> with <spanclass="math inline">\(1 \leq i_1 &lt; \cdots &lt; i_k \leq n\)</span>,<span class="math inline">\(dx^I = dx^{i_1} \wedge \cdots \wedgedx^{i_k}\)</span>, and <span class="math inline">\(f_I: U \to\mathbb{R}\)</span> are smooth functions.</p><h3 id="pullback-of-differential-forms">Pullback of DifferentialForms</h3><p>Let <span class="math inline">\(F: M \to N\)</span> be a smooth mapbetween manifolds, and let <span class="math inline">\(\omega \in\Omega^k(N)\)</span> be a differential <spanclass="math inline">\(k\)</span>-form on <spanclass="math inline">\(N\)</span>. The <strong>pullback</strong> of <spanclass="math inline">\(\omega\)</span> by <spanclass="math inline">\(F\)</span>, denoted as <spanclass="math inline">\(F^*\omega\)</span>, is a differential <spanclass="math inline">\(k\)</span>-form on <spanclass="math inline">\(M\)</span> defined by: <spanclass="math display">\[ (F^*\omega)(p)(v_1, v_2, \ldots, v_k) =\omega(F(p))(dF_p(v_1), dF_p(v_2), \ldots, dF_p(v_k)) \]</span> where<span class="math inline">\(dF_p: T_pM \to T_{F(p)}N\)</span> is thedifferential of <span class="math inline">\(F\)</span> at the point<span class="math inline">\(p\)</span>.</p><h4 id="local-coordinate-expression">Local Coordinate Expression</h4><p>Let <span class="math inline">\((U, x^1, \ldots, x^m)\)</span> and<span class="math inline">\((V, y^1, \ldots, y^n)\)</span> be coordinatecharts on <span class="math inline">\(M\)</span> and <spanclass="math inline">\(N\)</span> respectively, with <spanclass="math inline">\(F(U) \subseteq V\)</span>.</p><p>Let <span class="math inline">\(F: M \to N\)</span> have localcoordinate representation <span class="math inline">\(F^i = y^i \circ F\circ x^{-1}\)</span>, where <span class="math inline">\((x^1, \ldots,x^m)\)</span> are coordinates on <span class="math inline">\(M\)</span>and <span class="math inline">\((y^1, \ldots, y^n)\)</span> arecoordinates on <span class="math inline">\(N\)</span>.</p><p>If <span class="math inline">\(\omega\)</span> has the localexpression: <span class="math display">\[ \omega = \sum_{|I|=k} f_I \,dy^I \]</span> where <span class="math inline">\(f_I: V \to\mathbb{R}\)</span> are smooth functions.</p><p>Then the pullback <span class="math inline">\(F^*\omega\)</span> hasthe expression: <span class="math display">\[\begin{align*}F^*\omega &amp;= \sum_{|I|=k} (f_I \circ F) \, F^*(dy^I) \\&amp;= \sum_{|I|=k} (f_I \circ F) \, d(y^{i_1} \circ F) \wedge \cdots\wedge d(y^{i_k} \circ F)\end{align*}\]</span></p><p>Note: <span class="math display">\[ d(y^{i_j} \circ F) = d(F^{i_j}\circ x) = \sum_{l=1}^m \frac{\partial F^{i_j}}{\partial x^l} dx^l\]</span></p><p>Therefore: <span class="math display">\[ F^*\omega = \sum_{|I|=k}(f_I \circ F) \, \left(\sum_{l_1=1}^m \frac{\partial F^{i_1}}{\partialx^{l_1}} dx^{l_1}\right) \wedge \cdots \wedge \left(\sum_{l_k=1}^m\frac{\partial F^{i_k}}{\partial x^{l_k}} dx^{l_k}\right) \]</span></p><h4 id="properties-of-pullback">Properties of Pullback</h4><p>The pullback operation satisfies the following importantproperties:</p><ol type="1"><li><p><strong>Linearity</strong>: <spanclass="math inline">\(F^*(a\omega_1 + b\omega_2) = aF^*\omega_1 +bF^*\omega_2\)</span> for <span class="math inline">\(a,b \in\mathbb{R}\)</span></p></li><li><p><strong>Preservation of wedge product</strong>: <spanclass="math display">\[ F^*(\omega_1 \wedge \omega_2) = F^*\omega_1\wedge F^*\omega_2 \]</span></p></li><li><p><strong>Functoriality</strong>: If <span class="math inline">\(G:L \to M\)</span> and <span class="math inline">\(F: M \to N\)</span> aresmooth maps, then: <span class="math display">\[ (F \circ G)^* = G^*\circ F^* \]</span></p></li><li><p><strong>Identity</strong>: <spanclass="math inline">\((\text{id}_M)^* =\text{id}_{\Omega^*(M)}\)</span></p></li></ol><h3 id="exterior-derivative">Exterior Derivative</h3><p>The <strong>Exterior Derivative</strong> is an operator <spanclass="math inline">\(d: \Omega^k(M) \to \Omega^{k+1}(M)\)</span> thatgeneralizes the concept of differentiation to differential forms. It isdefined as follows: <span class="math display">\[d\omega = \sum_{|I|=k} \left( \sum_{j=1}^n \frac{\partial f_I}{\partialx^j} \right) dx^j \wedge dx^I\]</span> where <span class="math inline">\(\omega = \sum_{|I|=k} f_I \,dx^I\)</span> is a differential <spanclass="math inline">\(k\)</span>-form on <spanclass="math inline">\(M\)</span>, and <span class="math inline">\(dx^I =dx^{i_1} \wedge \cdots \wedge dx^{i_k}\)</span> for <spanclass="math inline">\(I = (i_1, \ldots, i_k)\)</span>.</p><p>It can be naturally extended to the exterior algebra <spanclass="math inline">\(\bigwedge^*(M)\)</span> as follows: <spanclass="math display">\[ d(\omega) = \sum_{k=0}^{n} d\omega_k\]</span></p><p>where <span class="math inline">\(\omega = \sum_{k=0}^{n}\omega_k\)</span> and <span class="math inline">\(\omega_k \in\Omega^k(M)\)</span>.</p><p>The exterior derivative satisfies the following properties: 1.<strong>Linearity</strong>: <span class="math inline">\(d(a\omega_1 +b\omega_2) = a d\omega_1 + b d\omega_2\)</span> for <spanclass="math inline">\(a,b \in \mathbb{R}\)</span>. 2.<strong>Nilpotency</strong>: <span class="math inline">\(d^2 =0\)</span>, i.e., <span class="math inline">\(d(d\omega) = 0\)</span>for any differential form <span class="math inline">\(\omega\)</span>.3. <strong>Antiderivation Property</strong>: <spanclass="math inline">\(d(\omega_1 \wedge \omega_2) = d\omega_1 \wedge\omega_2 + (-1)^k \omega_1 \wedge d\omega_2\)</span> for <spanclass="math inline">\(\omega_1 \in \Omega^k(M)\)</span> and <spanclass="math inline">\(\omega_2 \in \Omega^l(M)\)</span>. 4.<strong>Pullback Compatibility</strong>: For any smooth map <spanclass="math inline">\(F: M \to N\)</span>, we have <spanclass="math inline">\(F^*(d\omega) = d(F^*\omega)\)</span>. One canverify this property using local coordinates and chain rules. 5.<strong>Vanishing of Top Forms</strong>: <spanclass="math inline">\(d\omega = 0\)</span> if <spanclass="math inline">\(\omega\in \Omega^n(M)\)</span>, where <spanclass="math inline">\(n = \dim M\)</span>.</p><p>We only prove the nilpotency here:</p><p>Proof:</p><p>Only to prove <span class="math inline">\(\omega\in\Omega^k(M)\)</span>, <span class="math inline">\(\omega = \sum_{|I|=k}\alpha^{I} dx^{I}\)</span> $$ <spanclass="math display">\[\begin{align*}d \omega &amp;= \sum_{|I|=k}\sum_{i\in[n]}\frac{\partial\alpha^I}{\partial x^i}d x^i \wedge dx^I\\d(d\omega) &amp;= \sum_{i,j\in [n]}\sum_{|I|=k} \frac{\partial^2\alpha^I}{\partial x^i\partial x^j} d x^i \wedge d x^j \wedge d x^I \\&amp;= \sum_{1\leq i&lt;j\leq n} \sum_{|I|=k}\left ( \frac{\partial^2\alpha^I}{\partial x^i\partial x^j} - \frac{\partial^2\alpha^I}{\partial x^j\partial x^i} \right ) d x^i \wedge d x^j \wedge dx^I \\&amp;= 0\end{align*}\]</span> $$ since mixed partial derivatives commute forsmooth functions.</p><h3 id="lie-derivative-on-differential-forms">Lie Derivative onDifferential Forms</h3><p>The <strong>Lie derivative</strong> of a differential form <spanclass="math inline">\(\omega \in \Omega^k(M)\)</span> with respect to avector field <span class="math inline">\(X\)</span> is defined as: <spanclass="math display">\[ \mathcal{L}_X \omega = \frac{d}{dt} \bigg|_{t=0}\phi_t^* \omega \]</span> where <spanclass="math inline">\(\phi_t\)</span> is the flow of the vector field<span class="math inline">\(X\)</span> at time <spanclass="math inline">\(t\)</span>.</p><p>The Lie derivative satisfy the following property: 1. <spanclass="math inline">\(d\mathcal L_X \omega=\mathcal L_X d\omega\)</span>2. <span class="math inline">\(\mathcal L_X(\omega \wedge \eta)=\mathcalL_X \omega \wedge \eta + \omega \wedge \mathcal L_X\eta\)</span> 3.<span class="math inline">\(\mathcal L_{[X_1,X_2]} \omega = \mathcalL_{X_1}\mathcal L_{X_2}\omega - \mathcal L_{X_2}\mathcalL_{X_1}\omega\)</span> 4. <span class="math inline">\((\mathcalL_X{\omega})(X_1,\cdots,X_k)=\mathcalL_X(\omega(X_1,\cdots,X_k))-\sum_i\omega(X_1,\cdots,\mathcal L_XX_i,\cdots,X_k)\)</span> 5. <strong>Cartan’s Magic Formula</strong><span class="math display">\[L_{X}\omega=d\iota_X\omega + \iota_Xd\omega\]</span></p><p><strong>Proof</strong></p><ol type="1"><li><span class="math inline">\(d\mathcal L_X \omega =d\left(\frac{d}{dt} \bigg|_{t=0} \phi_t^* \omega \right)= \frac{d}{dt}\bigg|_{t=0} d(\phi_t^* \omega) = \frac{d}{dt} \bigg|_{t=0} \phi_t^*(d\omega) = \mathcal L_X d\omega\)</span></li><li><span class="math inline">\(\mathcal L_X(\omega \wedge \eta) =\frac{d}{dt} \bigg|_{t=0} \phi_t^* (\omega \wedge \eta) = \frac{d}{dt}\bigg|_{t=0} (\phi_t^* \omega \wedge \phi_t^* \eta) =\frac{d}{dt}\bigg|_{t=0} (\phi_t^*\omega)\wedge\eta+\omega\wedge\frac{d}{dt}\bigg|_{t=0}(\phi_t^*\eta) = \mathcal L_X\omega \wedge \eta + \omega \wedge \mathcal L_X\eta\)</span></li><li>Use (5).</li><li>Do induction as (5).</li><li>For the <span class="math inline">\(0\)</span>-form and <spanclass="math inline">\(1\)</span>-form, it is trivial proved to be true.Then inductively prove by decomposing it on local coordinates. Let <spanclass="math inline">\(\omega\)</span> be a <spanclass="math inline">\(k\)</span>-form. Locally, we have: <spanclass="math display">\[\omega=f dx^1\wedge \cdots \wedge dx^n=dx^1\wedge \omega_1\]</span></li></ol><p><span class="math display">\[\begin{align*}\mathcal L_X \omega &amp;= \mathcal L_X (dx^1) \wedge \omega_1 + dx^1\wedge \mathcal L_X\omega_1\\&amp;=[\iota_X d(dx^1)+d(\iota_X dx^1)]\wedge \omega_1+dx^1\wedge[\iota_X d\omega_1+ d(\iota_X \omega_1)]\end{align*}\]</span> <span class="math display">\[\begin{align*}d\iota_X \omega + \iota_X d\omega &amp;= d\iota_X(dx^1\wedge \omega_1) +\iota_X (-dx^1\wedge d\omega_1)\\&amp;=d((\iota_Xdx^1)\wedge \omega_1-dx^1\wedge \iota_X \omega_1)-(\iota_X dx^1)\wedge d\omega_1 + dx^1\wedge\iota_X d\omega_1\\&amp;=d(\iota_X dx^1)\wedge \omega_1+(\iota_Xdx^1)\wedged\omega_1+dx^1\wedge d(\iota_X\omega_1) -(\iota_X dx^1)\wedge d\omega_1+ dx^1\wedge\iota_X d\omega_1\\&amp;=d(\iota_X dx^1)\wedge \omega_1+dx^1\wedge d(\iota_X\omega_1) +dx^1\wedge\iota_X d\omega_1\\&amp;=d(\iota_X dx^1)\wedge \omega_1+dx^1\wedge [\iota_Xd\omega_1+d(\iota_X \omega_1)]\\&amp;=\mathcal L_X \omega\end{align*}\]</span></p><h3 id="integration-of-differential-forms">Integration of DifferentialForms</h3><h4 id="highest-degree-forms">Highest Degree Forms</h4><p>Let <span class="math inline">\(M\)</span> be a smooth manifold ofdimension <span class="math inline">\(n\)</span>. The <strong>highestdegree differential form</strong> on <spanclass="math inline">\(M\)</span> is a differential <spanclass="math inline">\(n\)</span>-form, denoted as <spanclass="math inline">\(\omega \in \Omega^n(M)\)</span>. It can beexpressed in local coordinates <spanclass="math inline">\((U,x)\)</span> as: <span class="math display">\[\omega = f \, dx^1 \wedge dx^2 \wedge \ldots \wedge dx^n \]</span> where<span class="math inline">\(f: M \to \mathbb{R}\)</span> is a smoothfunction and <span class="math inline">\(\{dx^1, dx^2, \ldots,dx^n\}\)</span> is the basis of the cotangent space at each point in<span class="math inline">\(M\)</span>.</p><p>If we change the local coordinates to <spanclass="math inline">\((y^1, y^2, \ldots, y^n)\)</span> by the pullbackmap <span class="math inline">\(\phi: V \to U\)</span>, <spanclass="math inline">\(y=\phi^*x = x\circ \phi\)</span>, then the highestdegree form transforms as: <span class="math display">\[ \phi^*(dx^1\wedge dx^2 \wedge \ldots \wedge dx^n) = \det\left(\frac{\partialy^i}{\partial x^j}\right) dy^1 \wedge dy^2 \wedge \ldots \wedge dy^n\]</span></p><p>We can define the integral of a highest degree form over <spanclass="math inline">\(U\)</span> as: <span class="math display">\[\int_U \omega = \int_U f \, dx^1 \wedge dx^2 \wedge \ldots \wedge dx^n=\int_U f dx^1 dx^2 \cdots dx^n \]</span> where the integral is takenover the open set <span class="math inline">\(U \subseteqM\)</span>.</p><p>To make integral is well-defined and does not depend on the choice oflocal coordinates, we need to ensure that the integral is invariantunder coordinate changes.</p><p>But according to the change of variables formula on integral over<span class="math inline">\(\mathbb{R}^n\)</span>, we only have: <spanclass="math display">\[\int_U f dx^1dx^2 \cdots dx^n = \int_V f \left|\det\left(\frac{\partialy^i}{\partial x^j}\right)\right| dy^1 dy^2 \cdots dy^n\]</span></p><p>So to make the integral well-defined, we need to restrict thetransformation to be orientation-preserving, i.e., <spanclass="math inline">\(\det\left(\frac{\partial y^i}{\partial x^j}\right)&gt; 0\)</span>.</p><p>We call the two charts <span class="math inline">\((U,x)\)</span> and<span class="math inline">\((V,y)\)</span><strong>orientation-compactible</strong> if the Jacobian determinant<span class="math inline">\(\det\left(\frac{\partial y^i}{\partialx^j}\right)\)</span> is positive for all points in <spanclass="math inline">\(U \cap V\)</span>.</p><p>For a manifold, if all of the transition maps between charts areorientation-preserving, we say that the manifold is<strong>oriented</strong>. And if such a choice of orientation can bemade, we call the manifold <strong>orientable</strong>. And we call suchatlas <span class="math inline">\(\mathcal{A}\)</span> an<strong>oriented atlas</strong> or <strong>orientation</strong> of themanifold <span class="math inline">\(M\)</span>.</p><h4 id="integration-on-manifolds">Integration on Manifolds</h4><p>Let <span class="math inline">\(M\)</span> be a smooth orientedmanifold of dimension <span class="math inline">\(n\)</span>. And let<span class="math inline">\((\rho, U)\)</span> be a smooth partition ofunity subordinate to an open cover <spanclass="math inline">\(\{U_i\}_{i \in I}\)</span> of <spanclass="math inline">\(M\)</span>, where <spanclass="math inline">\(\rho_i: M \to [0,1]\)</span> are smooth functionssuch that <span class="math inline">\(\sum_{i \in I} \rho_i = 1\)</span>on <span class="math inline">\(M\)</span> and <spanclass="math inline">\(\text{supp}(\rho_i) \subseteq U_i\)</span>. Theintegral of a differential <span class="math inline">\(n\)</span>-form<span class="math inline">\(\omega \in \Omega^n(M)\)</span> over themanifold <span class="math inline">\(M\)</span> is defined as: <spanclass="math display">\[\int_M \omega = \sum_{i \in I} \int_{U_i} \rho_i \omega\]</span> where <span class="math inline">\(\rho_i \omega\)</span> is ahighest degree form on <span class="math inline">\(U_i\)</span> and theintegral is taken over the open set <spanclass="math inline">\(U_i\)</span>.</p><p>Note <span class="math inline">\(\rho_i \omega\)</span> is compactlysupported in <span class="math inline">\(U_i\)</span> and is a smoothdifferential <span class="math inline">\(n\)</span>-form on <spanclass="math inline">\(U_i\)</span>. So the integral <spanclass="math inline">\(\int_{U_i} \rho_i \omega\)</span> is well-definedand finite. The integral <span class="math inline">\(\int_M\omega\)</span> is independent of the choice of partition of unity andthe open cover <span class="math inline">\(\{U_i\}_{i \inI}\)</span>.</p><h4 id="change-of-variables">Change of Variables</h4><p>Let <span class="math inline">\(\phi: M \to N\)</span> be a smoothmap between oriented manifolds <span class="math inline">\(M\)</span>and <span class="math inline">\(N\)</span> of dimension <spanclass="math inline">\(n\)</span>, with orientation <spanclass="math inline">\(\mathcal{A,B}\)</span> respectively. We say that<span class="math inline">\(\phi\)</span> is<strong>orientation-preserving</strong> if for every chart <spanclass="math inline">\((\psi, V)\in \mathcal{B}\)</span>, the chart <spanclass="math inline">\((\psi \circ \phi, \phi^{-1}(V))\)</span> isorientation-compactible with <spanclass="math inline">\(\mathcal{A}\)</span>. We say that <spanclass="math inline">\(\phi\)</span> is<strong>orientation-reversing</strong> if for every chart <spanclass="math inline">\((\psi, V)\in \mathcal{B}\)</span>, the chart <spanclass="math inline">\((\psi \circ \phi, \phi^{-1}(V))\)</span> is notorientation-compactible with <spanclass="math inline">\(\mathcal{A}\)</span>.</p><p>Actually, mapping <span class="math inline">\(\phi\)</span> is eitherorientation-preserving or orientation-reversing if <spanclass="math inline">\(M\)</span> and <spanclass="math inline">\(N\)</span> are connected. Otherwise, it eitherpreserves or reverses the orientation on each connected component of<span class="math inline">\(M\)</span> and <spanclass="math inline">\(N\)</span>.</p><p>The <strong>change of variables formula</strong> for the integral ofa differential <span class="math inline">\(n\)</span>-form <spanclass="math inline">\(\omega \in \Omega^n(M)\)</span> under the map<span class="math inline">\(\phi\)</span> is given by:</p><p>if <span class="math inline">\(\phi\)</span> is anorientation-preserving map, then: <span class="math display">\[\int_N \phi^* \omega = \int_M \omega\]</span> if <span class="math inline">\(\phi\)</span> is anorientation-reversing map, then: <span class="math display">\[\int_N \phi^* \omega = -\int_M \omega\]</span></p><p>The relation of the orientability and the differential forms is givenby the following theorem:</p><p><strong>Theorem</strong>: A smooth manifold <spanclass="math inline">\(M\)</span> is orientable if and only if thereexists a nowhere vanishing differential <spanclass="math inline">\(n\)</span>-form <span class="math inline">\(\omega\in \Omega^n(M)\)</span>.</p><p>And we call such <span class="math inline">\(n\)</span>-form <spanclass="math inline">\(\omega\)</span> a <strong>volume form</strong> on<span class="math inline">\(M\)</span>.</p><h3 id="stokes-theorem">Stokes Theorem</h3><h4 id="induced-orientation-on-the-boundary">Induced Orientation on theBoundary</h4><p>Let <span class="math inline">\(M^n\)</span> be an oriented smoothmanifold with boundary <span class="math inline">\(\partial M\)</span>.The boundary <span class="math inline">\(\partial M\)</span> is a smoothmanifold of dimension <span class="math inline">\(n-1\)</span>. And<span class="math inline">\(\mu\)</span> is a volume form on <spanclass="math inline">\(M\)</span> that defines the orientation of <spanclass="math inline">\(M\)</span>.</p><p>We can define an orientation on <span class="math inline">\(\partialM\)</span> induced by the orientation of <spanclass="math inline">\(M\)</span> as follows: At each point <spanclass="math inline">\(p \in \partial M\)</span>, we can choose a localchart <span class="math inline">\((U_{\alpha}, x^1, x^2, \ldots,x^n)\)</span> around <span class="math inline">\(p\)</span> such that<span class="math inline">\(x^n \geq 0\)</span> on <spanclass="math inline">\(U_{\alpha}\)</span> and <spanclass="math inline">\(x^n = 0\)</span> on <span class="math inline">\(U\cap \partial M\)</span>. The orientation on <spanclass="math inline">\(\partial M\)</span> is defined by the volume form:<span class="math display">\[ \eta_{\alpha}=\iota_{e_n} \mu =\iota_{-\frac{\partial}{\partial x^n}} \mu \]</span> where <spanclass="math inline">\(e_n = -\frac{\partial}{\partial x^n}\)</span> isthe outward-pointing normal vector field on <spanclass="math inline">\(\partial M\)</span>.</p><p>Or explicitly, <span class="math display">\[\eta_{\alpha} = (-1)^{n} f_{\alpha} dx^1 \wedge dx^2 \wedge \ldots\wedge dx^{n-1}\]</span> if <span class="math inline">\(\mu = f_{\alpha} dx^1 \wedgedx^2 \wedge \ldots \wedge dx^n\)</span> on <spanclass="math inline">\(U_{\alpha}\)</span>.</p><p>And <span class="math inline">\(\eta\)</span> gives a nowherevanishing <span class="math inline">\(n-1\)</span> form on <spanclass="math inline">\(\partial M\)</span>.</p><p>To integrate a <span class="math inline">\(n-1\)</span>-form <spanclass="math inline">\(\omega\)</span> on <spanclass="math inline">\(\partial M\)</span>, we locally write <spanclass="math display">\[\omega =  (-1)^n f\ dx^1\wedge \cdots \wedge dx^{n-1}\]</span> Then <span class="math display">\[\int_U \omega=\int_V f\ dx^1\cdots dx^{n-1}\]</span></p><p>The <strong>Stokes Theorem</strong> state that, for any <spanclass="math inline">\(\omega \in \Omega^{n-1}(M)\)</span> <spanclass="math display">\[\int_{\partial M} \iota_{\partial M}^* \omega = \int_{M} d\omega\]</span></p><p><strong>Proof</strong></p><p>If <span class="math inline">\(\omega\)</span> is compactly supportin coordinate chart <span class="math inline">\((U,x)\)</span></p><p><span class="math display">\[\omega = \sum_{i}(-1)^{i-1}f_idx^1\wedge\cdots\wedge\widehat{dx^i}\wedge\cdots\wedge dx^n\\\]</span> If <span class="math inline">\(U\simeq \mathbb R^{n}\)</span>,<span class="math inline">\(\omega\)</span> is <spanclass="math inline">\(0\)</span> on <span class="math inline">\(\partialM\)</span>, then: <span class="math display">\[\begin{align*}\int_U d\omega &amp;= \int_{\mathbb R^n}\sum_i \frac{\partialf_i}{\partial x^i} dx^1 \cdots dx^n\\&amp;= \sum_{i=1}^n \int_{\mathbbR^{n-1}}\left(\int_{-\infty}^{\infty}\frac{\partial f_i}{\partialx^i}dx^i\right) dx^1\cdots\widehat{dx^i}\cdots dx^n\\&amp;=0=\int_M \iota_{\partial M}^*\omega\end{align*}\]</span></p><p>If <span class="math inline">\(U\simeq \mathbb R^n_+\)</span>, then:<span class="math display">\[\begin{align*}\int_U d\omega &amp;= \int_{\mathbb R^n_+}\sum_i \frac{\partialf_i}{\partial x^i} dx^1\cdots  dx^n\\&amp;= \sum_{i=1}^{n-1} \int_{\mathbbR^{n-1}}\left(\int_{-\infty}^{\infty}\frac{\partial f_i}{\partialx^i}dx^i\right) dx^1\cdots\widehat{dx^i}\cdots dx^n + \int_{\mathbbR^{n-1}}\left(\int_{0}^{\infty}\frac{\partial f_n}{\partialx^n}dx^n\right) dx^1\cdots dx^{n-1}\\&amp;= \int_{\mathbb R^{n-1}} f_n( x^1,\cdots,x^{n-1},0) dx^1\cdotsdx^{n-1}\\&amp;=\int_{\partial M} \iota_{\partial M}^*\omega\end{align*}\]</span> The last step is because <spanclass="math inline">\(\iota_{\partial M}^*(dx^i)=dx^i\)</span> for <spanclass="math inline">\(i=1,\cdots,n-1\)</span> and <spanclass="math inline">\(\iota_{\partial M}^*(dx^n)=0\)</span>.</p><p>For general <span class="math inline">\(\omega\)</span>, we can usepartition of unity to prove it.</p><h2 id="de-rham-theory">de Rham Theory</h2><p>Let <span class="math inline">\(M\)</span> be a smooth manifold,<span class="math inline">\(\omega \in \Omega^k(M)\)</span> be a smoothdifferential <span class="math inline">\(k\)</span>-form. We call <spanclass="math inline">\(\omega\)</span> a <strong>closed form</strong> if<span class="math inline">\(d\omega = 0\)</span>, and we call <spanclass="math inline">\(\omega\)</span> an <strong>exact form</strong> ifthere exists a differential <spanclass="math inline">\((k-1)\)</span>-form <spanclass="math inline">\(\eta \in \Omega^{k-1}(M)\)</span> such that <spanclass="math inline">\(\omega = d\eta\)</span>.</p><p>We denote the space of closed <spanclass="math inline">\(k\)</span>-forms on <spanclass="math inline">\(M\)</span> as <span class="math inline">\(Z^k(M) =\{\omega \in \Omega^k(M) : d\omega = 0\}\)</span> and the space of exact<span class="math inline">\(k\)</span>-forms as <spanclass="math inline">\(B^k(M) = \{d\eta : \eta \in\Omega^{k-1}(M)\}\)</span>.</p><p>Let <span class="math inline">\(\dim M = n\)</span>, we have thefollowing result:</p><ul><li><span class="math inline">\(B^k(M) = Z^k(M) = \{0\}\)</span> if<span class="math inline">\(k &gt; n\)</span>.</li><li><span class="math inline">\(B^0(M) = \{0\}\)</span>, <spanclass="math inline">\(Z^0(M) = \{f\in C^{\infty}(M) | df=0\}\simeq\mathbb{R}^{K}\)</span>, where <span class="math inline">\(K\)</span> isthe number of connected components of <spanclass="math inline">\(M\)</span>.</li><li><span class="math inline">\(Z^n(M) = \Omega^n(M)\)</span></li></ul><p>By definition, <span class="math display">\[Z^k(M) = \ker (d: \Omega^k(M) \to \Omega^{k+1}(M))\]</span> <span class="math display">\[B^k(M) = \text{im} (d: \Omega^{k-1}(M) \to \Omega^k(M))\]</span></p><p>Since <span class="math inline">\(d^2 = 0\)</span>, we have thefollowing property: <span class="math display">\[B^k(M) \subseteq Z^k(M) \subseteq \Omega^k(M)\]</span></p><p>We define the <strong>de Rham cohomology</strong> of <spanclass="math inline">\(M\)</span> as: <span class="math display">\[H^k_{dR}(M) = Z^k(M) / B^k(M) \]</span> and <spanclass="math inline">\([\omega]\)</span> is the equivalence class of<span class="math inline">\(\omega \in Z^k(M)\)</span> in <spanclass="math inline">\(H^k_{dR}(M)\)</span>, called the <strong>de Rhamcohomology class</strong> of <spanclass="math inline">\(\omega\)</span>.</p><p><span class="math inline">\(H^k_{dR}(M)\)</span> is a vector spaceover <span class="math inline">\(\mathbb{R}\)</span> and we shall seethat it is linear with respect to the cup product.</p><p>Example:</p><p>For <span class="math inline">\(M = \mathbb{R}\)</span>, <spanclass="math inline">\(Z^0(\mathbb{R}) = \Omega^0(\mathbb{R}) = \mathbbR\)</span>, <span class="math inline">\(B^0(\mathbb{R}) =\{0\}\)</span>, hence <span class="math inline">\(H^0_{dR}(\mathbb{R})\simeq \mathbb{R}\)</span>, and <spanclass="math inline">\(H^k_{dR}(\mathbb{R}) \simeq \{0\}\)</span> for<span class="math inline">\(k &gt; 0\)</span>.</p><p>For <span class="math inline">\(M = \mathbb{S^1}\)</span>, we know<span class="math inline">\(H^0_{dR}(\mathbb{S^1}) \simeq\mathbb{R}\)</span> and <spanclass="math inline">\(H^k_{dR}(\mathbb{S^1}) \simeq \{0\}\)</span> for<span class="math inline">\(k &gt; 1\)</span>. So we only need tocompute <span class="math inline">\(H^1_{dR}(\mathbb{S^1})\)</span>.</p><p><span class="math display">\[\begin{align*}Z^1(\mathbb{S^1}) = \Omega^1(\mathbb{S^1}) &amp;= \{fd\theta|f\inC^{\infty}(\mathbb{S^1})\}\\&amp; = \{fd\theta | f\in C^{\infty}(\mathbb R), f \text{ periodic withperiod } 2\pi\}\\\end{align*}\]</span></p><p>Now, consider <span class="math inline">\(\omega \inB^1(\mathbb{S^1})\)</span>, we have <span class="math inline">\(\omega =d\eta\)</span> for some <span class="math inline">\(\eta \in\Omega^0(\mathbb{S^1})\)</span>. Since <spanclass="math inline">\(\eta\)</span> is a smooth function on <spanclass="math inline">\(\mathbb{S^1}\)</span>, it can be expressed as<span class="math inline">\(f(\theta)\)</span> for some periodicfunction <span class="math inline">\(f\)</span> with period <spanclass="math inline">\(2\pi\)</span>. Thus, we have: <spanclass="math display">\[\omega = d\eta = df(\theta) = g(\theta)d\theta\]</span> where <span class="math inline">\(g\)</span> is a periodicfunction with period <span class="math inline">\(2\pi\)</span> and <spanclass="math inline">\(\int_0^{2\pi} g(\theta) d\theta = 0\)</span>.Thus, <span class="math inline">\(H^1_{dR}(\mathbb{S^1}) \simeq\mathbb{R}\)</span>, which is the first de Rham cohomology group of thecircle since the cohomology class is represented by the integral valueof the 1-form over the circle.</p><p>Let <span class="math inline">\(\dim M = n\)</span>, we have thefollowing results: - <span class="math inline">\(H^k_{dR}(M) =\{0\}\)</span> for <span class="math inline">\(k &gt; n\)</span>. -<span class="math inline">\(H^0_{dR}(M) \simeq \mathbb{R}^{K}\)</span>,where <span class="math inline">\(K\)</span> is the number of connectedcomponents of <span class="math inline">\(M\)</span>.</p><h3 id="betti-numbers-and-euler-characteristic">Betti Numbers and EulerCharacteristic</h3><p>If <span class="math inline">\(\dim H^k_{dR}(M) &lt; \infty\)</span>for any <span class="math inline">\(k\)</span>, we define the<strong>Betti number</strong> <span class="math display">\[b_k(M) = \dimH^k_{dR}(M)\]</span> which is called the <spanclass="math inline">\(k\)</span>-th Betti number of the manifold <spanclass="math inline">\(M\)</span>.</p><p>The <strong>Euler characteristic</strong> of <spanclass="math inline">\(M\)</span> is defined as: <spanclass="math display">\[ \chi(M) = \sum_{k=0}^{\dim M} (-1)^k b_k(M)\]</span></p><p>From the above result, we know <spanclass="math inline">\(\chi(\mathbb R) = 1 - 0 = 1\)</span>, and <spanclass="math inline">\(\chi(\mathbb{S^1}) = 1 - 1 = 0\)</span>.</p><h3 id="pullback-1">Pullback</h3><p>Let <span class="math inline">\(F: M \to N\)</span> be a smooth mapbetween manifolds, the pullback of the de Rham cohomology is defined as:<span class="math display">\[ F^*: H^k_{dR}(N) \to H^k_{dR}(M) \]</span>by <span class="math inline">\(F^*([\omega]) = [F^*\omega]\)</span> for<span class="math inline">\(\omega \in Z^k(N)\)</span>.</p><p>Since <span class="math inline">\(dF^*= F^*d\)</span>, <spanclass="math inline">\(F^*\)</span> maps closed forms to closed forms andexact forms to exact forms. <span class="math display">\[ F^*(B^k(N))\subseteq B^k(M) \quad \text{and} \quad F^*(Z^k(N)) \subseteq Z^k(M)\]</span></p><p>Hence <span class="math inline">\(F^*\)</span> induces a well-definedmap on cohomology.</p><p>Note that the pullback of de Rham cohomology satisfies thecontravariant functoriality: 1. <span class="math inline">\((F \circG)^* = G^* \circ F^*\)</span> for smooth maps <spanclass="math inline">\(G: L \to M\)</span> and <spanclass="math inline">\(F: M \to N\)</span>. 2. <spanclass="math inline">\((\text{id}_M)^* =\text{id}_{H^*_{dR}(M)}\)</span>.</p><p>Thus, the de Rham cohomology is a contravariant functor from thecategory of smooth manifolds to the category of graded algebras over afield. So a diffeomorphism <span class="math inline">\(F: M \toN\)</span> induces an isomorphism <span class="math inline">\(F^*:H^*_{dR}(N) \to H^*_{dR}(M)\)</span>.</p><p>Particularly, <span class="math inline">\(b_k(M)\)</span> and <spanclass="math inline">\(\chi(M)\)</span> are diffeomorphisminvariants.</p><h3 id="cup-product">Cup Product</h3><p>Note that the wedge product <spanclass="math inline">\(\wedge\)</span> on differential forms induces aoperation on de Rham cohomology, called the <strong>cupproduct</strong>. <span class="math display">\[ \cup: H^k_{dR}(M) \timesH^l_{dR}(M) \to H^{k+l}_{dR}(M) \]</span> defined by <spanclass="math display">\[ [\omega_1] \cup [\omega_2] = [\omega_1 \wedge\omega_2] \]</span> for <span class="math inline">\(\omega_1 \inZ^k(M)\)</span> and <span class="math inline">\(\omega_2 \inZ^l(M)\)</span>.</p><p>Assume <span class="math inline">\(\omega_1 = d\eta_1\)</span> and<span class="math inline">\(\omega_2 = d\eta_2\)</span>, <spanclass="math inline">\(\omega,\zeta\)</span> are closed forms, to provecup product is well-defined, we calculate <span class="math display">\[\begin{align*}(\omega + d\eta_1) \wedge (\zeta + d\eta_2) &amp;= \omega \wedge \zeta +\omega \wedge d\eta_2 + d\eta_1 \wedge \zeta + d\eta_1 \wedge d\eta_2\\&amp;= \omega \wedge \zeta + \omega \wedge d\eta_2 + (-1)^{k} d\omega\wedge \eta_2 - (-1)^{k} d\omega \wedge d\eta_2 \\&amp;+ d\eta_1 \wedge \zeta + d\eta_1 \wedge d\eta_2\\&amp;= \omega \wedge \zeta + d((-1)^{k} \omega \wedge \eta_2) + d\eta_1\wedge \zeta + d\eta_1 \wedge d\eta_2 \\&amp;= \omega \wedge \zeta + d((-1)^{k} \omega \wedge \eta_2) + d(\eta_1\wedge \zeta) + d(\eta_1 \wedge d\eta_2)\\&amp;= \omega \wedge \zeta + d\left ((-1)^{k} \omega \wedge \eta_2 +\eta_1 \wedge \zeta + \eta_1 \wedge d\eta_2\right)\end{align*}\]</span> Therefore, we have <span class="math display">\[[\omega + d\eta_1] \cup [\zeta + d\eta_2]  = [\omega \wedge \zeta]\]</span> so the cup product is well-defined.</p><p>The cup product is bilinear, associative, and commutative up to asign: <span class="math display">\[ [\omega_1] \cup [\omega_2] =[\omega_1 \wedge \omega_2] = [(-1)^{kl} \omega_2 \wedge \omega_1] =(-1)^{kl} [\omega_2] \cup [\omega_1] \]</span></p><p>The cup product induces a graded algebra structure on the de Rhamcohomology: <span class="math display">\[ H^*_{dR}(M) =\bigoplus_{k=0}^{\dim M} H^k_{dR}(M) \]</span> with the cup product<span class="math inline">\(\cup\)</span> as the multiplicationoperation.</p><p>And <span class="math inline">\((H^*_{dR}(M), +, \cup)\)</span>becomes a graded commutative ring with identity, called the <strong>deRham cohomology ring</strong> of <spanclass="math inline">\(M\)</span>.</p><h3 id="homotopy-invariance">Homotopy Invariance</h3><p>We say <span class="math inline">\(M\)</span> and <spanclass="math inline">\(N\)</span> are homotopy equivalent if there existcontinuous maps <span class="math inline">\(f: M \to N\)</span> and<span class="math inline">\(g: N \to M\)</span> such that <spanclass="math inline">\(g \circ f\)</span> is homotopic to the identitymap on <span class="math inline">\(M\)</span> and <spanclass="math inline">\(f \circ g\)</span> is homotopic to the identitymap on <span class="math inline">\(N\)</span>.</p><p>If <span class="math inline">\(M\)</span> and <spanclass="math inline">\(N\)</span> are homotopy equivalent, then <spanclass="math inline">\(H^k_{dR}(M) \simeq H^k_{dR}(N)\)</span> for all<span class="math inline">\(k\)</span>. This is stronger than the factthat <span class="math inline">\(H^k_{dR}(M)\)</span> is adiffeomorphism invariant, as homotopy equivalence does not even requirethe manifolds to have the same dimension, e.g. <spanclass="math inline">\(\mathbb R^n\setminus \{0\}\)</span> and <spanclass="math inline">\(\mathbb S^{n-1}\)</span>.</p><p>Note: This shows that the de Rham cohomology is determined by thetopological structure of the manifold, not relevant to the smoothstructure.</p><h4 id="poincaré-lemma">Poincaré Lemma</h4><p>The <strong>Poincaré Lemma</strong> states that if <spanclass="math inline">\(U\)</span> is a star-shape area in <spanclass="math inline">\(\mathbb R^n\)</span>, then <spanclass="math inline">\(H^k_{dR}(U)=0\)</span> for <spanclass="math inline">\(k&gt;1\)</span>. Particularly, <spanclass="math inline">\(H^k_{DR}(\mathbb R^n) = 0\)</span>.</p><p>This is trivial since the star shape area can always contract to apoint.</p><p>Moreover, since for any point in a manifold has a neighborhood thatis diffeomorphic to a star shape area in <spanclass="math inline">\(\mathbb R^n\)</span>, we have the followingcollary:</p><p>For any <span class="math inline">\(k\)</span>-th closed form <spanclass="math inline">\(\omega \in Z^k(M)\)</span> and any <spanclass="math inline">\(p\in M\)</span>, there is a neighborhood <spanclass="math inline">\(U\ni p\)</span> and <spanclass="math inline">\((k-1)\)</span>-form <spanclass="math inline">\(\eta\in \Omega^k(U)\)</span> such that <spanclass="math inline">\(\omega = d \eta\)</span> on <spanclass="math inline">\(U\)</span>.</p><h4 id="proof-of-homotopy-invariance">Proof of Homotopy Invariance</h4><p>It suffices to prove that functor <spanclass="math inline">\(f\rightsquigarrow  f^*\)</span> is homotopicinvariant:</p><blockquote><p>If two smooth maps <span class="math inline">\(f,g: M \to N\)</span>are homotopic, then <span class="math inline">\(f^* = g^*: H^k_{dR}(N)\to H^k_{dR}(M)\)</span>.</p></blockquote><p>Because assume this holds, if <span class="math inline">\(M\)</span>and <span class="math inline">\(N\)</span> are homotopy equivalent, wecan use smooth approximations of the continuous maps to get smooth maps<span class="math inline">\(f: M \to N\)</span> and <spanclass="math inline">\(g: N \to M\)</span> such that <spanclass="math inline">\(g \circ f\)</span> and <spanclass="math inline">\(f \circ g\)</span> are homotopic to the identitymaps on <span class="math inline">\(M\)</span> and <spanclass="math inline">\(N\)</span>, respectively. Then we have <spanclass="math display">\[ (g \circ f)^* = f^* \circ g^* =\text{id}_{H^*_{dR}(M)} \]</span> <span class="math display">\[ (f \circg)^* = g^* \circ f^* = \text{id}_{H^*_{dR}(N)} \]</span> which impliesthat <span class="math inline">\(f^*\)</span> and <spanclass="math inline">\(g^*\)</span> are isomorphisms.</p><p>Now we prove the claim that if <span class="math inline">\(f,g: M \toN\)</span> are homotopic, then <span class="math inline">\(f^* =g^*\)</span>.</p><p>We define the <strong>cochain homotopy</strong> as follows: &gt; if<span class="math inline">\(f,g\in C^{\infty}(M,N)\)</span> arehomotopic. If there is a sequence of mapping <spanclass="math inline">\(h_k:\Omega^k(M) \to \Omega^{k-1}(M)\)</span>satisfying: <span class="math display">\[ g^*-f^* =d_{M}h_k+h_{k+1}d_{N}.\]</span> we say the sequence <spanclass="math inline">\(h=(h_k)\)</span> is a cochain homotopy between<span class="math inline">\(f^*\)</span> and <spanclass="math inline">\(g^*\)</span>.</p><p>If there exists such a cochain homotopy <spanclass="math inline">\(h\)</span>, for any <spanclass="math inline">\(\omega \in Z^k_{dR}(N)\)</span></p><p><span class="math display">\[g^*\omega - f^* \omega = (dh_{k}+h_{k+1}d)\omega = d(h_k\omega)+h_{k+1}(d\omega) = d(h_k\omega) \in B^k(M)\]</span></p><p>So <span class="math inline">\(f^*([\omega])=[f^*\omega] = [g^*\omega] = g^*([\omega])\)</span>.</p><p>Now we prove the existence of the cochain homotopy:</p><p>First, we prove a lemma</p><hr /><p>Let <span class="math inline">\(X\)</span> be a complete vector fieldon <span class="math inline">\(M\)</span>, <spanclass="math inline">\(\phi_t\)</span> be the flow generated by <spanclass="math inline">\(X\)</span>. Then there exists a linear operator<span class="math inline">\(Q_k:\Omega^k(M)\to\Omega^{k-1}(M)\)</span>s.t. : <span class="math display">\[\phi_1^*\omega-\omega= d Q_k(\omega)+ Q_{k+1}(d\omega)\]</span> <strong>Proof</strong> <span class="math display">\[\frac{d}{dt}\phi^*_t\omega=\left.\frac{d}{ds}\right|_{s=0}\phi^*_{s+t}=\left.\frac{d}{ds}\right|_{s=0}\phi^*_{s}\phi^*_{t}\omega=\mathcal{L}_X(\phi_t^*\omega)=d\iota_X(\phi^*_t\omega)+\iota_Xd(\phi^*_t\omega)\]</span> Therefore, denote <spanclass="math inline">\(Q_k(\omega)=\int_{0}^{1}\iota_X(\phi_t^*\omega)dt\)</span><span class="math display">\[\phi_1^*\omega-\omega = \int_{0}^{1}\left(\frac{d}{dt}\phi^*_t\omega\right ) = dQ_k(\omega) + Q_{k+1}(d\omega)\]</span></p><hr /><p>Now we can construct the cochain homotopy between <spanclass="math inline">\(f^*\)</span> and <spanclass="math inline">\(g^*\)</span> as follows: Let <spanclass="math inline">\(W=M\times R\)</span>, then <spanclass="math inline">\(X=\frac{\partial}{\partial t}\)</span> is acomplete vector field on <span class="math inline">\(W\)</span>. Let<span class="math inline">\(\phi_t: W \to W\)</span> be the flowgenerated by <span class="math inline">\(X\)</span>, then <spanclass="math inline">\(\phi_t(x,s)=(x,s+t)\)</span> is a smooth map.</p><p>By the Lemma, we have a linear operator <spanclass="math inline">\(Q_k: \Omega^k(W) \to \Omega^{k-1}(W)\)</span> suchthat: <span class="math display">\[\phi_1^*\omega - \omega = d Q_k(\omega) + Q_{k+1}(d\omega)\]</span></p><p>By the Whitney Approximation theorem, we can find a smooth homotopy<span class="math inline">\(H: M \times [0,1] \to N\)</span> such that<span class="math inline">\(H(x,0) = f(x)\)</span> and <spanclass="math inline">\(H(x,1) = g(x)\)</span> for all <spanclass="math inline">\(x \in M\)</span>.</p><p>Let <span class="math inline">\(\iota:M \hookrightarrow W\)</span> bethe inclusion map, <span class="math inline">\(\iota(x) =(x,0)\)</span>, then <span class="math display">\[f = H \circ \iota, \quad g = H \circ \phi_1 \circ \iota\]</span> Then we have: <span class="math display">\[g^*\omega - f^*\omega = \iota^* \phi_1^* H^* \omega - \iota^* H^* \omega= \iota^* (dQ_k+Q_{k+1}d)H^* \omega = (d \iota^* Q_k H^* + \iota^*Q_{k+1} H^* d)\omega\]</span></p><p>So <span class="math inline">\(h_k = \iota^* Q_k H^*\)</span> and<span class="math inline">\(h_{k+1} = \iota^* Q_{k+1} H^*\)</span> is acochain homotopy between <span class="math inline">\(f^*\)</span> and<span class="math inline">\(g^*\)</span>.</p><h3 id="de-rham-theorem">de Rham Theorem</h3><p>The famous <strong>de Rham theorem</strong> states that the de Rhamcohomology is isomorphic to the singular cohomology with realcoefficients, i.e., for any smooth manifold <spanclass="math inline">\(M\)</span>, <span class="math display">\[H^k_{dR}(M) \simeq H^k_{sing}(M; \mathbb{R}) \]</span></p><p>which we shall not prove here.</p><p>This theorem reveals the duality between the topological structureand the algebraic structure (differential forms) on a manifold.</p><h3 id="chain-complex">Chain Complex</h3><p>The <strong>chain complex</strong> is a sequence of abelian groups(or modules) <span class="math inline">\(\cdots A_0, A_1,A_2\cdots\)</span> connected by homomorphisms <spanclass="math inline">\(d_n: A_n\to A_{n-1}\)</span>, such that the imageof one homomorphism is contained in the kernel of the next. Thecomposition of any two consecutive maps shall be the zero maps, <spanclass="math inline">\(d_n \circ d_{n+1} = 0\)</span> or <spanclass="math inline">\(d^2=0\)</span> for short. <spanclass="math display">\[ \cdots \xleftarrow{d_0} A_0 \xleftarrow{d_1} A_1\xleftarrow{d_2} A_2 \xleftarrow{d_3} \cdots\]</span></p><p>The <strong>cochain complex</strong> is the dual notion to the chaincomplex.</p><p><span class="math display">\[ \cdots \xrightarrow{d^0} A^0\xrightarrow{d^1} A^1 \xrightarrow{d^2} A^2 \xrightarrow{d^3}\cdots\]</span> where <span class="math inline">\(d^{n}\circ d^{n+1} =0\)</span>.</p><p>The elements in the kernel of <span class="math inline">\(d\)</span>are called <strong>(co)cycles</strong> (or <strong>closed</strong>elements), and the elements in the image of <spanclass="math inline">\(d\)</span> are called<strong>(co)boundaries</strong> (or <strong>exact</strong> elements).Right from the definition of the differential, all boundaries arecycles. The <span class="math inline">\(n\)</span>-th<strong>(co)homology</strong> group <spanclass="math inline">\(H_n(H^n)\)</span> is the group of (co)cyclesmodulo (co)boundaries in degree <span class="math inline">\(n\)</span>,that is,</p><p><span class="math display">\[ H_n = \ker d_n/\text{im } d_{n+1} \left( H^n = \ker d^n/\text{im } d^{n-1}\right)\]</span></p><p>A <strong>exact sequence</strong> is a (co)chain complex whose(co)homology groups are all zero, which means all closed elements areexact. A short exact sequence is a bounded exact sequence in which onlythe groups <span class="math inline">\(A_k\)</span>, <spanclass="math inline">\(A_{k+1}\)</span>, <spanclass="math inline">\(A_{k+2}\)</span> may be nonzero. For example, thefollowing chain complex is a short exact sequence.</p><p><span class="math display">\[ \cdots \rightarrow 0 \rightarrow Z\xrightarrow {\times p} Z \twoheadrightarrow Z/p \rightarrow 0\rightarrow \cdots \]</span></p><h3 id="de-rham-complex">de Rham Complex</h3><p>The <strong>de Rham complex</strong> is the sequence of differentialforms: <span class="math display">\[ 0 \to \Omega^0(M) \xrightarrow{d}\Omega^1(M) \xrightarrow{d} \Omega^2(M) \xrightarrow{d} \ldots\xrightarrow{d} \Omega^n(M) \to 0 \]</span></p><h4 id="the-zig-zag-lemma">The Zig-Zag Lemma</h4><p>TODO.</p><h3 id="mayer-vietoris-sequence">Mayer-Vietoris Sequence</h3><p>TODO.</p><h2 id="poincaré-duality">Poincaré Duality</h2><h3 id="singular-homology">Singular Homology</h3><p>TODO.</p><h3 id="poincaré-duality-theorem">Poincaré Duality Theorem</h3><p>TODO.</p><h2 id="hodge-theory">Hodge Theory</h2><p>TODO.</p><h2 id="sheaf-theory">Sheaf Theory</h2><p>TODO.</p>]]></content>
    
    
    <categories>
      
      <category>Mathematics</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Differential Geometry</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>GAN and Wasserstein GAN</title>
    <link href="/2025/07/22/GAN-and-Wasserstein-GAN/"/>
    <url>/2025/07/22/GAN-and-Wasserstein-GAN/</url>
    
    <content type="html"><![CDATA[<h1 id="introduction">Introduction</h1><p>Generative Adversarial Networks is invented in 2014 by IanJ.Goodfellow, et. al. as a generative models, its idea derives from thegame theory where two player compete against one another.</p><p>So they design an architecture that has two network where one isresponible for generating fake data that is similar from the data andanother one is responible for discriminating between fake and truedata.</p><h1 id="mathematics-formulas">Mathematics Formulas</h1><p>GAN is formulated as a min-max problem</p><p><span class="math display">\[\min_{G} \max_{D} V(D,G)\]</span> <span class="math display">\[V(D,G) = \mathbb{E}_{x \sim p_{data}(x)}[\log D(x)] + \mathbb{E}_{x \simp_{g}(x)}[\log(1 - D(x))]\]</span></p><p>Where <span class="math inline">\(D\)</span> is the discriminator,<span class="math inline">\(G\)</span> is the generator, <spanclass="math inline">\(p_{data}(x)\)</span> is the distribution of thereal data, and <span class="math inline">\(p_{g}(x)\)</span> is thedistribution of the noise input to the generator.</p><p><span class="math inline">\(V(D,G)\)</span> can be interpreted as thevalue function that represents the “distance” between the real datadistribution and the generated data distribution.</p><h1 id="network-architecture">Network Architecture</h1><p>The architecture of GAN consists of two main components:</p><ol type="1"><li><strong>Generator (G)</strong>:</li></ol><ul><li>Noise input <span class="math inline">\(z\)</span> is sampled from aprior distribution (e.g., Gaussian or uniform).</li><li>Hidden layers transform the noise into a data sample.</li><li>Output layer generates a data sample (e.g., image).</li></ul><ol start="2" type="1"><li><strong>Discriminator (D)</strong>:</li></ol><ul><li>Takes a data sample (either real or generated).</li><li>Hidden layers process the sample.</li><li>Output layer produces a probability score indicating whether thesample is real or fake.</li></ul><h1 id="training-process">Training Process</h1><p>The training process of GAN involves alternating between training thediscriminator and the generator: 1. <strong>TrainDiscriminator</strong>: - Sample a batch of real data from the datasetwith label 1. - Sample a batch of noise and generate fake data using thegenerator with label 0. - Update the discriminator to maximize itsability to distinguish real from fake data. 2. <strong>TrainGenerator</strong>: - Sample a batch of noise and generate fake datausing the generator. - Update the generator to minimize thediscriminator’s ability to distinguish fake data from real data. 3.<strong>Repeat</strong>: - Continue alternating between training thediscriminator and the generator until convergence or for a fixed numberof epochs</p><h1 id="merits-and-limitations">Merits and Limitations</h1><h2 id="merits">Merits</h2><ul><li><strong>High-Quality Samples</strong>: GANs can generatehigh-quality samples that are often indistinguishable from realdata.</li><li><strong>Versatile Applications</strong>: GANs can be applied tovarious domains, including image generation</li><li><strong>Unsupervised Learning</strong>: GANs can learn fromunlabeled data, making them suitable for unsupervised learningtasks.</li></ul><h2 id="limitations">Limitations</h2><ul><li><strong>Training Instability</strong>: GANs can be difficult totrain due to issues like mode collapse, vanishing gradients, andoscillations.</li><li><strong>Sensitive to Hyperparameters</strong>: GANs require carefultuning of hyperparameters, such as learning rates and batch sizes, toachieve stable training.</li><li><strong>Lack of Diversity</strong>: GANs may generate samples thatlack diversity, especially if the training data is limited orbiased.</li></ul><h1 id="analysis">Analysis</h1><h2 id="mode-collapse">Mode Collapse</h2><p>Mode collapse is a common issue in GAN training where the generatorproduces a limited variety of outputs, often focusing on a few modes ofthe data distribution. This can lead to a lack of diversity in generatedsamples.</p><p>This is because the manifold of data is not necessarily connected(MNIST), but the transformation from the noise space to the data spaceis continuous. So it either gives up generating some data or generatessome data that is not actually in the data manifold. (In the vacantmiddle of two number manifold).</p><p>See <a href="https://arxiv.org/abs/2001.03698">AE-OT-GAN</a>.</p><h2 id="training-difficulties">Training Difficulties</h2><p>Training GANs can be challenging due to the adversarial nature of thetraining process. The more powerful the discriminator becomes, theharder it is for the generator to produce samples that fool thediscriminator.</p><p>This is because the loss function has the problem of vanishinggradients when the discriminator is too good at its job.</p><p>Fix <span class="math inline">\(G\)</span>, the optimal <spanclass="math inline">\(D\)</span> is given by: <spanclass="math display">\[D^*(x)=\frac{p_{data}(x)}{p_{data}(x)+p_{g}(x)}\]</span></p><p>Now we observe the loss of <span class="math inline">\(G\)</span> hasbecome: <span class="math display">\[2JS(p_{data}||p_{g})-2\log 2\]</span></p><p>But when the support of <span class="math inline">\(p_{data}\)</span>and <span class="math inline">\(p_{g}\)</span> has no intersection, theJS loss shall be constant. So the network cannot be trained since thereis no meaningful information can be acquired.</p><h1 id="wasserstein-gan-wgan">Wasserstein GAN (WGAN)</h1><p>Wasserstein GAN (WGAN) is a variant of GAN that addresses some of thelimitations of the original GAN formulation, particularly the issuesrelated to training stability and mode collapse. WGAN uses theWasserstein distance (also known as Earth Mover’s Distance) to measurethe distance between the real data distribution and the generated datadistribution.</p><p>See this article for more about Wasserstein distance: <ahref="https://notdesigned.github.io/2025/07/16/Wasserstein-Distance-and-Optimal-Transport/">WassersteinDistance and Optimal Transport</a></p><p>The important properties of Wasserstein distance are: 1.<strong>Continuity</strong>: The Wasserstein distance is continuous withrespect to the distributions. 2. <strong>Metric</strong>: TheWasserstein distance satisfies the triangle inequality and is a truemetric. 3. <strong>Independence of Support</strong>: When the supportsof two distributions have no overlap, Wasserstein distance stillprovides a suitable gradient for the generator to update.</p><h2 id="formulation">Formulation</h2><p>The original Wasserstein is difficult to compute, so WGAN uses asimplified version of the 1-Wasserstein distance, which can be computedusing the Kantorovich-Rubinstein duality:</p><p><span class="math display">\[W(p_{data}, p_{g}) = \sup_{\|f\|_L \leq 1} \mathbb{E}_{x \simp_{data}}[f(x)] - \mathbb{E}_{x \sim p_{g}}[f(x)]\]</span> Where <span class="math inline">\(f\)</span> is a 1-Lipschitzfunction, <span class="math inline">\(\|f\|_L \leq 1\)</span>.</p><p>Actually if <span class="math inline">\(f\)</span> is a K-Lipschitzfunction, just divide the result by <spanclass="math inline">\(K\)</span>. So long as the function is finiteLipschitz, it can be used to estimate the Wasserstein distance.</p><h2 id="modification-of-gan">Modification of GAN</h2><p>The loss function for the discriminator (critic) is modified to usethe Wasserstein distance instead of the original GAN loss. So theobjective becomes: <span class="math display">\[\max_{D} L = \mathbb{E}_{x \sim p_{data}}[D(x)] - \mathbb{E}_{x \simp_{g}}[D(x)]\]</span> The discriminator is now no longer trying to outputprobabilities, but rather to output a real-valued score that reflectsthe “realness” of the input data.</p><p>To ensure that <span class="math inline">\(D\)</span> is aK-Lipschitz function, WGAN applies weight clipping to thediscriminator’s weights. This means that the weights of thediscriminator are constrained to lie within a certain range (e.g.,[-0.01, 0.01]). So it must be limited to a finite Lipschitzconstant.</p><p>For the generator, since the first term is not dependent on <spanclass="math inline">\(G\)</span>, the objective becomes: <spanclass="math display">\[\min_{G} L = -\mathbb{E}_{x \sim p_{g}}[D(x)]\]</span></p>]]></content>
    
    
    <categories>
      
      <category>Machine Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Generative Models</tag>
      
      <tag>GAN</tag>
      
      <tag>Wasserstein Distance</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Wasserstein Distance and Optimal Transport</title>
    <link href="/2025/07/16/Wasserstein-Distance-and-Optimal-Transport/"/>
    <url>/2025/07/16/Wasserstein-Distance-and-Optimal-Transport/</url>
    
    <content type="html"><![CDATA[<h1 id="optimal-transport-problem">Optimal Transport Problem</h1><h2 id="definition">Definition</h2><p>Given two probability measures <spanclass="math inline">\(\mu\)</span> and <spanclass="math inline">\(\nu\)</span> on measurable spaces <spanclass="math inline">\((X, \mathcal{A})\)</span> and <spanclass="math inline">\((Y, \mathcal{B})\)</span>, respectively, the<strong>Optimal Transport Problem</strong> seeks to find a transportplan <span class="math inline">\(\pi\)</span> that minimizes the cost oftransporting mass from <span class="math inline">\(\mu\)</span> to <spanclass="math inline">\(\nu\)</span>. The cost function is typicallydefined as a function <span class="math inline">\(c: X \times Y \to\mathbb{R}\)</span>, which quantifies the cost of moving a unit massfrom point <span class="math inline">\(x \in X\)</span> to point <spanclass="math inline">\(y \in Y\)</span>.</p><p>The problem can be formulated as follows (Monge’s formulation): <spanclass="math display">\[\begin{aligned}\inf_{T} \int_{X} c(x, T(x)) \, d\mu(x)\end{aligned}\]</span> where <span class="math inline">\(T: X \to Y\)</span> is atransport map that pushes <span class="math inline">\(\mu\)</span>forward to <span class="math inline">\(\nu\)</span>, i.e., <spanclass="math inline">\(T_* \mu = \nu\)</span>.</p><p>Such a transport map <span class="math inline">\(T\)</span> is calledan <strong>optimal transport map</strong> if it minimizes the costfunction, although it is not always guaranteed to exist since thepushforward is not necessarily possible.</p><p>More generally, we can express the problem as (Kantorovich’sformulation): <span class="math display">\[\begin{aligned}\inf_{\pi \in \Pi(\mu, \nu)} \int_{X \times Y} c(x, y) \, d\pi(x, y)\end{aligned}\]</span> where <span class="math inline">\(\Pi(\mu, \nu)\)</span> isthe set of all joint distributions of <spanclass="math inline">\(\mu\)</span> and <spanclass="math inline">\(\nu\)</span>.</p><h2 id="dual-formulation">Dual Formulation</h2><p>The primal problem is a convex optimization problem since it islinear w.r.t. <span class="math inline">\(\pi\)</span> and the feasibleset is convex.</p><p>The constraint is that <span class="math inline">\(\pi\)</span> mustbe a joint distribution of <span class="math inline">\(\mu\)</span> and<span class="math inline">\(\nu\)</span>, which can be expressed as:<span class="math display">\[\begin{aligned}\int_{X \times Y} f(x) \, d\gamma(x, y) &amp;= \int_X f(x) \, d\mu(x),\quad \forall f \in C(X)\\\int_{X \times Y} g(y) \, d\gamma(x, y) &amp;= \int_Y g(y) \, d\nu(y),\quad \forall g \in C(Y)\end{aligned}\]</span> where <span class="math inline">\(C(X)\)</span> and <spanclass="math inline">\(C(Y)\)</span> are the spaces of continuousfunctions on <span class="math inline">\(X\)</span> and <spanclass="math inline">\(Y\)</span>, respectively.</p><p>This form allows us to derive the dual formulation of the optimaltransport problem, whose Lagrangian is given by: <spanclass="math display">\[\begin{aligned}L(\pi, \lambda, \mu) &amp;= \int_{X \times Y} c(x,y) d\pi + \int_{X}\lambda(x) ( d\mu -\operatorname{Proj}_X(d\pi)) + \int_{Y} \mu(y) (d\nu- \operatorname{Proj}_Y(d\pi)) \\&amp;= \int_{X \times Y} \left ( c(x,y) - \lambda(x) - \mu(y) \right )d\pi + \int_{X} \lambda(x) d\mu + \int_{Y} \mu(y) d\nu\end{aligned}\]</span></p><p>Thus the dual problem can be expressed as: <spanclass="math display">\[\begin{aligned}\sup_{\lambda, \mu} \inf_{\pi \in \Pi(\mu, \nu)} L(\pi, \lambda, \mu)\end{aligned}\]</span></p><p>For any point <span class="math inline">\((x,y)\)</span>, if <spanclass="math inline">\(c(x,y) - \lambda(x) - \mu(y) &lt; 0\)</span>, thenthe value of <span class="math inline">\(L(\pi, \lambda, \mu)\)</span>can be made arbitrarily to <span class="math inline">\(-\infty\)</span>by increasing <span class="math inline">\(\pi\)</span>.</p><p>Therefore, the optimal solution must satisfy: <spanclass="math display">\[c(x,y) \geq \lambda(x) + \mu(y), \quad \forall (x,y) \in X \times Y\]</span></p><p>And the dual problem now becomes: <span class="math display">\[\begin{aligned}\sup_{\lambda, \mu} \left \{ \int_{X} \lambda(x) d\mu + \int_{Y} \mu(y)d\nu : c(x,y) \geq \lambda(x) + \mu(y), \forall (x,y) \in X \times Y\right \}\end{aligned}\]</span></p><p>Or more compactly: <span class="math display">\[\begin{aligned}\sup_{\lambda, \mu} \left \{ \int_{X} \lambda(x) d\mu + \int_{Y} \mu(y)d\nu : \lambda \oplus \mu \leq c \right \}\end{aligned}\]</span></p><p>The strong duality holds under mild conditions, such <spanclass="math inline">\(X\)</span> and <spanclass="math inline">\(Y\)</span> are compact and <spanclass="math inline">\(c\)</span> is continuous. More generally, <spanclass="math inline">\(c\)</span> can be a lower semicontinuous functionand exist a feasible <span class="math inline">\(\pi\)</span> withfinite cost.</p><h2 id="wasserstein-distance">Wasserstein Distance</h2><p>When <span class="math inline">\(X = Y, c(x,y) = d(x,y)\)</span> is ametric, the optimal transport problem induces the <strong>Wassersteindistance</strong>: <span class="math display">\[\begin{aligned}W_p(\mu, \nu) = \inf_{\pi \in \Pi(\mu, \nu)} \left( \int_{X \times Y}d(x,y)^p \, d\pi(x,y) \right)^{1/p}\end{aligned}\]</span> This distance is a metric on the space of probability measureswith finite <span class="math inline">\(p\)</span>-th moment, and itsatisfies the properties of a metric: 1.<strong>Non-negativity</strong>: <span class="math inline">\(W_p(\mu,\nu) \geq 0\)</span> for all <span class="math inline">\(\mu,\nu\)</span>. 2. <strong>Identity of indiscernibles</strong>: <spanclass="math inline">\(W_p(\mu, \nu) = 0\)</span> if and only if <spanclass="math inline">\(\mu = \nu\)</span>. 3. <strong>Symmetry</strong>:<span class="math inline">\(W_p(\mu, \nu) = W_p(\nu, \mu)\)</span>. 4.<strong>Triangle inequality</strong>: <spanclass="math inline">\(W_p(\mu, \nu) + W_p(\nu, \sigma) \geq W_p(\mu,\sigma)\)</span> for all <span class="math inline">\(\mu, \nu,\sigma\)</span>.</p>]]></content>
    
    
    <categories>
      
      <category>Mathematics</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Optimal Transport</tag>
      
      <tag>Wasserstein Distance</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Densest Subgraph</title>
    <link href="/2025/06/16/Densest-Subgraph/"/>
    <url>/2025/06/16/Densest-Subgraph/</url>
    
    <content type="html"><![CDATA[<p>Reference:</p><ol type="1"><li><p><a href="https://dl.acm.org/doi/pdf/10.1145/3514221.3517837">AConvex-Programming Approach for Efficient Directed Densest SubgraphDiscovery</a></p></li><li><p><a href="">Efficient and Scalable Directed Densest SubgraphDiscovery</a></p></li></ol><h2 id="uds-problem">UDS problem</h2><h3 id="definition">Definition</h3><p>The Undirected Densest Subgraph (DS) problem is defined asfollows:</p><p><span class="math display">\[\max_{S\subseteq V} \frac{E(S)}{|S|}\]</span></p><p>where <span class="math inline">\(E(S)\)</span> is the number ofedges in the subgraph induced by <span class="math inline">\(S\)</span>.### Linear Program</p><p>TBD.</p><h2 id="dds-problem">DDS problem</h2><h3 id="definition-1">Definition</h3><p>The Directed Densest Subgraph (DDS) problem is defined as follows:<span class="math display">\[\max_{S,T\subseteq V} \frac{E(S,T)}{\sqrt{|S||T|}}\]</span> where <span class="math inline">\(E(S,T)\)</span> is thenumber of edges from <span class="math inline">\(S\)</span> to <spanclass="math inline">\(T\)</span>.</p><h3 id="linear-program">Linear Program</h3><p>The DDS problem can be formulated as the following linearprogram:</p><p><span class="math display">\[\begin{align*}\mathsf{LP}(c):\quad \text{max } &amp;  \sum_{(u,v)\in E}x_{u,v} \\\text{s.t. } &amp; \sum_{v\in V} x_{uv} \leq s_u, \forall u\in S \\&amp; \sum_{u\in V} x_{uv} \leq t_v, \forall v\in T \\&amp; x_{uv} \geq 0, \forall (u,v)\in E\\&amp; \sum_{u\in V} s_u = a\sqrt c\\&amp; \sum_{v\in V} t_v = \frac{b}{\sqrt c}\\&amp; a+b = 2\end{align*}\]</span> And we have two theorems:</p><h4 id="theorem-1">Theorem 1</h4><p>For a fixed <span class="math inline">\(c\)</span>, consider twoarbitrary sets <span class="math inline">\(P\)</span>,<spanclass="math inline">\(Q\subseteq V\)</span>, and let <spanclass="math inline">\(c&#39;=\frac{|P|}{|Q|}\)</span>. Then <spanclass="math inline">\(\text{OPT(LP(c))}\geq\frac{2\sqrt{cc&#39;}}{c+c&#39;}\rho(P,Q)\)</span>.</p><h4 id="theorem-2">Theorem 2</h4><p>Given a feasible solution <spanclass="math inline">\((x,s,t,a,b)\)</span> to <spanclass="math inline">\(\text{LP(c)}\)</span>, we can construct an <spanclass="math inline">\((S,T)\)</span>-induced subgraph <spanclass="math inline">\(G[S,T]\)</span> such that <spanclass="math inline">\(\sqrt{ab}\rho(S,T)\geq\text{OPT(LP(c))}=\sum_{(u,v)\in E}x_{u,v}\)</span>.</p><p>By setting <span class="math inline">\(c=\frac{|S^*|}{|T^*|}\)</span>in theorem 1, we have: <span class="math display">\[\max_c{\text{OPT(LP(c))}}\geq \rho(S^*,T^*)\]</span></p><p>From theorem 2, we have <span class="math inline">\(\max_{c}\text{OPT(LP(c))}\leq\sqrt{ab}\rho(S,T)\leq \rho(S,T)\leq\rho(S^*,T^*)\)</span>.</p><p>So <span class="math inline">\(\rho^*=\rho(S^*,T^*)=\max_{c}\text{OPT(LP(c))}\)</span></p><h3 id="dual-program">Dual Program</h3><p>The dual program of the DDS problem can be formulated as follows:</p><p><span class="math display">\[\begin{align*}\mathsf{DP}(c):&amp;&amp;\min &amp;&amp; \max_{u \in V} \{r_{\alpha}(u),r_{\beta}(u)\}\\&amp;&amp; \text{s.t.} &amp;&amp; \alpha_{u,v}+\beta_{v,u} &amp;=1,&amp;&amp; \forall (u,v) \in E\\&amp;&amp; &amp;&amp; 2\sqrt{c} \sum_{(u,v) \in E}\alpha_{u,v} &amp;=r_{\alpha}(u), &amp;&amp; \forall u \in V \\&amp;&amp; &amp;&amp; \frac{2}{\sqrt{c}} \sum_{(u,v) \in E}\beta_{v,u}&amp;= r_{\beta}(v), &amp;&amp; \forall v \in V \\&amp;&amp; &amp;&amp; \alpha_{u,v}, \beta_{v,u} &amp;\geq 0 &amp;&amp;\forall (u,v) \in E\end{align*}\]</span></p><h3 id="quadratic-program">Quadratic Program</h3><p>The dual program can be transformed into a quadratic program (QP) asfollows:</p><p><span class="math display">\[\begin{align*}\mathsf{QP}(c):&amp;&amp;\min&amp;&amp;\sqrt c \cdot \sum_{u\inV}w_{\alpha}(u)^2+\frac 1{\sqrt c} \cdot &amp; \sum_{v\inV}w_{\beta}(v)^2 \\&amp;&amp; \text{s.t.}&amp;&amp;\alpha_{u,v}+\beta_{v,u}&amp;=1,&amp;&amp; \forall (u,v) \in E\\&amp;&amp; &amp;&amp;\sum_{(u,v) \inE}\alpha_{u,v}&amp;=w_{\alpha}(u),&amp;&amp; \forall u \in V \\&amp;&amp; &amp;&amp;\sum_{(u,v) \inE}\beta_{v,u}&amp;=w_{\beta}(v),&amp;&amp; \forall v \in V \\&amp;&amp; &amp;&amp;\alpha_{u,v},\beta_{v,u} &amp;\geq 0 &amp;&amp;\forall (u,v) \in E\end{align*}\]</span></p>]]></content>
    
    
    <categories>
      
      <category>Mathematics</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Graph Theory</tag>
      
      <tag>Optimization</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Random Coordinate Descent Methods</title>
    <link href="/2025/06/14/Random-Coordinate-Descent-Methods/"/>
    <url>/2025/06/14/Random-Coordinate-Descent-Methods/</url>
    
    <content type="html"><![CDATA[<p>This is a reading note on the paper “Random Coordinate DescentMethods for Minimizing Decomposable Submodular Functions” by A. ENE, HuyL. NGUYEN</p><p><a href="https://arxiv.org/abs/1502.02643">arxiv</a></p><p>Knowledge prerequisite on submodular function (in Chinese):</p><p><ahref="https://notdesigned.github.io/2025/06/11/%E5%AD%90%E6%A8%A1%E5%87%BD%E6%95%B0%E4%BB%A5%E5%8F%8ALovasz%E6%8B%93%E5%B1%95/">子模函数以及Lovasz拓展</a></p><h2 id="preliminaries">Preliminaries</h2><p>Let <span class="math inline">\(V = \{1,2,\ldots, n\}\)</span>.</p><p>Regarding <span class="math inline">\(w\in \mathbb{R}^n\)</span> as amodular function <span class="math inline">\(w(A) = \sum_{i\in A}w_i\)</span>.</p><p>Let <span class="math inline">\(F: 2^V \to \mathbb{R}\)</span> be asubmodular function of the form: <span class="math display">\[\begin{align*}F &amp;= \sum_{i=1}^r F_i\\F(\emptyset) &amp;= 0\end{align*}\]</span> where each <span class="math inline">\(f_i\)</span> is asimple submodular function.</p><p>Additionally, we assume minimizing <spanclass="math inline">\(F_i+w\)</span> is easy, i.e., there existsefficient oracles to find the minimizer of <spanclass="math inline">\(F_i + w\)</span> for each <spanclass="math inline">\(i\)</span> and <span class="math inline">\(w\in\mathbb R^n\)</span>.</p><p>We want to minimize <span class="math inline">\(F\)</span>.</p><p>We pivot to the minimization of the Lovász extension <spanclass="math inline">\(f\)</span>.</p><p>Let <span class="math inline">\(f\)</span> be the Lovász extension of<span class="math inline">\(F\)</span> written as the support functionof the base polytope <span class="math inline">\(B(F)\)</span>: <spanclass="math display">\[f(x) = \max_{w\in B(F)} \langle w, x \rangle\]</span> where <span class="math inline">\(B(F) = \{w\in \mathbb{R}^n:w(A) \leq F(A), \forall A\subseteq V, w(V)=F(V)\}\)</span>.</p><p>And let <span class="math inline">\(f_i\)</span> be the Lovászextension of <span class="math inline">\(F_i\)</span>, <spanclass="math inline">\(B_i\)</span> be the base polytope of <spanclass="math inline">\(F_i\)</span>.</p><p>The paper considers a proximal version of the problem:</p><p><span class="math display">\[\min_{x\in \mathbb{R}^n} f(x) + \frac{1}{2}\|x\|^2 \equiv \min_{x\in\mathbb{R}^n} \sum_{i=1}^r f_i(x) + \frac{1}{2r}\|x\|^2\]</span></p><p>Given an optimal solution <span class="math inline">\(x\)</span> tothe proximal problem, one can obtain an optimal solution to the discreteproblem by thresholding <span class="math inline">\(x\)</span> at <spanclass="math inline">\(0\)</span>; more precisely, the optimal solutionto the discrete problem is given by <span class="math inline">\(A = \{i:x_i \geq 0\}\)</span>.</p><h3 id="lemma-1">Lemma 1</h3><p>The dual of the proximal problem is given by:</p><p><span class="math display">\[\max_{y^(1)\in B_1, \ldots, y^(r)\in B_r} -\frac{1}{2}\|\sum_{i=1}^ry^{(i)}\|^2.\]</span></p><p>The primal and dual variables are related by: <spanclass="math display">\[x = -\sum_{i=1}^r y^{(i)}.\]</span></p><h2 id="application-on-densest-subgraph-problem">Application on DensestSubgraph Problem</h2><p>To see the definition, please refer to the <ahref="https://notdesigned.github.io/2025/06/16/Densest-Subgraph/">DensestSubgraph Problem</a> post.</p><p>The quadratic formulation of DDS problem is defined as follows:</p><p><span class="math display">\[\begin{align*}\mathsf{QP}(c):&amp;&amp;\min&amp;&amp;\sqrt c \cdot \sum_{u\inV}\left(\sum_{(u,v) \in E}\alpha_{u,v}\right)^2+\frac 1{\sqrt c} \cdot&amp; \sum_{v\in V}\left(\sum_{(u,v) \in E}\beta_{v,u}\right)^2 \\&amp;&amp; \text{s.t.}&amp;&amp;\alpha_{u,v}+\beta_{v,u}&amp;=1,&amp;&amp; \forall (u,v) \in E\\&amp;&amp; &amp;&amp;\alpha_{u,v},\beta_{v,u} &amp;\geq 0 &amp;&amp;\forall (u,v) \in E\end{align*}\]</span></p><p>The dual of such quadratic program is formulated as follows:</p><h3 id="lagrangian-relaxation">Lagrangian relaxation</h3><p><span class="math display">\[\begin{align*}L = &amp; \sqrt{c} \sum_{u\in V}\left(\sum_{(u,v) \inE}\alpha_{u,v}\right)^2 + \frac{1}{\sqrt{c}} \sum_{v\inV}\left(\sum_{(u,v) \in E}\beta_{v,u}\right)^2 \\&amp; + \sum_{(u,v) \in E} \lambda_{u,v}(1 - \alpha_{u,v} - \beta_{v,u})\\&amp; - \sum_{(u,v) \in E} \rho_{u,v} \alpha_{u,v} - \sum_{(u,v) \in E}\sigma_{v,u} \beta_{v,u}\end{align*}\]</span></p><p>Now consider the KKT conditions:</p><h3 id="kkt-conditions">KKT Conditions</h3><h4 id="stationarity-conditions">Stationarity Conditions</h4><p><span class="math display">\[\begin{align*}\frac{\partial L}{\partial \alpha_{u,v}} &amp;= 2\sqrt{c} \sum_{(u,s)\in E}\alpha_{u,s} - \lambda_{u,v} - \rho_{u,v} = 0 \\\frac{\partial L}{\partial \beta_{v,u}} &amp;= \frac{2}{\sqrt{c}}\sum_{(t,v) \in E}\beta_{v,t} - \lambda_{u,v} - \sigma_{v,u} = 0\end{align*}\]</span></p><p>Define: <span class="math display">\[w_{\alpha}(u) = \sum_{(u,v) \in E}\alpha_{u,v}, \quad w_{\beta}(v) =\sum_{(u,v) \in E}\beta_{v,u}\]</span></p><p>Then: <span class="math display">\[\begin{align*}2\sqrt{c} w_{\alpha}(u) - \lambda_{u,v} - \rho_{u,v} &amp;= 0\\\frac{2}{\sqrt{c}} w_{\beta}(v) - \lambda_{u,v} - \sigma_{v,u} &amp;= 0\end{align*}\]</span> <span class="math display">\[\begin{align*}\Rightarrow\begin{cases}\lambda_{u,v} + \rho_{u,v} &amp;= 2\sqrt{c} \sum_{(u,s) \inE}\alpha_{u,s}\\\lambda_{u,v} + \sigma_{v,u} &amp;= \frac{2}{\sqrt{c}} \sum_{(t,v) \inE}\beta_{v,t}\end{cases}\end{align*}\]</span></p><h4 id="primal-feasibility-conditions">Primal FeasibilityConditions</h4><p><span class="math display">\[\begin{align*}\alpha_{u,v} + \beta_{v,u} &amp;= 1, &amp;&amp; \forall (u,v) \in E \\\alpha_{u,v}, \beta_{v,u} &amp;\geq 0, &amp;&amp; \forall (u,v) \in E\end{align*}\]</span></p><h4 id="dual-feasibility-conditions">Dual Feasibility Conditions</h4><p><span class="math display">\[\begin{align*}\rho_{u,v} &amp;\geq 0, &amp;&amp; \forall (u,v) \in E \\\sigma_{v,u} &amp;\geq 0, &amp;&amp; \forall (u,v) \in E\end{align*}\]</span></p><h4 id="complementary-slackness-conditions">Complementary SlacknessConditions</h4><p><span class="math display">\[\begin{align*}\rho_{u,v} \cdot \alpha_{u,v} &amp;= 0, &amp;&amp; \forall (u,v) \in E\\\sigma_{v,u} \cdot \beta_{v,u} &amp;= 0, &amp;&amp; \forall (u,v) \in E\end{align*}\]</span></p><h3 id="analysis-simplification">Analysis &amp; Simplification</h3><p>From the stationarity conditions, we notice that for any vertex <spanclass="math inline">\(u\)</span>, the quantity <spanclass="math inline">\(2\sqrt{c} \sum_{(u,s) \in E}\alpha_{u,s}\)</span>must be equal to <span class="math inline">\(\lambda_{u,v} +\rho_{u,v}\)</span> for <strong>all edges</strong> <spanclass="math inline">\((u,v)\)</span> incident to <spanclass="math inline">\(u\)</span>. This gives us:</p><p><span class="math display">\[\lambda_{u,v_1} + \rho_{u,v_1} =\lambda_{u,v_2} + \rho_{u,v_2} = \cdots\]</span></p><p>for all edges <span class="math inline">\((u,v_1), (u,v_2),\ldots\)</span> incident to vertex <spanclass="math inline">\(u\)</span>.</p><p>Similarly, for any vertex <span class="math inline">\(v\)</span>:<span class="math display">\[\lambda_{u_1,v} + \sigma_{v,u_1} =\lambda_{u_2,v} + \sigma_{v,u_2} = \cdots\]</span></p><p>Let us define: <span class="math display">\[x_u := 2\sqrt{c}\sum_{(u,s) \in E}\alpha_{u,s}, \quad y_v := \frac{2}{\sqrt{c}}\sum_{(t,v) \in E}\beta_{v,t}\]</span></p><p>Then we have: <span class="math display">\[\lambda_{u,v} + \rho_{u,v}= x_u, \quad \lambda_{u,v} + \sigma_{v,u} = y_v\]</span></p><p>Substituting stationarity conditions back to Lagrangian: <spanclass="math display">\[\begin{align*}\inf_{\alpha,\beta,w}L(\alpha,\beta,w,\lambda,\rho,\sigma) = &amp;\sqrt{c} \sum_{u\in V}\left(\sum_{(u,v) \in E}\alpha_{u,v}\right)^2 +\frac{1}{\sqrt{c}} \sum_{v\in V}\left(\sum_{(u,v) \inE}\beta_{v,u}\right)^2 \\&amp; + \sum_{(u,v) \in E} \lambda_{u,v}(1 - \alpha_{u,v} - \beta_{v,u})\\&amp; - \sum_{(u,v) \in E} \rho_{u,v} \alpha_{u,v} - \sum_{(u,v) \in E}\sigma_{v,u} \beta_{v,u}\\= &amp; \sqrt{c} \sum_{u\in V}\left(\sum_{(u,v) \inE}\alpha_{u,v}\right)^2 + \frac{1}{\sqrt{c}} \sum_{v\inV}\left(\sum_{(u,v) \in E}\beta_{v,u}\right)^2 \\&amp; + \sum_{(u,v) \in E} \lambda_{u,v} \\&amp; - \sum_{(u,v) \in E} (\lambda_{u,v} + \rho_{u,v}) \alpha_{u,v} -\sum_{(u,v) \in E} (\lambda_{u,v} + \sigma_{v,u}) \beta_{v,u}\\= &amp; \sqrt{c} \sum_{u\in V}\left(\sum_{(u,v) \inE}\alpha_{u,v}\right)^2 + \frac{1}{\sqrt{c}} \sum_{v\inV}\left(\sum_{(u,v) \in E}\beta_{v,u}\right)^2 \\&amp; + \sum_{(u,v) \in E} \lambda_{u,v} - \sum_{(u,v) \in E}\left(2\sqrt{c} \sum_{(u,s) \in E}\alpha_{u,s}\right) \alpha_{u,v} \\&amp; - \sum_{(u,v) \in E} \left(\frac{2}{\sqrt{c}} \sum_{(t,v) \inE}\beta_{v,t}\right) \beta_{v,u} \\= &amp; \sqrt{c} \sum_{u\in V}\left(\sum_{(u,v) \inE}\alpha_{u,v}\right)^2 + \frac{1}{\sqrt{c}} \sum_{v\inV}\left(\sum_{(u,v) \in E}\beta_{v,u}\right)^2 \\&amp; + \sum_{(u,v) \in E} \lambda_{u,v} - 2\sqrt{c} \sum_{u \in V}\left(\sum_{(u,v) \in E}\alpha_{u,v}\right)^2 \\&amp; - \frac{2}{\sqrt{c}} \sum_{v \in V} \left(\sum_{(u,v) \inE}\beta_{v,u}\right)^2 \\= &amp; -\sqrt{c} \sum_{u\in V}\left(\sum_{(u,v) \inE}\alpha_{u,v}\right)^2 - \frac{1}{\sqrt{c}} \sum_{v\inV}\left(\sum_{(u,v) \in E}\beta_{v,u}\right)^2 \\&amp; + \sum_{(u,v) \in E} \lambda_{u,v} \\= &amp; -\frac{1}{4\sqrt{c}} \sum_{u\in V} x_u^2 - \frac{\sqrt{c}}{4}\sum_{v\in V} y_v^2 + \sum_{(u,v) \in E} \lambda_{u,v}\end{align*}\]</span></p><p>To maximize the dual objective, we want to maximize <spanclass="math inline">\(\lambda_{u,v}\)</span> which means minimizing<span class="math inline">\(\rho_{u,v} + \sigma_{v,u}\)</span> subjectto:</p><ul><li><span class="math inline">\(\rho_{u,v}, \sigma_{v,u} \geq0\)</span></li><li><span class="math inline">\(\lambda_{u,v} = x_u - \rho_{u,v} = y_v -\sigma_{v,u}\)</span></li></ul><p>This gives us <span class="math inline">\(\rho_{u,v} - \sigma_{v,u} =x_u - y_v\)</span>.</p><p><strong>Case 1:</strong> If <span class="math inline">\(x_u \geqy_v\)</span>, set <span class="math inline">\(\sigma_{v,u} = 0\)</span>and <span class="math inline">\(\rho_{u,v} = x_u - y_v \geq 0\)</span>.Then <span class="math inline">\(\lambda_{u,v} = y_v\)</span>.</p><p><strong>Case 2:</strong> If <span class="math inline">\(x_u \leqy_v\)</span>, set <span class="math inline">\(\rho_{u,v} = 0\)</span>and <span class="math inline">\(\sigma_{v,u} = y_v - x_u \geq0\)</span>. Then <span class="math inline">\(\lambda_{u,v} =x_u\)</span>.</p><p>Therefore: <span class="math inline">\(\lambda_{u,v} = \min(x_u,y_v)\)</span>.</p><p>So the dual objective function can be simplified to: <spanclass="math display">\[\begin{align*}\mathsf{DQP}(c) &amp;= \max_{x,y} -\frac{1}{4\sqrt{c}} \sum_{u\in V}x_u^2 - \frac{\sqrt{c}}{4} \sum_{v\in V} y_v^2 + \sum_{(u,v) \in E}\min(x_u, y_v)\end{align*}\]</span></p><p>And the constraints are given by the primal feasibility conditions,which is equivalent to:</p><p><span class="math display">\[\begin{align*}\frac{1}{2\sqrt{c}}\sum_{u \in V} x_u + \frac{\sqrt{c}}{2}\sum_{v \in V}y_v = |E| \\x_u, y_v \geq 0, \quad \forall u \in V, v \in V\end{align*}\]</span></p><p>Because a valid solution of primal can be constructed by using flowtheory.</p><h3 id="dual-quadratic-program">Dual Quadratic Program</h3><p>Set new <span class="math inline">\(x=(\frac{x}{2},\frac{y}{2})\)</span>, we can rewrite the dual quadratic program asfollows:</p><p><span class="math display">\[\begin{align*}\mathsf{DQP}(c):&amp;\min_{x\in \mathbb R^{2n}}  \frac{1}{2}x^TWx -\sum_{(u,v)\in E}\min(x_u,x_{n+v})\\&amp;\text{s.t. } a^Tx = |E| \\&amp;\quad\quad\quad x_u \geq 0, \quad \forall u \in V\\&amp;\text{where } W=\begin{bmatrix}\frac{1}{\sqrt c}I_n &amp; 0 \\0 &amp; \sqrt{c}I_n\end{bmatrix}, \quad \vec{a} = [\frac{1}{\sqrt{c}}, \frac{1}{\sqrt{c}}, \ldots,\frac{1}{\sqrt{c}},\sqrt{c}, \sqrt{c}, \ldots, \sqrt{c}]^T\end{align*}\]</span></p><h4 id="submodular-function-representation">Submodular FunctionRepresentation</h4><p>Let define the submodular function <spanclass="math inline">\(F:2^{V\coprod V}\to \mathbb R\)</span> asfollows:</p><p><span class="math display">\[F(S,T) = \sum_{(u,v)\in E} F_{u,v}(S,T) = -|E(S,T)|\]</span></p><p><span class="math inline">\(F_{u,v}(S,T)\)</span> is defined asfollows:</p><p><span class="math display">\[F_{u,v}(S,T) = \begin{cases}0 &amp; \text{if } u\notin S \text{ and } v\notin T \\0 &amp; \text{if } u\in S \text{ and } v\notin T \\0 &amp; \text{if } u\notin S \text{ and } v\in T\\-1 &amp; \text{if } u\in S \text{ and } v\in T\end{cases}\]</span></p><p>The base polytope <span class="math inline">\(B(F_{u,v})\)</span> isdefined as follows:</p><p><span class="math display">\[B(F_{u,v}) = \{w\in \mathbb{R}^{V\coprod V}: w(S,T) \leq F(S,T), \forallS\subseteq V, T\subseteq V, w(V,V)=F(V,V)\}\]</span></p><p>Obviously we have <span class="math inline">\(w_{j}=0\)</span> forall <span class="math inline">\(j\neq u \text{ or } {v+n}\)</span>.(Here use <span class="math inline">\(v+n\)</span> to denote the vertexcorresponding to <span class="math inline">\(v\)</span> in the secondcopy of <span class="math inline">\(V\)</span>.)</p><p>And <span class="math inline">\(w_{u}+w_{v+n} = -1\)</span>.</p><p>the Lovász extension of <span class="math inline">\(F_{u,v}\)</span>is given by:</p><p><span class="math display">\[f_{u,v}(x) = \max_{w\in B(F_{u,v})} \langle w, x \rangle\]</span></p><p>which just equals to <span class="math inline">\(-\min(x_u,x_{n+v})\)</span>.</p><p>Thus, with <span class="math inline">\(a^Tx\)</span> normalized from<span class="math inline">\(|E|\)</span> to <spanclass="math inline">\(2\)</span>,</p><p>the dual quadratic program can be rewritten as: <spanclass="math display">\[\begin{align*}\mathsf{DQP}(c):&amp;\min_{x\in \mathbb R^{2n}}  \frac{1}{2}x^TWx +\sum_{(u,v)\in E} f_{u,v}(x)\\&amp;\text{s.t. } a^Tx = 2 \\&amp;\quad\quad\quad x_u, y_v \geq 0, \quad \forall u \in V, v \in V \\&amp;\text{where } W=\begin{bmatrix}\frac{1}{\sqrt c}I_n &amp; 0 \\0 &amp; \sqrt{c}I_n\end{bmatrix}, \quad \vec{a} = [\frac{1}{\sqrt{c}}, \frac{1}{\sqrt{c}},\ldots, \frac{1}{\sqrt{c}},\sqrt{c}, \sqrt{c}, \ldots, \sqrt{c}]^T\end{align*}\]</span></p><h3 id="another-derivation">Another Derivation</h3><p>Just ignore the quadratic term, we can rewrite the dual quadraticprogram as a ratio linear program (RLP): <span class="math display">\[\begin{align*}\mathsf{RLP}(c):&amp;\max_{x\in \mathbb R^{2n}}  \sum_{(u,v)\inE}\min(x_u,x_{n+v})\\&amp;\text{s.t. } a^Tx = 2 \\&amp;\quad\quad\quad x_u, y_v \geq 0, \quad \forall u \in V, v \in V \\&amp;\text{where } \vec{a} = [\frac{1}{\sqrt{c}}, \frac{1}{\sqrt{c}},\ldots, \frac{1}{\sqrt{c}},\sqrt{c}, \sqrt{c}, \ldots, \sqrt{c}]^T\end{align*}\]</span></p><p>Let’s introduce the definition of c-biased density: <spanclass="math display">\[\begin{gather*}\rho_c(S,T) = \frac{2 \sqrt c \sqrt{c&#39;}}{c+c&#39;}  \frac{|E(S,T)|}{\sqrt{|S||T|}}=\frac{2|E(S,T)|}{\frac{1}{\sqrtc} |S|+ \sqrt c |T|}\\\text{where } c&#39; = \frac{|S|}{|T|}\end{gather*}\]</span></p><p>And the c-biased DDS is defined as the subgraph G(S,T) such that<span class="math inline">\(\rho_c(S,T)\)</span> is maximized.</p><p>We have the following theorem: #### Theorem 1</p><p>The optimal value of <spanclass="math inline">\(\mathsf{RLP}(c)\)</span>, <spanclass="math inline">\(\text{OPT}({\mathsf{RLP}(c)})=\rho^*_c(S,T)\)</span>and the optimal solution can be recovered by thresholding.</p><h5 id="proof">Proof</h5><p>First, we show that <spanclass="math inline">\(\text{OPT}({\mathsf{RLP}(c)})\geq\rho^*_c(S,T)\)</span>.</p><p>For any vertex sets <span class="math inline">\(S \subseteq V, T\subseteq V\)</span>, consider the feasible solution to RLP:</p><p><span class="math inline">\(x_u = \alpha\)</span> for <spanclass="math inline">\(u \in S\)</span>, <span class="math inline">\(x_u= 0\)</span> for <span class="math inline">\(u \notin S\)</span></p><p><span class="math inline">\(x_{n+v} = \beta\)</span> for <spanclass="math inline">\(v \in T\)</span>, <spanclass="math inline">\(x_{n+v} = 0\)</span> for <spanclass="math inline">\(v \notin T\)</span></p><p>The constraint gives us:</p><p><span class="math display">\[\frac{1}{\sqrt{c}}|S| \cdot \alpha + \sqrt{c}|T| \cdot \beta = 2\]</span></p><p>The objective value is:</p><p><span class="math display">\[\sum_{(u,v) \in E} \min(x_u, x_{n+v}) = |E(S,T)| \cdot \min(\alpha,\beta)\]</span></p><p>To maximize this, we want to maximize <spanclass="math inline">\(\min(\alpha, \beta)\)</span> subject to theconstraint.</p><p>If <span class="math inline">\(\alpha = \beta\)</span>, then: <spanclass="math inline">\(\alpha = \frac{2}{\frac{1}{\sqrt{c}}|S| +\sqrt{c}|T|}\)</span></p><p>Objective = <span class="math inline">\(|E(S,T)| \cdot\frac{2}{\frac{1}{\sqrt{c}}|S| + \sqrt{c}|T|}\)</span></p><p>This can be rewritten as: <span class="math display">\[\frac{2|E(S,T)|}{\frac{1}{\sqrt{c}}|S| + \sqrt{c}|T|}  = \rho_c(S,T)\]</span></p><p>Therefore: <span class="math inline">\(\text{OPT}({\mathsf{RLP}(c)})\geq \max_{S,T} \rho_c(S,T)\)</span></p><p>Now we show that <spanclass="math inline">\(\text{OPT}({\mathsf{RLP}(c)}) \leq \max_{S,T}\rho_c(S,T)\)</span>.</p><p>The LP objective can be rewritten as: <span class="math display">\[\sum_{(u,v) \in E} \min(x^*_u, y^*_v) = \sum_{(u,v) \in E}\int_0^{\infty} \mathbf{1}[x^*_u \geq t \text{ and } y^*_v \geq t] \, dt\]</span></p><p>For each threshold <span class="math inline">\(t \geq 0\)</span>,define:</p><ul><li><span class="math inline">\(S_t = \{u \in V : x^*_u \geqt\}\)</span> (vertices in first part with value ≥ t)</li><li><span class="math inline">\(T_t = \{v \in V : y^*_v \geqt\}\)</span> (vertices in second part with value ≥ t)</li></ul><p>Then the number of edges between <spanclass="math inline">\(S_t\)</span> and <spanclass="math inline">\(T_t\)</span> is given by: <spanclass="math display">\[\sum_{(u,v) \in E} \mathbf{1}[x^*_u \geq t \text{and } y^*_v \geq t] = |E(S_t, T_t)|\]</span></p><p>Consider the constraint: <span class="math display">\[\begin{align*}\frac{1}{\sqrt{c}}\sum_{u} x^*_u + \sqrt{c}\sum_{v} y^*_v &amp;=\frac{1}{\sqrt{c}}\sum_{u} \int_0^{\infty} \mathbf{1}[x^*_u \geq t] \,dt + \sqrt{c}\sum_{v} \int_0^{\infty} \mathbf{1}[y^*_v \geq t] \, dt\\&amp;= \int_0^{\infty} \left( \frac{1}{\sqrt{c}}|S_t| + \sqrt{c}|T_t|\right) dt\end{align*}\]</span></p><p>So <span class="math display">\[\frac{\text{LP objective}}{2} = \frac{\int_0^{\infty} |E(S_t, T_t)| \,dt}{\int_0^{\infty} \left( \frac{1}{\sqrt{c}}|S_t| + \sqrt{c}|T_t|\right) dt}\]</span></p><p>By averaging principle:</p><p><span class="math display">\[\frac{\int g(t) dt}{\int h(t) dt} \leq \max_t \frac{g(t)}{h(t)}\]</span></p><p>We get: <span class="math display">\[\frac{\text{LP objective}}{2} \leq \max_{t \geq 0} \frac{|E(S_t,T_t)|}{\frac{1}{\sqrt{c}}|S_t| + \sqrt{c}|T_t|}\]</span></p><p>Thus: <span class="math display">\[\text{OPT}({\mathsf{RLP}(c)}) \leq \max_{S,T} \rho_c(S,T)\]</span></p><p>This completes the proof of Theorem 1.</p><p>And <span class="math inline">\(\text{DQP}\)</span> can be consideredas a proximal version of <spanclass="math inline">\(\text{RLP}\)</span>.</p><h3 id="complexity-analysis">Complexity Analysis</h3><p>Let <span class="math inline">\(\kappa = \max\{\sqrt c,\frac{1}{\sqrt c}\}\)</span>,</p><p><span class="math display">\[\begin{gather*}r=m\\y\in \R^{2n*r}=(y^{(1)},...,y^{(r)})\\W=\begin{bmatrix}c^{1/4}I_n &amp; 0 \\0 &amp; c^{-1/4}I_n\end{bmatrix},S=\frac{1}{\sqrt r}[W,W,...,W]\in{\R^{2n\times{2nr}}}\\%f=\sum_{(u,v)\in E}f_{u,v}(x)\\g=r||Sy||^2\\\nabla g(y)=2rS^T Sy\\||\nabla_ig(x)-\nabla_ig(y)||\leq L_i||x^{(i)}-y^{(i)}||,\text{for allvectors }x,y\in \R^{2nr} \text{that differ only in block } i\\L_i=2\max\{\sqrt c,\frac{1}{\sqrt c}\}=2\kappa\\S^T S = \frac{1}{r} \begin{bmatrix}W^T W &amp; W^T W &amp; \cdots &amp; W^T W \\W^T W &amp; W^T W &amp; \cdots &amp; W^T W \\\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\W^T W &amp; W^T W &amp; \cdots &amp; W^T W\end{bmatrix}\end{gather*}\]</span></p><h3 id="key-sets-and-definitions">Key Sets and Definitions</h3><p><strong>Feasible Set:</strong> <span class="math inline">\(Y =\prod_{i=1}^r B(F_i) = B(F_1) \times B(F_2) \times \cdots \timesB(F_r)\)</span></p><p><strong>Null Space:</strong> <span class="math inline">\(Q = \{y \in\mathbb{R}^{2nr} : Sy = 0\} = \left\{y \in \mathbb{R}^{2nr} :\sum_{i=1}^r W y^{(i)} = 0\right\}\)</span></p><p><strong>Optimal Solution Set:</strong> <span class="math inline">\(E= \{y \in Y : g(y) = \min_{z \in Y} g(z)\}\)</span></p><p><strong>Alternative Characterization of <spanclass="math inline">\(E\)</span>:</strong> <span class="math inline">\(E= \{y \in Y : d(y, Q) = d(Y, Q)\}\)</span></p><p>where <span class="math inline">\(d(y, Q) = \min_{z \in Q} \|y -z\|\)</span> and <span class="math inline">\(d(Y, Q) = \min_{y \in Y}d(y, Q)\)</span>.</p><p><strong>Geometric Interpretation:</strong></p><ul><li><span class="math inline">\(E\)</span> represents the set of pointsin the feasible region <span class="math inline">\(Y\)</span> that areclosest to satisfying the constraint <span class="math inline">\(Sy =0\)</span></li><li>Points in <span class="math inline">\(E\)</span> are optimalsolutions to our proximal optimization problem</li><li>The set <span class="math inline">\(E\)</span> connects to optimalsolutions of the original discrete problem via thresholding</li></ul><h4 id="analogue-of-theorem-2">Analogue of theorem 2:</h4><p><span class="math display">\[||S(y-y^*)|| \geq \frac{1}{2nr}||y-y^*||\]</span></p><p>Proof:</p><p>The result is identical to the one in the original paper.</p><p><a href="https://g.co/gemini/share/7a4f1299ca26">Proof ofgemini</a></p><h4 id="analogue-of-lemma-7">Analogue of lemma 7:</h4><p>Let <span class="math inline">\(R \subseteq \{1, 2, \ldots,r\}\)</span> be a random subset where each <span class="math inline">\(i\in \{1, 2, \ldots, r\}\)</span> is included independently withprobability <span class="math inline">\(1/r\)</span>. For vectors <spanclass="math inline">\(x, h \in \mathbb{R}^{2nr}\)</span>, let <spanclass="math inline">\(h_R\)</span> be defined by <spanclass="math inline">\((h_R)^{(i)} = h^{(i)}\)</span> if <spanclass="math inline">\(i \in R\)</span> and <spanclass="math inline">\((h_R)^{(i)} = 0\)</span> otherwise. Then: <spanclass="math display">\[E[g(x + h_R)] \leq g(x) + \frac{1}{r}\langle\nabla g(x), h\rangle +\frac{2\kappa}{r}\|h\|^2\]</span></p><p><strong>Proof:</strong></p><p>We have: <span class="math display">\[E[g(x + h_R)] = E[r\|S(x +h_R)\|^2]\]</span> <span class="math display">\[= E[r\|Sx\|^2 +r\|Sh_R\|^2 + 2r\langle Sx, Sh_R\rangle]\]</span> <spanclass="math display">\[= g(x) + rE[\|Sh_R\|^2] +\frac{1}{r}\langle\nabla g(x), h\rangle\]</span></p><p>The key term to bound is: <span class="math display">\[E[\|Sh_R\|^2]= E\left[\left\|\frac{1}{\sqrt{r}} \sum_{i \in R} Wh^{(i)}\right\|^2\right] = \frac{1}{r} E\left[\left\|W \sum_{i \in R}h^{(i)}\right\|^2\right]\]</span></p><p>Using the matrix norm bound: <spanclass="math display">\[E\left[\left\|W \sum_{i \in R}h^{(i)}\right\|^2\right] \leq \|W\|^2 E\left[\left\|\sum_{i \in R}h^{(i)}\right\|^2\right]\]</span></p><p>From standard coordinate descent analysis: <spanclass="math display">\[E\left[\left\|\sum_{i \in R}h^{(i)}\right\|^2\right] \leq \frac{2}{r} \sum_{i=1}^r \|h^{(i)}\|^2 =\frac{2}{r} \|h\|^2\]</span></p><p>Since <span class="math inline">\(\|W\|^2 = \kappa\)</span>: <spanclass="math display">\[E[\|Sh_R\|^2] \leq \frac{1}{r} \cdot \kappa \cdot\frac{2}{r} \|h\|^2 = \frac{2\kappa}{r^2} \|h\|^2\]</span></p><p>Therefore: <span class="math display">\[E[g(x + h_R)] \leq g(x) +\frac{1}{r}\langle\nabla g(x), h\rangle +\frac{2\kappa}{r}\|h\|^2\]</span></p><h4 id="analogue-of-theorem-8">Analogue of theorem 8:</h4><p>Consider iteration <span class="math inline">\(k\)</span> of theAPPROX algorithm. Let <spanclass="math inline">\(y_k=\theta_k^2u_{k+1}+z_{k+1}\)</span>, Let <spanclass="math inline">\(y^*=\arg \min_{y\in E} ||y-y_k||\)</span> is theoptimal solution that is closest to <spanclass="math inline">\(y_k\)</span>. Then we have: <spanclass="math display">\[E_{\xi_\ell}[g(y_{\ell+1}) - g(y^*)] \leq \frac{8r^2 \kappa}{(k - 1 +2r)^2} \left(\left(1 - \frac{1}{r}\right)(g(y_\ell) - g(y^*)) +2\|y_\ell - y^*\|^2\right)\]</span> <strong>Proof:</strong></p><p>This follows directly from applying Fercoq-Richtárik’s Theorem 3with:</p><ul><li><span class="math inline">\(\tau = 1\)</span> (samplingparameter)</li><li><span class="math inline">\(\nu_i = 4\kappa\)</span> for each <spanclass="math inline">\(i \in \{1, 2, \ldots, r\}\)</span></li></ul><h4 id="geometric-bound-from-theorem-2-analogue">Geometric Bound fromTheorem 2 Analogue</h4><p>We need the relationship between function values and distances.</p><p><strong>Key Geometric Relationship:</strong> For our specificobjective function <span class="math inline">\(g(y) =r\|Sy\|^2\)</span>, we have: <span class="math display">\[g(y_\ell) =g(y^*) + \langle\nabla g(y^*), y_\ell - y^*\rangle + \int_0^1\langle\nabla g(y^* + t(y_\ell - y^*)) - \nabla g(y^*), y_\ell -y^*\rangle dt\]</span></p><p>This uses the fundamental theorem of calculus. For <spanclass="math inline">\(\phi(t) = g(y^* + t(y_\ell - y^*))\)</span>: <spanclass="math display">\[g(y_\ell) - g(y^*) = \int_0^1 \phi&#39;(t) dt =\int_0^1 \langle\nabla g(y^* + t(y_\ell - y^*)), y_\ell - y^*\rangledt\]</span></p><p>Since <span class="math inline">\(\nabla g(z) = 2rS^T Sz\)</span>:<span class="math display">\[\nabla g(y^* + t(y_\ell - y^*)) - \nablag(y^*) = 2rS^T S \cdot t(y_\ell - y^*)\]</span></p><p>Therefore: <span class="math display">\[\int_0^1 \langle\nabla g(y^*+ t(y_\ell - y^*)) - \nabla g(y^*), y_\ell - y^*\rangle dt = \int_0^12rt\|S(y_\ell - y^*)\|^2 dt = r\|S(y_\ell - y^*)\|^2\]</span></p><p>Since <span class="math inline">\(y^*\)</span> is optimal, <spanclass="math inline">\(\langle\nabla g(y^*), y_\ell - y^*\rangle \leq0\)</span> (first-order optimality condition).</p><p>Thus: <span class="math display">\[g(y_\ell) - g(y^*) \geqr\|S(y_\ell - y^*)\|^2\]</span></p><h4 id="applying-theorem-2-analog">Applying Theorem 2 Analog</h4><p>From our Theorem 2 analog: <span class="math display">\[\|S(y_\ell -y^*)\| \geq \frac{1}{2nr} \|y_\ell - y^*\|\]</span></p><p>Therefore: <span class="math display">\[g(y_\ell) - g(y^*) \geqr\|S(y_\ell - y^*)\|^2 \geq \frac{r}{(2nr)^2} \|y_\ell -y^*\|^2\]</span> <span class="math display">\[= \frac{1}{4n^2r} \|y_\ell- y^*\|^2\]</span></p><p><strong>Geometric Bound:</strong> <spanclass="math display">\[\|y_\ell - y^*\|^2 \leq 4n^2r (g(y_\ell) -g(y^*))\]</span></p><h4 id="single-epoch-analysis">Single Epoch Analysis</h4><p>Consider epoch <span class="math inline">\(\ell\)</span>. Let <spanclass="math inline">\(y_{\ell+1}\)</span> be the solution constructed bythe APPROX algorithm after running for <spanclass="math inline">\(T\)</span> iterations starting with <spanclass="math inline">\(y_\ell\)</span> (so <spanclass="math inline">\(z_0 = y_\ell\)</span>).</p><p>Let <span class="math inline">\(y^* = \arg\min_{y \in E} \|y -y_{\ell+1}\|\)</span> be the optimal solution closest to <spanclass="math inline">\(y_{\ell+1}\)</span>.</p><p>By Theorem 8 Analog with <span class="math inline">\(k = T\)</span>:<span class="math display">\[E_{\xi_\ell}[g(y_{\ell+1}) - g(y^*)] \leq\frac{8r^2 \kappa}{(T - 1 + 2r)^2} \left(\left(1 -\frac{1}{r}\right)(g(y_\ell) - g(y^*)) + 2\|y_\ell -y^*\|^2\right)\]</span></p><p>From Step 3: <span class="math display">\[\|y_\ell - y^*\|^2 \leq4n^2r (g(y_\ell) - g(y^*))\]</span></p><p>Therefore: <span class="math display">\[E_{\xi_\ell}[g(y_{\ell+1}) -g(y^*)] \leq \frac{8r^2 \kappa}{(T - 1 + 2r)^2} \left(\left(1 -\frac{1}{r}\right) + 2 \cdot 4n^2r\right) (g(y_\ell) -g(y^*))\]</span></p><p><span class="math display">\[= \frac{8r^2 \kappa}{(T - 1 + 2r)^2}\left(1 - \frac{1}{r} + 8n^2r\right) (g(y_\ell) - g(y^*))\]</span></p><p>For large <span class="math inline">\(n, r\)</span>, the dominantterm is <span class="math inline">\(8n^2r\)</span>, so: <spanclass="math display">\[E_{\xi_\ell}[g(y_{\ell+1}) - g(y^*)] \leq\frac{8r^2 \kappa \cdot 8n^2r}{(T - 1 + 2r)^2} (g(y_\ell) -g(y^*))\]</span></p><p><span class="math display">\[= \frac{64n^2r^3 \kappa}{(T - 1 + 2r)^2}(g(y_\ell) - g(y^*))\]</span></p><h4 id="epoch-length-calculation">Epoch Length Calculation</h4><p>To achieve a factor of <spanclass="math inline">\(\frac{1}{2}\)</span> improvement per epoch, weneed: <span class="math display">\[\frac{64n^2r^3 \kappa}{(T - 1 +2r)^2} \leq \frac{1}{2}\]</span></p><p>This gives us: <span class="math display">\[(T - 1 + 2r)^2 \geq128n^2r^3 \kappa\]</span></p><p><span class="math display">\[T - 1 + 2r \geq 8\sqrt{2} nr^{3/2}\kappa^{1/2}\]</span></p><p>For large <span class="math inline">\(r\)</span>, we can approximate:<span class="math display">\[T \geq 8\sqrt{2} nr^{3/2} \kappa^{1/2}\approx 11.31 nr^{3/2} \kappa^{1/2}\]</span></p><p>To ensure the bound holds robustly, we choose: <spanclass="math display">\[T = 12nr^{3/2} \kappa^{1/2}\]</span></p><p>This gives us: <spanclass="math display">\[E_{\xi_\ell}[g(y_{\ell+1}) - g(y^*)] \leq\frac{1}{2} (g(y_\ell) - g(y^*))\]</span></p><h3 id="conclusion">Conclusion</h3><p>From now on, here an iteration is defined as one pass through theentire set of edges <span class="math inline">\(|E| = r = m\)</span> forthe convenience of comparison.</p><p>After <span class="math inline">\(\ell\)</span> epochs of the ACDMalgorithm (equivalently, <spanclass="math inline">\(12n\sqrt{r\kappa}\ell\)</span> iterations), wehave: <span class="math inline">\(E[g(y_{\ell+1}) - g(y^*)] \leq\frac{1}{2^{\ell+1}}(g(y_0) - g(y^*))\)</span></p><p>where <span class="math inline">\(y^* = \arg\min_{y \in E} \|y -y_{\ell+1}\|\)</span> is the optimal solution in <spanclass="math inline">\(E\)</span> that is closest to <spanclass="math inline">\(y_{\ell+1}\)</span>.</p><h4 id="theorem-5.6">Theorem 5.6</h4><p>Suppose <span class="math inline">\(\|g(y)-g(y^*)\| \leq\epsilon\)</span>, then <spanclass="math inline">\(\|y\|_{\infty}-\|y^*\|_{\infty} \leq 2 \sqrt{\kappa \epsilon}\)</span></p><p>Now we can use the above result to give a guarantee for the <spanclass="math inline">\(\epsilon\)</span>-approximation c-DDSsubproblem.</p><p>To solve the <spanclass="math inline">\(\epsilon\)</span>-approximation c-DDS subproblem,we need <span class="math display">\[\|y\|_{\infty}-\|y^*\|_{\infty} \leq \epsilon\]</span></p><p>which requires: <span class="math display">\[\|g(y)-g(y^*)\| \leq \frac{\epsilon^2}{4\kappa}\]</span> Thus, in expectation, we need to run the algorithm for <spanclass="math inline">\(\ell\)</span> epochs, where: <spanclass="math display">\[\ell = \log_2\left(\frac{g(y_0) - g(y^*)}{\epsilon^2/(4\kappa)}\right)\]</span></p><p>We upper bound <span class="math inline">\(\|g(y_0) -g(y^*)\|\)</span> by the maximum value of the objective function <spanclass="math inline">\(g\)</span>.</p><p>Denote <span class="math inline">\(\psi(G) = \max\{\Delta^+(G),  \Delta^-(G)\}\)</span>, where <spanclass="math inline">\(\Delta^+(G)\)</span> and <spanclass="math inline">\(\Delta^-(G)\)</span> denote the maximum outdegreeand indegree of directed graph <span class="math inline">\(G\)</span>,respectively</p><p>We can show that <span class="math inline">\(g(y) = \sqrt c\sum_{u\in V}\left(\sum_{(u,v) \in E}\alpha_{u,v}\right)^2 +\frac{1}{\sqrt{c}} \sum_{v\in V}\left(\sum_{(u,v) \inE}\beta_{v,u}\right)^2\)</span> is upper bounded by <spanclass="math inline">\(\psi(G)\cdot |E| \cdot \kappa\)</span></p><p>Thus, we have: <span class="math display">\[\ell =\log_2\left(\frac{\psi(G)\cdot |E|}{\epsilon^2}\right)\]</span></p><p>So we need to run the algorithm in iterations of <spanclass="math display">\[T \cdot \ell = 12n\sqrt{m\kappa} \log_2\left(\frac{\psi(G)\cdot|E|}{\epsilon^2}\right)\]</span></p><p>In terms of asymptotic complexity, this gives us: <spanclass="math display">\[\mathcal{O}\left(n\sqrt {m\kappa} \log\left(\frac{\psi(G)\cdotm}{\epsilon^2}\right)\right)\]</span> to solve a c-DDS subproblem.</p><p>The set of C is defined as <span class="math display">\[C=\{\frac ab | 1\leq a,b \leq n, a,b \in \mathbb{Z}^+\}\]</span></p><p>For the DDS problem, define <span class="math inline">\(\Phi =\sum_{c\in C} \max\{c^{\frac 14},c^{-\frac 14}\}\)</span></p><p>Then the total complexity of solving all c-DDS subproblems is: <spanclass="math display">\[\mathcal{O}\left(\Phi n\sqrt {m} \log\left(\frac{\psi(G)\cdotm}{\epsilon^2}\right)\right)\]</span></p>]]></content>
    
    
    <categories>
      
      <category>Mathematics</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Optimization</tag>
      
      <tag>Coordinate Descent</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>子模函数以及Lovász拓展</title>
    <link href="/2025/06/11/%E5%AD%90%E6%A8%A1%E5%87%BD%E6%95%B0%E4%BB%A5%E5%8F%8ALovasz%E6%8B%93%E5%B1%95/"/>
    <url>/2025/06/11/%E5%AD%90%E6%A8%A1%E5%87%BD%E6%95%B0%E4%BB%A5%E5%8F%8ALovasz%E6%8B%93%E5%B1%95/</url>
    
    <content type="html"><![CDATA[<p>参考文献:</p><p><a href="https://en.wikipedia.org/wiki/Submodular_function">Wikipedia- Submodular function</a></p><p><ahref="https://www.cs.princeton.edu/~hy2/teaching/fall22-cos521/notes/SFM.pdf">Lecture7: Submodular Functions, Lovász Extension and Minimization</a></p><p><a href="https://arxiv.org/pdf/1111.6453">Learning with SubmodularFunctions: A Convex Optimization Perspective</a></p><h2 id="子模函数-submodular-function">子模函数 (SubmodularFunction)</h2><p>一个函数 <span class="math inline">\(f: 2^N \to \mathbb{R}\)</span>被称为子模函数，如果对于任意的 <span class="math inline">\(A \subseteq B\subseteq N\)</span> 和 <span class="math inline">\(x \in N \setminusB\)</span>，都有以下不等式成立：</p><p><span class="math display">\[f(A \cup \{x\}) - f(A) \geq f(B \cup \{x\}) - f(B)\]</span></p><p>一个等价的定义是，如果对于任意的 <span class="math inline">\(A\subseteq B\)</span> 和 <span class="math inline">\(x \in N \setminusB\)</span>，都有：</p><p><span class="math display">\[f(A \cup \{x\}) - f(A) \geq f(B \cup \{x\}) - f(B)\]</span></p><p>第一个不等式可以被解释为“边际收益递减”，即添加一个元素到集合 <spanclass="math inline">\(A\)</span>的收益大于或等于添加同样的元素到更大的集合 <spanclass="math inline">\(B\)</span> 的收益。</p><p>几个常见的子模函数示例包括：</p><ul><li>令图 <span class="math inline">\(G = (V, E)\)</span>，定义函数 <spanclass="math inline">\(f(S) = |E(S, \bar S)|\)</span>，则<spanclass="math inline">\(f\)</span> 是子模的，其中 <spanclass="math inline">\(E(S, \bar S)\)</span> 表示集合 <spanclass="math inline">\(S\)</span> 和其补集 <spanclass="math inline">\(\bar S\)</span> 之间的边集。</li><li>令图 <span class="math inline">\(G = (U, V, E)\)</span>为一个二分图，<span class="math inline">\(|U| = n\)</span>。 对于所有<span class="math inline">\(S \subseteq U\)</span>，定义 <spanclass="math inline">\(f(S) = |N(S)|\)</span>，其中 <spanclass="math inline">\(N(S)\)</span> 是 <spanclass="math inline">\(S\)</span> 的邻居节点集合，则 <spanclass="math inline">\(f\)</span> 是子模的。<spanclass="math inline">\(f\)</span>也是单调的。</li><li>令 <span class="math inline">\(x_1,x_2,\cdots, x_n\)</span> 是 <spanclass="math inline">\(n\)</span> 个离散随机变量。对任意 <spanclass="math inline">\(A \subseteq [n]\)</span>，定义 <spanclass="math inline">\(f(A) = H(X_A)\)</span>，其中 <spanclass="math inline">\(H(X_A)\)</span> 是 <spanclass="math inline">\(X_A\)</span> 的熵，则 <spanclass="math inline">\(f\)</span> 是子模的。</li></ul><h2 id="lovász拓展-lovász-extension">Lovász拓展 (Lovász Extension)</h2><blockquote><p>In a word, the Lovasz extension is a weighted average of the valuesat the corners.</p></blockquote><p>对于函数 <span class="math inline">\(f: 2^N \to\mathbb{R}\)</span>，其Lovász拓展定义为：</p><p><span class="math display">\[\begin{align}\hat{f}(x) = \mathbb{E}_{\lambda \sim \text{Uniform}(0,1)}[f(\{i \in N :x_i \geq \lambda\})]\end{align}\]</span></p><p>其中 <span class="math inline">\(x = (x_1, x_2, \ldots, x_n)\in\mathbb{R}^n\)</span>。</p><p>Lovász拓展被称为拓展，因为它保留了离散点集上的函数值。注意到，对于离散点<spanclass="math inline">\(z\in \{0,1\}^n, \lambda \in [0,1]\)</span>，都有：<span class="math inline">\(\{ i | z_i\geq \lambda\}= \{ i | z_i = 1 \} = S\)</span>。所以 <span class="math inline">\(\hatf\)</span> 与 <span class="math inline">\(f\)</span>在离散点集上是相同的。</p><h3 id="等价的构造定义">等价的构造定义</h3><p>将向量 <span class="math inline">\(x\)</span> 的分量按降序重新排列为<span class="math inline">\(x_{\sigma(1)} \geq x_{\sigma(2)} \geq \ldots\geq x_{\sigma(n)}\)</span>，然后定义集合序列 <spanclass="math inline">\(S_i = \{ \sigma(1), \sigma(2), \ldots, \sigma(i)\} = \{i \in N : x_i \geq x_{\sigma(i)}\}\)</span>。</p><p>则：</p><p><span class="math display">\[\begin{align}\hat{f}(x) = \sum_{i=1}^n f(S_{i}) \cdot (x_{\sigma(i)} -x_{\sigma(i+1)})\end{align}\]</span></p><p>并约定 <span class="math inline">\(x_{\sigma(n+1)} = 0\)</span>。</p><p>这个定义可以看作是对函数 <span class="math inline">\(f\)</span>在每个集合 <span class="math inline">\(S_i\)</span>上的值进行加权平均，其中权重是相邻元素之间的差值。</p><p>对于子模函数 <spanclass="math inline">\(f\)</span>，另一个有用的定义是，令 <spanclass="math inline">\(B(f)\)</span> 为 <spanclass="math inline">\(f\)</span> 的拟阵多面体（polymatroid）</p><p><span class="math display">\[B(f) = \{ w\in \mathbb{R}^n : w(N) = f(N), w(A) \leq f(A) \text{ for all} A \subseteq N \}\]</span></p><p>则</p><p><span class="math display">\[\begin{align}\hat{f}(x) = \max_{y \in B(f)} \langle x, y \rangle\end{align}\]</span></p><h3 id="等价性证明">等价性证明</h3><p><span class="math inline">\((1)\iff (2)\)</span></p><p><span class="math display">\[\begin{align*}\hat{f}(x) &amp;= \mathbb{E}_{\lambda \sim \text{Uniform}(0,1)}[f(\{i\in N : x_i \geq \lambda\})] \\&amp;= \int_0^1 f(\{i \in N : x_i \geq \lambda\}) d\lambda \\\end{align*}\]</span> 按 <span class="math inline">\(\{x_{\sigma(i)}\}\)</span>分段积分： <span class="math display">\[\begin{align*}\hat{f}(x) &amp;= \int_{\sigma(1)}^1 f(\{j \in N : x_j \geq \lambda\})d\lambda + \sum_{i=1}^{n} \int_{x_{\sigma(i+1)}}^{x_{\sigma(i)}} f(\{j\in N : x_j \geq \lambda\}) \, d\lambda \\&amp;= 0 + \sum_{i=0}^{n} f(\{j \in N : x_j \geq x_{\sigma(i)}\}) \cdot(x_{\sigma(i)} - x_{\sigma(i+1)}) \\&amp;= \sum_{i=1}^{n} f(\{j \in N : x_j \geq x_{\sigma(i)}\}) \cdot(x_{\sigma(i)} - x_{\sigma(i+1)}) \\&amp;= \sum_{i=1}^{n} f(S_i) \cdot (x_{\sigma(i)} - x_{\sigma(i+1)})\end{align*}\]</span></p><p><span class="math inline">\((2) \iff (3)\)</span></p><p><strong>引理：</strong></p><p>对于子模函数 <span class="math inline">\(f\)</span>，其拟阵多面体<span class="math inline">\(B(f)\)</span> 的顶点一一对应于排列 <spanclass="math inline">\(\pi\)</span> <span class="math display">\[\begin{align*}w_i^{\pi} = f(\{j \in N : \sigma^{-1}(j) \leq \sigma^{-1}(i)\}) - f(\{j\in N : \sigma^{-1}(j) &lt; \sigma^{-1}(i)\})\end{align*}\]</span></p><p>证明：</p><p><span class="math inline">\(w^{\pi}(N)=f(N)\)</span>是平凡的</p><p>对于任意的 <span class="math inline">\(A \subseteqN\)</span>，考虑一个排列 <spanclass="math inline">\(\tau\)</span>，它首先列出 <spanclass="math inline">\(A\)</span> 中的元素（按某种顺序），然后列出 <spanclass="math inline">\(N \setminus A\)</span> 中的元素。</p><p>对于这个排列 <span class="math inline">\(\tau\)</span>，我们有：<span class="math display">\[\begin{align*}w^{\tau}(A) &amp;= f(A) - f(\emptyset) \\&amp;= f(A)\end{align*}\]</span></p><p>现在考虑任意的排列 <spanclass="math inline">\(\pi\)</span>，我们可以证明如果某个 <spanclass="math inline">\(a\notin A\)</span> 出现在 <spanclass="math inline">\(b\in A\)</span> 的前面，则交换它们的位置不会减少<span class="math inline">\(w(A)\)</span> 的值。</p><p>原来 <span class="math inline">\(b\)</span> 的贡献是 <spanclass="math inline">\(f(S\cup \{a,b\}) - f(S\cup \{a\})\)</span>，交换后变为 <span class="math inline">\(f(S\cup {b}) -f(S)\)</span>。</p><p>而子模函数性质告诉我们，<span class="math inline">\(f(S\cup \{a,b\})- f(S\cup \{a\}) \geq f(S\cup \{b\}) - f(S)\)</span>。</p><p>因此，<span class="math inline">\(w^{\tau}(A)\)</span> 在所有排列<span class="math inline">\(\pi\)</span> 中是最大的。</p><p>所以<span class="math inline">\(w^{\pi}(A) \leq f(A), \forall\pi\)</span></p><p>另外，注意到 <span class="math inline">\(\forall \sigma, \forall1\leq k \leq n, w^\sigma(\{\sigma(1), \sigma(2), \ldots, \sigma(k)\}) =f(\{\sigma(1), \sigma(2), \ldots, \sigma(k)\})\)</span>。</p><p>所以 <span class="math inline">\(w^{\sigma}\)</span> 满足 <spanclass="math inline">\(n\)</span> 个线性无关约束，故 <spanclass="math inline">\(w^{\sigma}\)</span> 是 <spanclass="math inline">\(B(f)\)</span> 的顶点。</p><p><span class="math inline">\(\square\)</span></p><p>现在证明 <span class="math inline">\((2) \iff (3)\)</span>：</p><p>不难知道最优解 <span class="math inline">\(w^*\)</span>一定是对应排列 <span class="math inline">\(\sigma\)</span> 的顶点。</p><p>注意到 <span class="math inline">\(\{j| j \in N : \sigma^{-1}(j) \leqi\} = \{j| x_j \geq x_{\sigma(i)}\}\)</span></p><p>我们有：</p><p><span class="math display">\[\begin{align*}\hat{f}(x) &amp;= \max_{y \in B(f)} \langle x, y \rangle \\&amp;= \max_{\pi} \langle x, w^{\pi} \rangle \\&amp;= \sum_{i=1}^{n} x_{\sigma(i)} (f(\{j \in N : \sigma^{-1}(j) \leqi\}) - f(\{j \in N : \sigma^{-1}(j) &lt; i\})) \\&amp;= \sum_{i=1}^{n} x_{\sigma(i)} (f(\{j \in N : x_j \geqx_{\sigma(i)}\}) - f(\{j \in N : x_j &gt; x_{\sigma(i)}\})) \\&amp;= \sum_{i=1}^{n} f(\{j \in N : x_j \geq x_{\sigma(i)}\})(x_{\sigma(i)} - x_{\sigma(i+1)}) \\&amp;= \sum_{i=1}^{n} f(S_i) (x_{\sigma(i)} - x_{\sigma(i+1)}) \\\end{align*}\]</span></p><h3 id="lovász拓展在子模函数上的性质">Lovász拓展在子模函数上的性质</h3><p>我们有如下定理：</p><p><strong>定理</strong>：对于函数 <span class="math inline">\(f: 2^N\to \mathbb{R}\)</span>，<span class="math inline">\(f\)</span>是子模函数当且仅当其 Lovász 拓展 <spanclass="math inline">\(\hat{f}\)</span> 是凸函数。</p><p>充分性：如果 <span class="math inline">\(f\)</span>是子模函数，则对于任意的 <span class="math inline">\(x, y \in\mathbb{R}^n\)</span> 和 <span class="math inline">\(\theta \in [0,1]\)</span>，都有： <span class="math display">\[\hat{f}(\theta x + (1 - \theta) y) \leq \theta \hat{f}(x) + (1 - \theta)\hat{f}(y)\]</span> 证明：由 Lovász 拓展的定义，我们有： <spanclass="math display">\[\begin{align*}\hat{f}(\theta x + (1 - \theta) y) &amp;= \mathbb{E}_{\lambda \sim\text{Uniform}(0,1)}[f(\{i \in N : (\theta x + (1 - \theta) y)_i \geq\lambda\})] \\&amp;= \mathbb{E}_{\lambda \sim \text{Uniform}(0,1)}[f(\{i \in N :\theta x_i + (1 - \theta) y_i \geq \lambda\})] \\&amp;\leq \mathbb{E}_{\lambda \sim \text{Uniform}(0,1)}[\theta f(\{i \inN : x_i \geq \lambda\}) + (1 - \theta) f(\{i \in N : y_i \geq\lambda\})] \\&amp;= \theta \mathbb{E}_{\lambda \sim \text{Uniform}(0,1)}[f(\{i \in N: x_i \geq \lambda\})] + (1 - \theta) \mathbb{E}_{\lambda \sim\text{Uniform}(0,1)}[f(\{i \in N : y_i \geq \lambda\})] \\&amp;= \theta \hat{f}(x) + (1 - \theta) \hat{f}(y)\end{align*}\]</span> 因此，<span class="math inline">\(\hat{f}\)</span>是凸函数。</p><p>必要性：如果 <span class="math inline">\(\hat{f}\)</span>是凸函数，则对于任意的 <span class="math inline">\(A \subseteqB\)</span> 和 <span class="math inline">\(x \in N \setminusB\)</span>，（这里将<span class="math inline">\(f\)</span>视作<spanclass="math inline">\(\hat f\)</span>限制到<spanclass="math inline">\(\{0,1\}^N\)</span>上）</p><p>都有： <span class="math display">\[f(A \cup \{x\}) - f(A) \geq f(B \cup \{x\}) - f(B)\]</span> 证明：</p><p>令 <span class="math inline">\(A&#39; = A \cup \{x\}\)</span> 和<span class="math inline">\(B&#39; = B \cup \{x\}\)</span></p><p><span class="math display">\[\mathbb{1}_{A&#39;}+\mathbb{1}_{B} = \mathbb{1}_{A} +\mathbb{1}_{B&#39;}\]</span> 因此， <span class="math display">\[\begin{align*}\hat{f}(\frac 12 \mathbb{1}_{A&#39;} +\frac 12\mathbb{1}_{B}) &amp;\leq\frac 12 \hat{f}(\mathbb{1}_{A&#39;}) + \frac 12 \hat{f}(\mathbb{1}_{B})\\\iff \hat{f}(\frac 12 \mathbb{1}_{A} +\frac 12\mathbb{1}_{B&#39;})&amp;\leq \frac 12 f(A&#39;) + \frac 12 f(B) \\\end{align*}\]</span></p><p>现在我们用Lovász拓展的定义来计算左侧，设 <spanclass="math inline">\(z = \frac 12 \mathbb{1}_{A} +\frac12\mathbb{1}_{B&#39;}\)</span>，则： - 对于 <spanclass="math inline">\(i \in A\)</span>，<span class="math inline">\(z_i= 1\)</span> - 对于 <span class="math inline">\(i \in (B\setminus A)\cup\{x\}\)</span>，<span class="math inline">\(z_i = 1/2\)</span> -其他情况， <span class="math inline">\(z_i = 0\)</span></p><p>所以 <span class="math display">\[\begin{align*}\hat{f}(\frac 12 \mathbb{1}_{A} +\frac 12\mathbb{1}_{B&#39;}) &amp;=\int_0^{\frac 12} f(\{i \in N : z_i \geq \lambda\}) d\lambda +\int_{\frac 12}^1 f(\{i \in N : z_i \geq \lambda\}) d\lambda \\&amp;= \int_0^{\frac 12} f(B&#39;) d\lambda + \int_{\frac 12}^1 f(A)d\lambda \\&amp;= \frac 12 f(B&#39;) + \frac 12 f(A) \\\end{align*}\]</span></p><p>因此，我们得到了： <span class="math display">\[\begin{align*}\frac 12 f(B&#39;) + \frac 12 f(A) &amp;\leq \frac 12 f(A&#39;) + \frac12 f(B)\\\iff f(A \cup \{x\}) - f(A) &amp;\geq f(B \cup \{x\}) - f(B)\end{align*}\]</span> 这就证明了必要性。</p>]]></content>
    
    
    <categories>
      
      <category>Mathematics</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Optimization</tag>
      
      <tag>Submodular Functions</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>机器学习中常见的损失函数</title>
    <link href="/2025/06/10/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%AD%E5%B8%B8%E8%A7%81%E7%9A%84%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/"/>
    <url>/2025/06/10/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%AD%E5%B8%B8%E8%A7%81%E7%9A%84%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/</url>
    
    <content type="html"><![CDATA[<h2 id="回归任务损失函数regression-losses">回归任务损失函数(RegressionLosses)</h2><h3 id="均方误差mse-mean-squared-error">均方误差(MSE, Mean SquaredError)</h3><p>均方误差是回归任务中最常用的损失函数之一。它计算预测值与实际值之间差异的平方和的平均值。公式如下：</p><p><span class="math display">\[\text{MSE} = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2\]</span></p><p>其中，<span class="math inline">\(y_i\)</span> 是实际值，<spanclass="math inline">\(\hat{y}_i\)</span> 是预测值，<spanclass="math inline">\(n\)</span> 是样本数量。</p><p>MSE的优点是对大误差有较强的惩罚作用，因为误差被平方了。这使得模型在训练时更倾向于减少大误差。然而，MSE 对异常值非常敏感，因为异常值的平方会显著增加总损失。</p><h3 id="平均绝对误差mae-mean-absolute-error">平均绝对误差(MAE, MeanAbsolute Error)</h3><p>平均绝对误差是另一种常用的回归损失函数。它计算预测值与实际值之间差异的绝对值的平均值。公式如下：</p><p><span class="math display">\[\text{MAE} = \frac{1}{n} \sum_{i=1}^{n} |y_i - \hat{y}_i|\]</span></p><p>MAE 的优点是对异常值不如 MSE 敏感，因此在数据中存在异常值时，MAE可能更合适。然而，MAE在梯度下降时可能导致不稳定的梯度，因为绝对值函数在零点处不可导。</p><h3 id="huber-损失函数">Huber 损失函数</h3><p>Huber 损失函数结合了 MSE 和 MAE 的优点。它在误差小于某个阈值时使用MSE，在误差大于该阈值时使用 MAE。公式如下：</p><p><span class="math display">\[\text{Huber} =\begin{cases}\frac{1}{2}(y_i - \hat{y}_i)^2 &amp; \text{if } |y_i - \hat{y}_i| \leq\delta \\\delta \cdot (|y_i - \hat{y}_i| - \frac{1}{2}\delta) &amp;\text{otherwise}\end{cases}\]</span></p><p>其中，<span class="math inline">\(\delta\)</span>是一个超参数，用于控制 MSE 和 MAE 的切换点。 Huber损失函数在处理异常值时表现良好，因为它在误差较大时不会像 MSE那样过于敏感。</p><h2id="分类任务损失函数classification-losses">分类任务损失函数(ClassificationLosses)</h2><h3 id="交叉熵损失cross-entropy-loss">交叉熵损失(Cross-EntropyLoss)</h3><p>交叉熵损失是分类任务中最常用的损失函数。它衡量了预测分布与实际分布之间的差异。对于二分类问题，交叉熵损失的公式如下：</p><p><span class="math display">\[\text{Cross-Entropy} = -\frac{1}{n} \sum_{i=1}^{n} [y_i \log(\hat{y}_i)+ (1 - y_i) \log(1 - \hat{y}_i)]\]</span></p><p>对于多分类问题，交叉熵损失的公式为：</p><p><span class="math display">\[\text{Cross-Entropy} = -\frac{1}{n} \sum_{i=1}^{n} \sum_{j=1}^{k} y_{ij}\log(\hat{y}_{ij})\]</span></p><p>其中，<span class="math inline">\(y_i\)</span> 是实际标签，<spanclass="math inline">\(\hat{y}_i\)</span> 是预测概率，<spanclass="math inline">\(k\)</span> 是类别数量。交叉熵损失的优点是它对概率分布的差异非常敏感，因此在训练分类模型时效果很好。它也具有良好的数学性质，使得梯度下降算法能够有效地优化模型参数。</p><h3 id="二元交叉熵损失binary-cross-entropy-loss">二元交叉熵损失(BinaryCross-Entropy Loss)</h3><p>二元交叉熵损失是交叉熵损失的特例，专门用于二分类问题。它的公式与上述二分类交叉熵损失相同，但通常使用Sigmoid 函数将输出转换为概率值。</p><h3id="类别平衡交叉熵损失class-balanced-cross-entropy-loss">类别平衡交叉熵损失(Class-BalancedCross-Entropy Loss)</h3><p>类别平衡交叉熵损失是一种改进的交叉熵损失，旨在处理类别不平衡问题。它通过为每个类别分配不同的权重来平衡损失。公式如下：</p><p><span class="math display">\[\text{Class-Balanced Cross-Entropy} = -\frac{1}{n} \sum_{i=1}^{n}w_{y_i} \log(\hat{y}_{i})\]</span></p><p>其中，<span class="math inline">\(w_{y_i}\)</span> 是类别 <spanclass="math inline">\(y_i\)</span>的权重，通常根据类别的频率进行计算。这样可以减少对少数类的忽视，提高模型在不平衡数据集上的性能。</p><h3 id="focal-loss">Focal Loss</h3><p>Focal Loss是一种用于处理类别不平衡问题的损失函数。它在交叉熵损失的基础上引入了一个调节因子，使得模型更关注难以分类的样本。公式如下：</p><p><span class="math display">\[\text{Focal Loss} = -\frac{1}{n} \sum_{i=1}^{n} (1 - \hat{y}_i)^\gammay_i \log(\hat{y}_i)\]</span></p><p>其中，<span class="math inline">\(\gamma\)</span>是一个超参数，用于控制调节因子的强度。当 <spanclass="math inline">\(\gamma = 0\)</span> 时，Focal Loss等同于交叉熵损失。随着 <span class="math inline">\(\gamma\)</span>的增大，模型对难以分类的样本的关注程度增加。</p><h3 id="kullback-leibler-散度kl-divergence">Kullback-Leibler 散度(KLDivergence)</h3><p>Kullback-Leibler散度是一种衡量两个概率分布之间差异的损失函数。它通常用于变分自编码器等模型中。公式如下：</p><p><span class="math display">\[\text{KL Divergence} = \sum_{i=1}^{n} y_i\log\left(\frac{y_i}{\hat{y}_i}\right)\]</span></p><p>KL散度的优点是它可以处理连续分布，并且在概率分布之间的差异较大时具有较强的惩罚作用。然而，它对零概率事件非常敏感，因此在实际应用中需要小心处理。</p><h2 id="其他常见损失函数">其他常见损失函数</h2><h3 id="wasserstein距离">Wasserstein距离</h3><p>Wasserstein距离是一种衡量两个概率分布之间差异的距离度量。它基于最优传输理论，能够更好地捕捉分布之间的差异，尤其是在高维空间中。</p><p>Wasserstein 距离的定义如下:</p><p><span class="math display">\[\text{Wasserstein\_Distance}(P, Q) = \inf_{\gamma \in \Gamma(P, Q)}\int_{X \times X} c(x, y) \operatorname{d}\gamma(x, y)\]</span></p><p>其中，<span class="math inline">\(P\)</span> 和 <spanclass="math inline">\(Q\)</span> 是两个概率分布，<spanclass="math inline">\(\Gamma(P, Q)\)</span> 是所有可能的联合分布，<spanclass="math inline">\(c(x, y)\)</span> 是成本函数，表示从 <spanclass="math inline">\(x\)</span> 到 <spanclass="math inline">\(y\)</span> 的传输成本。 Wasserstein距离的优点是它具有更好的数学性质，能够处理分布之间的细微差异，并且在高维空间中表现良好。</p><p>而且相比于KL散度，Wasserstein是真正的距离度量，满足三角不等式和对称性。</p><h3 id="wasserstein-损失函数wasserstein-loss">Wasserstein损失函数(Wasserstein Loss)</h3><p>Wasserstein损失函数是一种用于生成对抗网络(GAN)的损失函数。它基于Wasserstein距离，能够更好地处理生成模型中的模式崩溃问题。公式如下：</p><p><span class="math display">\[\text{Wasserstein Loss} = \mathbb{E}[D(x)] - \mathbb{E}[D(G(z))]\]</span></p><p>其中，<span class="math inline">\(D(x)\)</span>是判别器对真实样本的评分，<span class="math inline">\(G(z)\)</span>是生成器生成的样本。Wasserstein损失函数的优点是它具有更好的梯度性质，使得 GAN 的训练更加稳定。</p>]]></content>
    
    
    <categories>
      
      <category>Machine Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Loss Functions</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>测试文章</title>
    <link href="/2025/06/07/%E6%B5%8B%E8%AF%95%E6%96%87%E7%AB%A0/"/>
    <url>/2025/06/07/%E6%B5%8B%E8%AF%95%E6%96%87%E7%AB%A0/</url>
    
    <content type="html"><![CDATA[]]></content>
    
    
    <categories>
      
      <category>Other</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Test</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>安瓦尔·谢克《资本主义》读书笔记</title>
    <link href="/2025/06/07/%E5%AE%89%E7%93%A6%E5%B0%94%C2%B7%E8%B0%A2%E5%85%8B%E3%80%8A%E8%B5%84%E6%9C%AC%E4%B8%BB%E4%B9%89%E3%80%8B/"/>
    <url>/2025/06/07/%E5%AE%89%E7%93%A6%E5%B0%94%C2%B7%E8%B0%A2%E5%85%8B%E3%80%8A%E8%B5%84%E6%9C%AC%E4%B8%BB%E4%B9%89%E3%80%8B/</url>
    
    <content type="html"><![CDATA[<h2 id="前言">前言</h2><p>这本书主要回答了几个重要的问题：</p><p>1.如何解释1940年前后巨大的差异：在战前表现出价格持续的涨跌波动。而在战后时期，物价水平呈现出新的变动模式，即永无止歇的增长。以及解释用黄金价格修正的长波规律。——这个是贯穿全文的主线，从第二章开始讨论至最后一章。</p><p>2.现代宏观经济学的体系建立在一些既不现实也不必要的假设之下，例如边际效率递减，超理性假设。如何从真实竞争视角，或者从涌现特质来推导常见的经验法则，例如斜向下的需求曲线。</p><p>3.为什么新自由主义没有实现它所宣称的充分就业和竞争力均等化？为什么国际贸易失衡不会自动消除？如果自由贸易会带来持续的贸易不平衡，收支平衡何以得以保持（美元双循环理论）？以及实际有效汇率的决定。</p><p>4.菲利普斯曲线的有效性为什么昙花一现？以及对菲利普斯初始问题的三种回答——三种新的菲利普斯曲线。</p><p>5.是什么决定了整体利润率的变化路径？为什么利润率会下降，下降又是怎么被逆转的。哪些是结构性的，哪些是周期性的？经济危机和信贷泡沫为何在巨大的世界变化下不受影响地周期性的上演？</p><p>这本书秉持了三个重要的思路：一、真实竞争，而非完全竞争。二、对利润率的追逐与套利动机。三、与经验证据的符合。</p><h3 id="对问题一的理解">对问题一的理解</h3><p>谢克从供求的视角切入，分为两部分，这里略过了前面所有的理论基础（例如工资份额上升如何导致利润率下降，最大增长率和生产矩阵等）：</p><p>一、以信用为动力的需求拉动。</p><p>二、以利润率为基础的供给抵抗。</p><p>1.法定货币，以及更进一步的现代货币理论（MMT）将国家从技术约束中解放出来，可以创造无限的信贷。</p><p>2.部分进行商品支出的新增国内信贷构成了新增购买力。（涉及新增信贷的效率，空转问题）</p><p>3.商品市场上的总超额需求一般来自直接进行商品支出的新增国外信贷，加上对外部门的经常项目余额</p><p>4.超额需求可以在一定界限内提高增长率，这个界限来自利润率可能产生的负反馈效应。新增的需求将在剩余率最低的行业造成瓶颈，并通过涨价的方式改变经济系统的构成提高其增长潜力，从而改变瓶颈部门的实物剩余率。但是随着系统逐渐接近最大可能的利润率，更多的产出项目将达到瓶颈。而这个过程因为失去了商品货币的锚定，只能以普遍的通货膨胀作为其润滑。换句话说，当实际增长率接近理论最大值时，瓶颈会越来越频繁地出现。</p><p>“设当有效需求量再增加时，已无增加产量之作用，仅使成本单位随有效需求作同比例上涨，此种情况，可称之为真正的通货膨胀。到这点为止，货币膨胀之效果，只是程度问题，在该点以前，我们找不出一点可以划一条清楚界线，宣称现在已到通货膨胀之境。因为在该点以前货币数量每增加一次……其作用一部分在提高单位成本，一部分在增加产量。”（Keynes 1964，ch.21，303）</p><h3 id="现实的危机">现实的危机</h3><p>世界不是凯恩斯主义者的理想国，因为国家不是阶层中立的，它无力累进加税以平衡收支，因此经济政策对政治的服从必然导致货币政策的滥用。新自由主义通过无风险利率的降低来提高净回报率，通过压低工资份额来弥补技术进步引起的利润率下降，通过巨大的信贷倾泻来强撑金融大厦极限的繁荣。对大企业的保护，压低失业率并提高经济增长的政治诉求驱使着Fed，BOJ等央行在近0利率下，放任了信贷的腐烂增长与危险金融衍生品的发展。</p><p>而在巨大的信贷压顶下，各国央行只能在这个道路上一条道走到黑，绝不能停止债券的购买，一旦量化宽松，就难以回头。</p><h1 id="读书笔记">读书笔记</h1><h2 id="第二章-动荡中的趋势与被掩盖的结构">第二章动荡中的趋势与被掩盖的结构</h2><p>作者首先介绍了一些基于经验的资本主义重要特征：</p><h3 id="动荡中的增长">2.1 动荡中的增长</h3><ul><li>名义价格计价下的指数增长</li><li>增长的动荡和周期性</li><li>战争开始与经济上行以及战争结束与经济下滑之间的关联</li></ul><figure><img src="图2.1.png" alt="图2.1 美国工业生产指数（1860—2010年）" /><figcaption aria-hidden="true">图2.1美国工业生产指数（1860—2010年）</figcaption></figure><figure><img src="图2.2.png" alt="图2.2 美国实际投资指数（1832—2010年）" /><figcaption aria-hidden="true">图2.2美国实际投资指数（1832—2010年）</figcaption></figure><figure><img src="图2.3.png" alt="图2.3 美国实际工资指数（1832—2010年）" /><figcaption aria-hidden="true">图2.3美国实际工资指数（1832—2010年）</figcaption></figure><figure><img src="图2.4C.png" alt="图2.4C 商业周期（1903—1939年）" /><figcaption aria-hidden="true">图2.4C商业周期（1903—1939年）</figcaption></figure><h3 id="生产率实际工资和实际单位劳动力成本">2.2生产率、实际工资和实际单位劳动力成本</h3><ul><li>生产率的长期稳定增长</li><li>实际工资和生产率在1889-1970年间的同步增长，以及1970年后实际工资的停滞</li><li>实际单位劳动力成本复杂多变的趋势</li></ul><figure><img src="图2.5.png"alt="图2.5 美国制造业生产率和生产工人实际工资指数（1889—2010年，1889=100）" /><figcaption aria-hidden="true">图2.5美国制造业生产率和生产工人实际工资指数（1889—2010年，1889=100）</figcaption></figure><figure><img src="图2.6.png"alt="图2.6 美国制造业实际单位生产劳动力成本指数（1889—2010年）" /><figcaption aria-hidden="true">图2.6美国制造业实际单位生产劳动力成本指数（1889—2010年）</figcaption></figure><blockquote><p>图2.5清楚地表明，在20世纪80年代初，因里根政府对劳动力实施打击政策，加之来自外国企业的竞争不断加剧，美国制造业工人遭受了引人注目、一直持续至今的实际工资增长停滞。</p></blockquote><blockquote><p>生产率的增长为实际工资的潜在增长、因而每个工人实际消费的潜在增长提供了物质基础。但是生产力水平的提高不会自动带来实际工资的增长。二者之间的联系需要通过某些社会机制和制度机制建立（这往往来之不易），而这些关联又总是存在被打破的可能。</p></blockquote><p>这里谢克暗示了劳资斗争和阶级斗争在分配中起到的作用。</p><h3 id="失业率">2.3 失业率</h3><ul><li>失业率的周期性，以及政府在抑制萧条方面采取的措施和其影响</li></ul><figure><img src="图2.7.png" alt="图2.7 美国的失业率（1890—2010年）" /><figcaption aria-hidden="true">图2.7美国的失业率（1890—2010年）</figcaption></figure><blockquote><p>……失业率的峰值相较前两次大萧条低得多，其平均水平也仅仅达到了此前两次大萧条的2/3。这提醒我们，经济政策和社会结构可能会在改善失业方面发挥巨大的积极作用。问题在于，这需要付出多少成本或者会带来怎样意料之外的结果。</p></blockquote><blockquote><p>我将论证，抑制萧条的一个结果是延长了萧条的持续时间：抑制萧条也可能抑制经济复苏，日本在20世纪后30多年的经历便证明了这一点。尽管如此，这并不意味着严重的萧条比长期的经济停滞好。在这两种情况下，劳动者和资本家付出的代价是不同的，社会制度在决定这种代价如何在二者之间进行分摊时起到重要作用。</p></blockquote><p>即使政府对失业率采取的抑制措施会导致经济复苏的延迟，但在目前的社会制度下还是有必要的，因为它可以把剧烈的经济萧条的代价变为长期的经济停滞的代价，而这阻止了暴烈的社会动荡。</p><h3 id="价格通货膨胀和黄金价格长波">2.4价格、通货膨胀和黄金价格长波</h3><ul><li>“通货膨胀”只是一个现代现象</li></ul><figure><img src="图2.8.png"alt="图2.8 美国和英国的批发价格指数（1780—2010年，1930=100，对数值）" /><figcaption aria-hidden="true">图2.8美国和英国的批发价格指数（1780—2010年，1930=100，对数值）</figcaption></figure><blockquote><p>在1780—1940年这超过一个半世纪的时间里，价格变化呈现出明显的长期摇摆，但没有一个总的趋势。正是这种波形特征奠定了“长期波动”的概念（见第5章）。但是，在1940年之后，价格不停上涨。显然，我们必须对价格走势的这一根本性改变做出解释（见第15章）。1940年前后两种模式间的比较还引发了第三个问题。在1940年之前，我们不但有价格的长期波动，还有与价格下跌阶段相伴的大萧条。但是，在1940年之后，价格的长期波动似乎已经完全消失，而20世纪70—80年代的大滞胀与价格下跌绝对没有任何关系。</p></blockquote><ul><li>用黄金表示的价格指数的有效周期</li></ul><figure><img src="图2.10.png"alt="图2.10 美国和英国用盎司黄金表示的批发价格指数（1780◎—2010年，1930=100，对数值）" /><figcaption aria-hidden="true">图2.10美国和英国用盎司黄金表示的批发价格指数（1780◎—2010年，1930=100，对数值）</figcaption></figure><blockquote><p>所以，大萧条和长期价格波动之间的联系似乎在1940年左右的某个时间点被不可逆转地斩断了。真的是这样吗？有必要指出的是，一件商品的价格就是其市场价值在另一种东西上的表现，而这另一种东西便是社会公认的“货币”。但货币不是一个单一的物品，它具有一系列同时存在但处于不同层次的表现形式：依赖于特定银行健康状况的信用货币；依赖于特定国家政府健康状况的国家通货；以及像黄金这样可广泛交换的商品，其官方或非官方地位有赖于全球商品流通的健康状况。</p></blockquote><blockquote><p>这些不同的货币形式产生于商品生产本身，并被国家采用和调整。货币的各种不同形式之间的竞争通过彼此之间的兑换比率表达出来。当发生银行挤兑时，信用货币相对于纸币和贵金属贬值。在最糟糕的情形中，银行账户不过是一些无法兑现的承诺，并且一部分信用货币会凭空蒸发。类似地，当人们开始严重怀疑一个国家的经济是否健康时，其通货会对其他国家的通货贬值，同时也相对于黄金这一国际金融体系中的（目前是非正式的）最终货币贬值。如果在这一特定通货和其他通货之间有“固定”的兑换比率，使这一比率保持稳定的压力就会不断加大，直到人们不得不放弃这些固定比率。</p></blockquote><p>谢克暗示了产生两种不同价格波动模式的原因：信用货币和黄金的不同表现形式。1940年前，价格波动是以黄金为锚的，而1940年后，价格波动是以信用货币为锚的。所以后面的通胀可以解释为信用货币的贬值。</p><p>从图2.10上，谢克继续指出黄金价格指数与所谓的康波经济周期的联系。</p><blockquote><p>因此，以共同的国际金本位而非各自的国家通货为标准来考察英国和美国的物价是具有启发意义的。为此，只需要把每个国家的物价除以用该国通货所表示的黄金价格。图2.10呈现了用这一方法得到的英美两国的批发价格指数。由此产生的“黄金价格长波”向我们展示了一些相当有意思的事情。不仅之前的长波动都清晰可见，且其周期性与康德拉季耶夫一开始所提出的长波的周期性相近，而且现在在战后时期也出现了两个清晰的长波。第一个长波在1970年达到峰值，然后在20世纪70年代和80年代初进入强势下行阶段。这一时期恰恰是人们认为的一场普遍性的经济危机爆发的时期（vanDuijin 1983，chs.1-2; Shaikh1987a）。第二个长波在2000年达到峰值，并且我们可以看到，始于2007—2008年的全球危机到来得十分准时（见第16~17章）。</p></blockquote><p>评论：刻舟求剑地按这个趋势预测，下一次全球危机将发生在2030年左右。</p><h3 id="一般利润率">2.5 一般利润率</h3><ul><li>美国利润率从1947—1982年下降超过了45%，并在此后保持平稳。</li></ul><figure><img src="图2.11.png" alt="图2.11 美国的企业利润率（1947—2011年）" /><figcaption aria-hidden="true">图2.11美国的企业利润率（1947—2011年）</figcaption></figure><blockquote><p>这就立刻引出许多关键性的问题：是什么决定了整体利润率的变化路径？为什么利润率会下降，下降又是怎么被逆转的？这些问题又进一步地指向了另一个问题，即我们应该怎样区分以下两种不同的情形：一种是利润变化的结构性趋势，另一种是之前提到的周期性的、由一系列事件共同导致的经济波动对利润率变化的影响（见图2.4A~图2.4C）。对一般利润率的分析将为我们提供一个了解宏观经济增长及经济周期的切入点。</p></blockquote><h3 id="动荡的套利">2.6 动荡的套利</h3><ul><li>新增资本存在不同部门间利润率的均等化，而非现存旧资本。</li></ul><figure><img src="图2.12.png"alt="图2.12 美国制造业平均利润率（1960—1989年）" /><figcaption aria-hidden="true">图2.12美国制造业平均利润率（1960—1989年）</figcaption></figure><figure><img src="图2.13.png"alt="图2.13 美国制造业增量利润率（1960—1989年）" /><figcaption aria-hidden="true">图2.13美国制造业增量利润率（1960—1989年）</figcaption></figure><h3 id="相对价格">2.7 相对价格</h3><ul><li>任何一件商品的价格均可以表示为两个不同要素的乘积。第一个要素是与生产该商品相关的垂直整合单位劳动力成本。第二个要素是与生产该商品的行业相关的垂直整合的利润-工资比。</li></ul><h3 id="世界范围内的收敛和离散趋势">2.8 世界范围内的收敛和离散趋势</h3><ul><li>资本主义带来了愈发加剧的不平等。</li></ul><figure><img src="图2.16.png"alt="图2.16 最富裕4国和最贫穷4国的人均GDP（以国际元为单位，对数值）" /><figcaption aria-hidden="true">图2.16最富裕4国和最贫穷4国的人均GDP（以国际元为单位，对数值）</figcaption></figure><h3 id="概括与结论">2.9 概括与结论</h3><ul><li><p>成功的资本主义经济体是以强有力的长期模式为特征的。实际产出、投资和生产率的变化路径证明，经济增长以及不断提升的社会效益是这一制度的根本特征。</p></li><li><p>这一体系的增长总是贯穿在反复出现的波动中，通过反复出现的波动表现出来，并且总是被周期性的“大萧条”打断。</p></li><li><p>持续性的技术变革是资本主义的另一大典型特征，它表现为不断增长的生产率。</p></li><li><p>企业有强烈的动机抗拒实际工资的增长超过生产率的增长。实际工资取决于劳资力量之间的角力。制度是重要的，然而它们却只能在竞争和积累划定的界限之内运行。</p></li><li><p>几个世纪以来，价格一直呈现长期摇摆，而没有长期趋势。然而，在战后的整个资本世界里，这一模式发生了剧烈变化。物价开始不断上涨，通货膨胀开始表现为一种自然现象。而在以黄金为锚的价格指数中，价格的长期波动模式仍然存在。</p></li><li><p>美国的一般利润率在1947—1982年急剧下降，之后只是部分反弹。</p></li><li><p>只有增量利润率与新的资本（即投资）有关系。因此，只有增量利润率才会因（新）资本在部门间的流动而实现均等化。平均利润率在大部分情况下各不相同，但增量利润率一次又一次地“交叉”。</p></li><li><p>行业间相对价格结构中的大部分被预计由相对直接和间接（垂直整合的）单位劳动力成本决定。</p></li><li><p>对最富裕国家和最贫穷国家相对实际人均GDP的考察，揭示出在这样一个深陷资本主义网络的世界中，不平等程度日益加深。</p></li></ul><h2 id="第三章-微观基础和宏观模式">第三章 微观基础和宏观模式</h2><p>从第二章的结论中，谢克提出两个问题：</p><p>一、资本主义是一个动态的社会系统，其文化、制度和政策从长期来看会发生根本性的变化，那么一代又一代的无数个体持续进行的交互作用如何可能产生稳定的循环模式呢？</p><p>二、对于我们实际发现的那种动荡模式，均衡、调整过程和动力学的哪些理论概念是适用的？</p><p>实际上归结到一个经典的科学现象，宏观的现象，往往只基于系统的涌现特质，类似于物理里的Ising模型。</p><p>我们自然可以研究它的稳态，动力学过程，关键的宏观量和普适性分类。</p><p>谢克写道：</p><blockquote><p>第一个问题将我们引向微观过程和宏观模式的关系问题。微观经济学之所以重要，是因为个体行为人会做出选择，而这些选择将产生个体性结果与社会性结果。……我们可以看到，反对超理性行为与代表性行为人的历史性、经验性和分析性证据都是无可反驳的。而且，对主要的经验性研究结果的解释是可以从大量的个体决策方式中得出的。这是因为塑形结构（诸如预算约束和社会影响）在产生总体模式中发挥了决定性作用。传统的理论构想既站不住脚，也非必要。</p></blockquote><p>这里指出，传统经济学中对理性选择的假设：一、是站不住脚的，二、是非必要的（对宏观模式没有决定性影响）。</p><blockquote><p>一旦认识到极为不同的微观基础可以产生相同的市场层面或整体经济的模式，我们就可以将微观经济学划分为两种不同的命题：（1）可以从大量微观基础推导出来的基于经验的命题，包括斜向下的需求曲线、必需品收入弹性的差异、收入驱动的消费函数等；（2）基于个体行为特定特征的命题，这里所假定的基础是理性选择（后一命题包括那些关于市场过程有效率、和谐和一般最优的常见定理）。这样划分的优点在于为分析个体经济行为可能具有的特征拓展了空间，同时又保留了在经济分析中发挥重要作用的主要微观经济模式。</p></blockquote><p>这里谢克提出了一个重要的划分：宏观经济学中有两类命题，一类是基于经验的，可以从多种微观基础推导出来的命题，另一类是基于特定微观基础（理性选择）的命题。前者更为重要，因为它们对宏观模式有决定性影响，而后者则不然。</p><p>换言之，这里谢克决定用涌现理论来重新审视宏观经济学。</p>]]></content>
    
    
    <categories>
      
      <category>Reading Notes</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Economics</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
