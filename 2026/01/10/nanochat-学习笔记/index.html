

<!DOCTYPE html>
<html lang="en" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">

  <link rel="apple-touch-icon" sizes="76x76" href="/img/fluid.png">
  <link rel="icon" href="/img/fluid.png">
  

  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="Luocheng Liang">
  <meta name="keywords" content="">
  
    <meta name="description" content="本文档记录解读学习 Andrej Karpathy 开源项目 nanochat 的笔记。 学习的 git 代码版本是: f5a0ea4。 项目简介 nanochat 是一个用（几乎，有一点 Rust 用来训练分词器）纯 Python 实现的全栈 GPT库，包括 pretrain、midtrain、finetune、inference 等。它的目标是最简化帮助理解大型语言模型（LLM）的工作原理。">
<meta property="og:type" content="article">
<meta property="og:title" content="nanochat 学习笔记">
<meta property="og:url" content="https://notdesigned.github.io/2026/01/10/nanochat-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/index.html">
<meta property="og:site_name" content="Adscn&#39;s Blog">
<meta property="og:description" content="本文档记录解读学习 Andrej Karpathy 开源项目 nanochat 的笔记。 学习的 git 代码版本是: f5a0ea4。 项目简介 nanochat 是一个用（几乎，有一点 Rust 用来训练分词器）纯 Python 实现的全栈 GPT库，包括 pretrain、midtrain、finetune、inference 等。它的目标是最简化帮助理解大型语言模型（LLM）的工作原理。">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2026-01-10T16:20:15.000Z">
<meta property="article:modified_time" content="2026-02-26T12:57:24.809Z">
<meta property="article:author" content="Luocheng Liang">
<meta property="article:tag" content="LLM">
<meta property="article:tag" content="Open Source">
<meta name="twitter:card" content="summary_large_image">
  
  
  
  <title>nanochat 学习笔记 - Adscn&#39;s Blog</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1749284_5i9bdhy70f8.css">



<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1736178_k526ubmyhba.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"notdesigned.github.io","root":"/","version":"1.9.8","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"follow_dnt":true,"baidu":null,"google":{"measurement_id":null},"tencent":{"sid":null,"cid":null},"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false},"umami":{"src":null,"website_id":null,"domains":null,"start_time":"2024-01-01T00:00:00.000Z","token":null,"api_server":null}},"search_path":"/local-search.xml","include_content_in_search":true};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  


  
<meta name="generator" content="Hexo 8.1.1"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>Adscn&#39;s Blog</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/blog/" target="_self">
                <i class="iconfont icon-home-fill"></i>
                <span>Home</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/blog/" target="_self">
                <i class="iconfont icon-archive-fill"></i>
                <span>Blog</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/" target="_self">
                <i class="iconfont icon-archive-fill"></i>
                <span>Archives</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/" target="_self">
                <i class="iconfont icon-category-fill"></i>
                <span>Categories</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/" target="_self">
                <i class="iconfont icon-tags-fill"></i>
                <span>Tags</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/" target="_self">
                <i class="iconfont icon-user-fill"></i>
                <span>About</span>
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              <i class="iconfont icon-search"></i>
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">
              <i class="iconfont icon-dark" id="color-toggle-icon"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/default.png') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="nanochat 学习笔记"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2026-01-10 16:20" pubdate>
          January 10, 2026 pm
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          5.8k words
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          49 mins
        
      </span>
    

    
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <h1 id="seo-header">nanochat 学习笔记</h1>
            
            
              <div class="markdown-body">
                
                <p>本文档记录解读学习 Andrej Karpathy 开源项目 <a
target="_blank" rel="noopener" href="https://github.com/karpathy/nanochat">nanochat</a> 的笔记。</p>
<p>学习的 git 代码版本是: <code>f5a0ea4</code>。</p>
<h2 id="项目简介">项目简介</h2>
<p>nanochat 是一个用（几乎，有一点 Rust 用来训练分词器）纯 Python
实现的全栈 GPT库，包括 pretrain、midtrain、finetune、inference
等。它的目标是最简化帮助理解大型语言模型（LLM）的工作原理。</p>
<p>计划从以下几个方面来学习这个项目：</p>
<ol type="1">
<li><strong>代码结构</strong>：了解项目的整体架构和各个模块的功能。</li>
<li><strong>模型实现</strong>：理解 Transformer 模型的实现细节。</li>
<li><strong>训练流程</strong>：学习模型的训练过程，包括数据预处理、训练循环等。</li>
</ol>
<p>Wiki 参考： - <a
target="_blank" rel="noopener" href="https://deepwiki.com/karpathy/nanochat">nanochat Wiki</a></p>
<h2 id="项目安装">项目安装</h2>
<p>我使用的机器与环境如下： - windows上的wsl2+Ubuntu 22.04 - CUDA 13.1 -
RAM: 48GB - GPU: RTX 5080 16GB</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash">git <span class="hljs-built_in">clone</span> https://github.com/karpathy/nanochat.git<br><span class="hljs-built_in">cd</span> nanochat<br>uv venv<br>uv <span class="hljs-built_in">sync</span> --extra gpu<br><span class="hljs-built_in">source</span> .venv/bin/activate<br></code></pre></td></tr></table></figure>
<p>试试跑起来 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash">python -m nanochat.report reset<br>python -m nanochat.dataset -n 240<br>python -m scripts.tok_train --max_chars=2000000000 --vocab_size=65536<br>python -m scripts.tok_eval<br>python -m scripts.base_train --depth=4 --max_seq_len=512 --device_batch_size=1 --eval_tokens=512 --core_metric_every=-1 --total_batch_size=512 --num_iterations=20<br></code></pre></td></tr></table></figure></p>
<h2 id="代码结构">代码结构</h2>
<ul>
<li><code>nanochat/</code>：核心代码目录，包含模型实现、数据处理等模块。
<ul>
<li><code>adamw.py</code>：AdamW 优化器实现。</li>
<li><code>checkpoint_manager.py</code>：检查点管理器，用于保存和加载模型,优化器状态等。</li>
<li><code>common.py</code>：常用工具函数。有打印，logging，检测torch
device等。</li>
<li><code>core_eval.py</code>：评价模型性能的核心函数。</li>
<li><code>dataloader.py</code>：数据加载器实现。</li>
<li><code>dataset.py</code>：数据集处理模块。</li>
<li><code>engine.py</code>：用于推理对话的引擎，发送和接收token</li>
<li><code>execution.py</code>：一个沙盒执行环境</li>
<li><code>gpt.py</code>：GPT 模型的核心实现。</li>
<li><code>loss_eval.py</code>：损失评估模块。</li>
<li><code>muon.py</code>：Muon 优化器实现。</li>
<li><code>report.py</code>：报告生成模块。</li>
<li><code>tokenizer.py</code>：分词器实现。</li>
</ul></li>
</ul>
<h2 id="代码实现">代码实现</h2>
<p>从前到后，按照模型本身的逻辑顺序来学习代码实现，外加训练，评估等辅助功能。</p>
<h3 id="数据集与分词器">数据集与分词器</h3>
<p>数据集相关代码在 <code>dataset.py</code> 和
<code>dataloader.py</code> 中，分词器在 <code>tokenizer.py</code>
中。</p>
<h4 id="分词器实现">分词器实现</h4>
<p>首先，<code>tokenizer.py</code>
实现了两个分词器类：<code>HuggingFaceTokenizer</code> 和
<code>RustBPETokenizer</code>。按照描述，两者应该都是基于 BPE（Byte Pair
Encoding）算法的分词器。只是 <code>HuggingFaceTokenizer</code> 使用了
Hugging Face 的 <code>tokenizers</code> 库，可能为了完整的兼容。而
<code>RustBPETokenizer</code> 是基于 <code>tiktoken</code> 库并用
RustBPE 训练。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python">SPECIAL_TOKENS = [<br>    <span class="hljs-comment"># every document begins with the Beginning of Sequence (BOS) token that delimits documents</span><br>    <span class="hljs-string">&quot;&lt;|bos|&gt;&quot;</span>,<br>    <span class="hljs-comment"># tokens below are only used during finetuning to render Conversations into token ids</span><br>    <span class="hljs-string">&quot;&lt;|user_start|&gt;&quot;</span>, <span class="hljs-comment"># user messages</span><br>    <span class="hljs-string">&quot;&lt;|user_end|&gt;&quot;</span>,<br>    <span class="hljs-string">&quot;&lt;|assistant_start|&gt;&quot;</span>, <span class="hljs-comment"># assistant messages</span><br>    <span class="hljs-string">&quot;&lt;|assistant_end|&gt;&quot;</span>,<br>    <span class="hljs-string">&quot;&lt;|python_start|&gt;&quot;</span>, <span class="hljs-comment"># assistant invokes python REPL tool</span><br>    <span class="hljs-string">&quot;&lt;|python_end|&gt;&quot;</span>,<br>    <span class="hljs-string">&quot;&lt;|output_start|&gt;&quot;</span>, <span class="hljs-comment"># python REPL outputs back to assistant</span><br>    <span class="hljs-string">&quot;&lt;|output_end|&gt;&quot;</span>,<br>]<br><br><span class="hljs-comment"># <span class="hljs-doctag">NOTE:</span> this split pattern deviates from GPT-4 in that we use \p&#123;N&#125;&#123;1,2&#125; instead of \p&#123;N&#125;&#123;1,3&#125;</span><br><span class="hljs-comment"># I did this because I didn&#x27;t want to &quot;waste&quot; too many tokens on numbers for smaller vocab sizes.</span><br><span class="hljs-comment"># I haven&#x27;t validated that this is actually a good idea, TODO.</span><br>SPLIT_PATTERN = <span class="hljs-string">r&quot;&quot;&quot;&#x27;(?i:[sdmt]|ll|ve|re)|[^\r\n\p&#123;L&#125;\p&#123;N&#125;]?+\p&#123;L&#125;+|\p&#123;N&#125;&#123;1,2&#125;| ?[^\s\p&#123;L&#125;\p&#123;N&#125;]++[\r\n]*|\s*[\r\n]|\s+(?!\S)|\s+&quot;&quot;&quot;</span><br></code></pre></td></tr></table></figure>
<p>这标注了特殊的 token 列表
<code>SPECIAL_TOKENS</code>，以及用于预分词的正则表达式
<code>SPLIT_PATTERN</code>。</p>
<p>这里简单分析一下它的正则表达式逻辑： -
<code>'(?i:[sdmt]|ll|ve|re)</code>：匹配常见的缩写形式，如 ’s, ’d, ’m,
’t, ll, ve, re（不区分大小写）。 -
<code>[^\r\n\p{L}\p{N}]?+\p{L}+</code>：匹配以字母开头的单词，前面可以有一个非字母数字字符。
- <code>\p{N}{1,2}</code>：匹配1到2位的数字序列，标准的 GPT-4
使用的是1到3位数字。这里做了修改以节省 token 空间。 -
<code>?[^\s\p{L}\p{N}]++[\r\n]*</code>：匹配非空白、非字母数字的字符，前面可以有一个空格，后面可以跟换行符。
- <code>\s*[\r\n]</code>：匹配换行符，前面可以有任意数量的空白字符。 -
<code>\s+(?!\S)</code>：匹配空白字符，后面不跟非空白字符（即匹配行尾的空白）。
- <code>\s+</code>：匹配任意数量的空白字符。</p>
<p>先来看 <code>HuggingFaceTokenizer</code> 类：</p>
<p><a
target="_blank" rel="noopener" href="https://huggingface.co/docs/tokenizers/v0.20.3/">文档参考</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> tokenizers <span class="hljs-keyword">import</span> Tokenizer <span class="hljs-keyword">as</span> HFTokenizer<br><span class="hljs-keyword">from</span> tokenizers <span class="hljs-keyword">import</span> pre_tokenizers, decoders, Regex<br><span class="hljs-keyword">from</span> tokenizers.models <span class="hljs-keyword">import</span> BPE<br><span class="hljs-keyword">from</span> tokenizers.trainers <span class="hljs-keyword">import</span> BpeTrainer<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">HuggingFaceTokenizer</span>:<br>    <span class="hljs-string">&quot;&quot;&quot;Light wrapper around HuggingFace Tokenizer for some utilities&quot;&quot;&quot;</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, tokenizer</span>):<br>        <span class="hljs-variable language_">self</span>.tokenizer = tokenizer<br>    <span class="hljs-comment"># ...</span><br><span class="hljs-meta">    @classmethod</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">train_from_iterator</span>(<span class="hljs-params">cls, text_iterator, vocab_size</span>):<br>        <span class="hljs-comment"># 从一个文本迭代器，加上一个词汇表大小，来训练一个 Hugging Face BPE 分词器</span><br>        tokenizer = HFTokenizer(BPE(<br>            byte_fallback=<span class="hljs-literal">True</span>, <span class="hljs-comment"># needed!</span><br>            unk_token=<span class="hljs-literal">None</span>,<br>            fuse_unk=<span class="hljs-literal">False</span>,<br>        ))<br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">当 byte_fallback 设为 True 时，分词器的处理逻辑会发生根本变化：</span><br><span class="hljs-string"></span><br><span class="hljs-string">首选尝试：按照正常的词表（如 BPE 或 Unigram 算法生成的子词）进行分词。</span><br><span class="hljs-string"></span><br><span class="hljs-string">触发降级（Fallback）：如果遇到一个完全不在词表里的字符，它不会直接输出 [UNK]。</span><br><span class="hljs-string"></span><br><span class="hljs-string">字节化处理：分词器会将该未知字符转化为其 UTF-8 编码的字节（Bytes）。</span><br><span class="hljs-string"></span><br><span class="hljs-string">字节 Token 映射：词表中预先保留了 256 个基础 Token，分别对应字节 0 到 255。分词器会将这个未知字符拆解为一系列字节 Token。</span><br><span class="hljs-string"></span><br><span class="hljs-string">unk_token (str, optional) — 指定未知字符的替代 Token，这里 None，表示不使用特殊的未知 Token。</span><br><span class="hljs-string"></span><br><span class="hljs-string">fuse_unk (bool, optional) — 是否将连续的未知 Token 融合为一个单独的 Token.</span><br><span class="hljs-string"></span><br><span class="hljs-string">这里整体的意思是，把 byte-level 作为最后的兜底方案，确保任何输入字符都能被分词器处理，而不是直接归为未知 Token。</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br>        <span class="hljs-comment"># Normalizer: None</span><br>        tokenizer.normalizer = <span class="hljs-literal">None</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">不进行任何正规化处理，保持原始文本不变。（比如不转换大小写，不去除标点，重音符号等）</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br>        <span class="hljs-comment"># Pre-tokenizer: GPT-4 style</span><br>        <span class="hljs-comment"># the regex pattern used by GPT-4 to split text into groups before BPE</span><br>        <span class="hljs-comment"># <span class="hljs-doctag">NOTE:</span> The pattern was changed from \p&#123;N&#125;&#123;1,3&#125; to \p&#123;N&#125;&#123;1,2&#125; because I suspect it is harmful to</span><br>        <span class="hljs-comment"># very small models and smaller vocab sizes, because it is a little bit wasteful in the token space.</span><br>        <span class="hljs-comment"># (but I haven&#x27;t validated this! TODO)</span><br>        gpt4_split_regex = Regex(SPLIT_PATTERN)<br>        <span class="hljs-comment"># huggingface 要求 PATTERN 必须用 Regex 包裹</span><br>        tokenizer.pre_tokenizer = pre_tokenizers.<span class="hljs-type">Sequence</span>([<br>            pre_tokenizers.Split(pattern=gpt4_split_regex, behavior=<span class="hljs-string">&quot;isolated&quot;</span>, invert=<span class="hljs-literal">False</span>),<br>            pre_tokenizers.ByteLevel(add_prefix_space=<span class="hljs-literal">False</span>, use_regex=<span class="hljs-literal">False</span>)<br>        ])<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        behavior=&quot;isolated&quot;：表示匹配到的部分会被单独作为一个 token 处理，而不是与周围的文本合并。</span><br><span class="hljs-string">        invert=False：表示按照正则表达式的匹配结果进行分割，而不是取反。</span><br><span class="hljs-string">        pre_tokenizers.ByteLevel：将文本转换为字节级别的表示，确保每个字符都能被处理。</span><br><span class="hljs-string">        add_prefix_space=False：不在文本前添加额外的空格。</span><br><span class="hljs-string">        use_regex=False：不使用正则表达式进行字节级别的处理。</span><br><span class="hljs-string"></span><br><span class="hljs-string">        这个预分词器的设计目的是先用 GPT-4 风格的正则表达式进行初步分割，然后再将每个部分转换为字节级别的表示，确保任何字符都能被处理。</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        <span class="hljs-comment"># Decoder: ByteLevel (it pairs together with the ByteLevel pre-tokenizer)</span><br>        tokenizer.decoder = decoders.ByteLevel()<br>        <span class="hljs-comment"># Post-processor: None</span><br>        tokenizer.post_processor = <span class="hljs-literal">None</span><br>        <span class="hljs-comment"># Trainer: BPE</span><br>        trainer = BpeTrainer(<br>            vocab_size=vocab_size,<br>            show_progress=<span class="hljs-literal">True</span>,<br>            min_frequency=<span class="hljs-number">0</span>, <span class="hljs-comment"># no minimum frequency</span><br>            initial_alphabet=pre_tokenizers.ByteLevel.alphabet(),<br>            special_tokens=SPECIAL_TOKENS,<br>        )<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        vocab_size (int) — 词汇表的大小，决定了分词器可以识别的不同子词数量。</span><br><span class="hljs-string">        show_progress (bool) — 是否在训练过程中显示进度条。</span><br><span class="hljs-string">        min_frequency (int) — 一个子词被纳入词汇表的最低出现频率，这里设为 0，表示不设限制。</span><br><span class="hljs-string">        initial_alphabet (List[str]) — 初始字母表，这里使用 ByteLevel 预分词器的字母表，确保所有字节都被包含在内。</span><br><span class="hljs-string">        special_tokens (List[str]) — 特殊 Token 列表，如 &lt;|bos|&gt; 等，这些 Token 会被单独处理。</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        <span class="hljs-comment"># 调用Huggingface的训练方法</span><br>        tokenizer.train_from_iterator(text_iterator, trainer)<br>        <span class="hljs-keyword">return</span> cls(tokenizer)<br></code></pre></td></tr></table></figure>
<p><code>RustBPETokenizer</code> 类的实现：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> pickle<br><span class="hljs-keyword">import</span> rustbpe<br><span class="hljs-keyword">import</span> tiktoken<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">RustBPETokenizer</span>:<br>    <span class="hljs-string">&quot;&quot;&quot;Light wrapper around tiktoken (for efficient inference) but train with rustbpe&quot;&quot;&quot;</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, enc, bos_token</span>):<br>        <span class="hljs-variable language_">self</span>.enc = enc<br>        <span class="hljs-variable language_">self</span>.bos_token_id = <span class="hljs-variable language_">self</span>.encode_special(bos_token)<br><br><span class="hljs-meta">    @classmethod</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">train_from_iterator</span>(<span class="hljs-params">cls, text_iterator, vocab_size</span>):<br>        <span class="hljs-comment"># 1) train using rustbpe</span><br>        tokenizer = rustbpe.Tokenizer()<br>        <span class="hljs-comment"># the special tokens are inserted later in __init__, we don&#x27;t train them here</span><br>        vocab_size_no_special = vocab_size - <span class="hljs-built_in">len</span>(SPECIAL_TOKENS)<br>        <span class="hljs-keyword">assert</span> vocab_size_no_special &gt;= <span class="hljs-number">256</span>, <span class="hljs-string">f&quot;vocab_size_no_special must be at least 256, got <span class="hljs-subst">&#123;vocab_size_no_special&#125;</span>&quot;</span><br>        tokenizer.train_from_iterator(text_iterator, vocab_size_no_special, pattern=SPLIT_PATTERN)<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        先训练一个 RustBPE 分词器，减去特殊 token 的数量，确保基础词汇表大小足够大（至少256个，满足所有byte）。</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        <span class="hljs-comment"># 2) construct the associated tiktoken encoding for inference</span><br>        pattern = tokenizer.get_pattern()<br>        mergeable_ranks_list = tokenizer.get_mergeable_ranks()<br>        mergeable_ranks = &#123;<span class="hljs-built_in">bytes</span>(k): v <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> mergeable_ranks_list&#125;<br>        <span class="hljs-comment"># 此处将 rustbpe 输出的数据转换为 &#123;字节序列: 优先级数值&#125; 的字典格式。</span><br><br>        tokens_offset = <span class="hljs-built_in">len</span>(mergeable_ranks)<br>        special_tokens = &#123;name: tokens_offset + i <span class="hljs-keyword">for</span> i, name <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(SPECIAL_TOKENS)&#125;<br>        <span class="hljs-comment"># 为特殊 token 分配唯一的 token id，确保它们不会与基础词汇表冲突。</span><br>        enc = tiktoken.Encoding(<br>            name=<span class="hljs-string">&quot;rustbpe&quot;</span>,<br>            pat_str=pattern,<br>            mergeable_ranks=mergeable_ranks, <span class="hljs-comment"># dict[bytes, int] (token bytes -&gt; merge priority rank)</span><br>            special_tokens=special_tokens, <span class="hljs-comment"># dict[str, int] (special token name -&gt; token id)</span><br>        )<br>        <span class="hljs-keyword">return</span> cls(enc, <span class="hljs-string">&quot;&lt;|bos|&gt;&quot;</span>) <span class="hljs-comment"># 实例化并返回 RustBPETokenizer 对象</span><br><span class="hljs-comment"># ...</span><br></code></pre></td></tr></table></figure>
<h4 id="数据集">数据集</h4>
<p>数据是 fineweb_100b 数据集的一个子集，<a
target="_blank" rel="noopener" href="https://huggingface.co/datasets/karpathy/fineweb-edu-100b-shuffle">link</a>。下载到
<code>$BASE_DIR/base_data</code> 目录下。 数据被切片成多个
<code>.parquet</code> 文件。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs py"><span class="hljs-keyword">def</span> <span class="hljs-title function_">parquets_iter_batched</span>(<span class="hljs-params">split, start=<span class="hljs-number">0</span>, step=<span class="hljs-number">1</span></span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    Iterate through the dataset, in batches of underlying row_groups for efficiency.</span><br><span class="hljs-string">    - split can be &quot;train&quot; or &quot;val&quot;. the last parquet file will be val.</span><br><span class="hljs-string">    - start/step are useful for skipping rows in DDP. e.g. start=rank, step=world_size</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-keyword">assert</span> split <span class="hljs-keyword">in</span> [<span class="hljs-string">&quot;train&quot;</span>, <span class="hljs-string">&quot;val&quot;</span>], <span class="hljs-string">&quot;split must be &#x27;train&#x27; or &#x27;val&#x27;&quot;</span><br>    parquet_paths = list_parquet_files()<br>    parquet_paths = parquet_paths[:-<span class="hljs-number">1</span>] <span class="hljs-keyword">if</span> split == <span class="hljs-string">&quot;train&quot;</span> <span class="hljs-keyword">else</span> parquet_paths[-<span class="hljs-number">1</span>:]<br>    <span class="hljs-keyword">for</span> filepath <span class="hljs-keyword">in</span> parquet_paths:<br>        pf = pq.ParquetFile(filepath)<br>        <span class="hljs-keyword">for</span> rg_idx <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(start, pf.num_row_groups, step):<br>            rg = pf.read_row_group(rg_idx)<br>            texts = rg.column(<span class="hljs-string">&#x27;text&#x27;</span>).to_pylist()<br>            <span class="hljs-keyword">yield</span> texts<br></code></pre></td></tr></table></figure>
<p>parquet 文件是一个列式存储格式，这里按行组（row
group）来读取，避免一次性加载过多数据。</p>
<p>而 DataLoader 负责读取数据并生成训练所需的一个批次的 token 序列：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br></pre></td><td class="code"><pre><code class="hljs py"><span class="hljs-keyword">def</span> <span class="hljs-title function_">tokenizing_distributed_data_loader_with_state</span>(<span class="hljs-params">B, T, split, tokenizer_threads=<span class="hljs-number">4</span>, tokenizer_batch_size=<span class="hljs-number">128</span>, device=<span class="hljs-string">&quot;cuda&quot;</span>, resume_state_dict=<span class="hljs-literal">None</span></span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    Stream pretraining text from parquet files, tokenize, yield training batches.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    This implementation became a bit more complex because we wish to support approximate resume training.</span><br><span class="hljs-string">    Instead of turning this into a Class, we opt to return the state_dict with every batch,</span><br><span class="hljs-string">    and then the caller can pass in a state_dict to resume training from a desired point.</span><br><span class="hljs-string">    Note that this resumption is atm only *approximate* for simplicity.</span><br><span class="hljs-string">    We won&#x27;t repeat the same documents but we might skip a few.</span><br><span class="hljs-string">    The state_dict that is returned can be later passed into this function via `resume_state_dict` to approximately resume.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Perfect state resumption is possible but would be a lot more bloated, probably not worth it atm.</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-keyword">assert</span> split <span class="hljs-keyword">in</span> [<span class="hljs-string">&quot;train&quot;</span>, <span class="hljs-string">&quot;val&quot;</span>], <span class="hljs-string">&quot;split must be &#x27;train&#x27; or &#x27;val&#x27;&quot;</span><br><br>    <span class="hljs-comment"># infinite iterator over document batches (list of text strings)</span><br>    ddp, ddp_rank, ddp_local_rank, ddp_world_size = get_dist_info()<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">document_batches</span>():<br>        parquet_paths = list_parquet_files()<br>        <span class="hljs-keyword">assert</span> <span class="hljs-built_in">len</span>(parquet_paths) != <span class="hljs-number">0</span>, <span class="hljs-string">&quot;No dataset parquet files found, did you run dataset.py?&quot;</span><br>        parquet_paths = parquet_paths[:-<span class="hljs-number">1</span>] <span class="hljs-keyword">if</span> split == <span class="hljs-string">&quot;train&quot;</span> <span class="hljs-keyword">else</span> parquet_paths[-<span class="hljs-number">1</span>:]<br>        resume_pq_idx = resume_state_dict[<span class="hljs-string">&quot;pq_idx&quot;</span>] <span class="hljs-keyword">if</span> resume_state_dict <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">else</span> <span class="hljs-number">0</span><br>        resume_rg_idx = resume_state_dict[<span class="hljs-string">&quot;rg_idx&quot;</span>] <span class="hljs-keyword">if</span> resume_state_dict <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">else</span> <span class="hljs-literal">None</span><br>        first_pass = <span class="hljs-literal">True</span><br>        pq_idx = resume_pq_idx <span class="hljs-comment"># we kick off parquet files at the resume index (or by default just 0)</span><br>        <span class="hljs-keyword">while</span> <span class="hljs-literal">True</span>: <span class="hljs-comment"># iterate infinitely (multi-epoch)</span><br>            pq_idx = resume_pq_idx <span class="hljs-keyword">if</span> first_pass <span class="hljs-keyword">else</span> <span class="hljs-number">0</span><br>            <span class="hljs-keyword">while</span> pq_idx &lt; <span class="hljs-built_in">len</span>(parquet_paths): <span class="hljs-comment"># iterate over all parquet files</span><br>                filepath = parquet_paths[pq_idx]<br>                pf = pq.ParquetFile(filepath)<br>                <span class="hljs-comment"># Start from resume point if resuming on same file, otherwise from DDP rank</span><br>                <span class="hljs-comment"># I know this state resumption is a little bit tricky and a little bit hacky... sigh.</span><br>                <span class="hljs-keyword">if</span> first_pass <span class="hljs-keyword">and</span> (resume_rg_idx <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>) <span class="hljs-keyword">and</span> (pq_idx == resume_pq_idx):<br>                    base_idx = resume_rg_idx // ddp_world_size <span class="hljs-comment"># in units of ddp_world_size</span><br>                    base_idx += <span class="hljs-number">1</span> <span class="hljs-comment"># advance by 1 so that we definitely don&#x27;t repeat data after resuming</span><br>                    rg_idx = base_idx * ddp_world_size + ddp_rank<br>                    <span class="hljs-keyword">if</span> rg_idx &gt;= pf.num_row_groups:<br>                        pq_idx += <span class="hljs-number">1</span><br>                        <span class="hljs-keyword">continue</span><br>                    resume_rg_idx = <span class="hljs-literal">None</span> <span class="hljs-comment"># set to None as we only want to do this a single time</span><br>                <span class="hljs-keyword">else</span>:<br>                    rg_idx = ddp_rank<br>                <span class="hljs-keyword">while</span> rg_idx &lt; pf.num_row_groups:<br>                    rg = pf.read_row_group(rg_idx)<br>                    batch = rg.column(<span class="hljs-string">&#x27;text&#x27;</span>).to_pylist() <span class="hljs-comment"># each batch is a parquet group, e.g. 1024 rows</span><br>                    <span class="hljs-comment"># the tokenizer encode might want to go in even smaller batches, e.g. 128 rows</span><br>                    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, <span class="hljs-built_in">len</span>(batch), tokenizer_batch_size):<br>                        <span class="hljs-keyword">yield</span> batch[i:i+tokenizer_batch_size], (pq_idx, rg_idx)<br>                    rg_idx += ddp_world_size <span class="hljs-comment"># advance to the next row group (in DDP)</span><br>                pq_idx += <span class="hljs-number">1</span> <span class="hljs-comment"># advance to the next parquet file</span><br>            first_pass = <span class="hljs-literal">False</span><br>    batches = document_batches()<br><br>    <span class="hljs-comment"># Now emit batches of tokens.</span><br>    needed_tokens = B * T + <span class="hljs-number">1</span> <span class="hljs-comment"># +1 is because we also need the target at the last token</span><br>    <span class="hljs-comment"># get the tokenizer and the bos token</span><br>    tokenizer = get_tokenizer()<br>    bos_token = tokenizer.get_bos_token_id()<br>    <span class="hljs-comment"># scratch buffer holds the tokens for one iteration</span><br>    token_buffer = deque() <span class="hljs-comment"># we stream tokens on the right and pop from the left</span><br>    <span class="hljs-keyword">while</span> <span class="hljs-literal">True</span>:<br>        <span class="hljs-comment"># Accumulate enough tokens for one iteration before yielding.</span><br>        <span class="hljs-keyword">while</span> <span class="hljs-built_in">len</span>(token_buffer) &lt; needed_tokens:<br>            doc_batch, (pq_idx, rg_idx) = <span class="hljs-built_in">next</span>(batches)<br>            token_lists = tokenizer.encode(doc_batch, prepend=bos_token, num_threads=tokenizer_threads)<br>            <span class="hljs-keyword">for</span> tokens <span class="hljs-keyword">in</span> token_lists:<br>                token_buffer.extend(tokens)<br>        <span class="hljs-comment"># Move tokens from the deque into the scratch buffer</span><br>        tokens = [token_buffer.popleft() <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(needed_tokens)]<br>        <span class="hljs-comment"># CUDA supports memory pinning for asynchronous transfers between CPU and GPU</span><br>        use_cuda_optimizations = device == <span class="hljs-string">&quot;cuda&quot;</span><br>        scratch = torch.tensor(tokens, dtype=torch.long, pin_memory=use_cuda_optimizations) <span class="hljs-comment"># in PyTorch, long=int64</span><br>        <span class="hljs-comment"># Create the inputs/targets as 1D tensors</span><br>        inputs_cpu = scratch[:-<span class="hljs-number">1</span>]<br>        targets_cpu = scratch[<span class="hljs-number">1</span>:]<br>        <span class="hljs-comment"># Reshape to 2D and move to GPU async</span><br>        inputs = inputs_cpu.view(B, T).to(device=device, non_blocking=use_cuda_optimizations)<br>        targets = targets_cpu.view(B, T).to(device=device, non_blocking=use_cuda_optimizations)<br>        state_dict = &#123;<span class="hljs-string">&quot;pq_idx&quot;</span>: pq_idx, <span class="hljs-string">&quot;rg_idx&quot;</span>: rg_idx&#125; <span class="hljs-comment"># we need this in case we wish to approximately resume training</span><br>        <span class="hljs-keyword">yield</span> inputs, targets, state_dict<br></code></pre></td></tr></table></figure>
<p>document_batches 函数流式 yields tokenized_batch_size
个文本列表+(parquet文件索引，行组索引)，然后外层循环不断累积 token
直到满足一个训练批次的需求，再切分成 inputs 和 targets
返回，并附带当前的 parquet
文件和行组索引状态，方便后续恢复训练时使用。</p>
<h3 id="优化器">优化器</h3>
<p>优化器代码在 <code>adamw.py</code> 和 <code>muon.py</code> 中。</p>
<h4 id="adamw-优化器">AdamW 优化器</h4>
<p>令参数为 <span class="math inline">\(\theta_t\)</span>，学习率为
<span class="math inline">\(\alpha\)</span>，一阶矩估计为 <span
class="math inline">\(m_t\)</span>，二阶矩估计为 <span
class="math inline">\(v_t\)</span>，偏置修正后的一阶矩为 <span
class="math inline">\(\hat{m}_t\)</span>，偏置修正后的二阶矩为 <span
class="math inline">\(\hat{v}_t\)</span>，权重衰减系数为 <span
class="math inline">\(\lambda\)</span>。 <span class="math display">\[
\begin{align*}
\theta_{t+1} &amp;= \theta_t - \alpha \frac{\hat{m}_t}{\sqrt{\hat{v}_t}
+ \epsilon} - \alpha \lambda \theta_t \\
m_{t+1} &amp;= \beta_1 m_t + (1 - \beta_1) g_t\\
v_{t+1} &amp;= \beta_2 v_t + (1 - \beta_2) g_t^2\\
\hat{m}_t &amp;= \frac{m_t}{1 - \beta_1^t} \\
\hat{v}_t &amp;= \frac{v_t}{1 - \beta_2^t} \\
\end{align*}
\]</span></p>
<p>这里采取分布式训练的方式实现 AdamW 优化器，关键是利用了
<code>torch.distributed.reduce_scatter_tensor</code>与
<code>torch.distributed.all_gather_into_tensor</code>
来实现梯度的分布式平均和切分，从而减少每个 GPU 的内存占用。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">@torch.compile</span><br><span class="hljs-meta">@torch.no_grad()</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">step</span>(<span class="hljs-params">self</span>):<br>    rank = dist.get_rank()<br>    world_size = dist.get_world_size()<br>    reduce_scatter_futures: <span class="hljs-built_in">list</span>[torch.Future] = []<br>    all_reduce_futures: <span class="hljs-built_in">list</span>[torch.Future] = []<br>    grad_slices = []<br>    <span class="hljs-keyword">for</span> group <span class="hljs-keyword">in</span> <span class="hljs-variable language_">self</span>.param_groups:<br>        params: <span class="hljs-built_in">list</span>[Tensor] = group[<span class="hljs-string">&quot;params&quot;</span>]<br>        <span class="hljs-keyword">for</span> base_i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(params)):<br>            <span class="hljs-keyword">assert</span> params[base_i].shape[<span class="hljs-number">0</span>] % world_size == <span class="hljs-number">0</span>, <span class="hljs-string">f&quot;First dim of parameter shape <span class="hljs-subst">&#123;params[base_i].shape&#125;</span> must be divisible by world size <span class="hljs-subst">&#123;world_size&#125;</span>&quot;</span><br>            grad = params[base_i].grad<br>            rank_size = grad.shape[<span class="hljs-number">0</span>] // world_size<br>            grad_slice = torch.empty_like(grad[:rank_size])<br>            reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=<span class="hljs-literal">True</span>).get_future())<br>            grad_slices.append(grad_slice)<br></code></pre></td></tr></table></figure>
<p>这里先对每个参数的梯度进行 <code>reduce_scatter</code>
操作，将梯度平均后切分成多个片段，每个 GPU
只保留自己负责的片段，减少内存占用。</p>
<p>目前有 grad_slices 列表，存储了每个参数在当前 GPU
上的梯度片段和对应的 future 对象。 接下来对每个参数组，读取AdamW
的超参数，再对每个参数进行更新：</p>
<p>状态存储在 self.state[p] 中，包括 step 计数器，一阶矩 exp_avg
和二阶矩 exp_avg_sq。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><code class="hljs python">idx = <span class="hljs-number">0</span><br><span class="hljs-keyword">for</span> group <span class="hljs-keyword">in</span> <span class="hljs-variable language_">self</span>.param_groups:<br>    beta1, beta2 = group[<span class="hljs-string">&#x27;betas&#x27;</span>]<br>    eps = group[<span class="hljs-string">&#x27;eps&#x27;</span>]<br>    wd = group[<span class="hljs-string">&#x27;weight_decay&#x27;</span>]<br>    params = group[<span class="hljs-string">&#x27;params&#x27;</span>]<br>    <span class="hljs-keyword">for</span> base <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(params)):<br>        reduce_scatter_futures[idx].wait()<br>        p = params[base]<br>        rank_size = p.shape[<span class="hljs-number">0</span>] // world_size<br>        p_slice = p[rank * rank_size:(rank + <span class="hljs-number">1</span>) * rank_size]<br>        lr = group[<span class="hljs-string">&#x27;lr&#x27;</span>] * <span class="hljs-built_in">getattr</span>(p, <span class="hljs-string">&quot;lr_mul&quot;</span>, <span class="hljs-number">1.0</span>)<br>        state = <span class="hljs-variable language_">self</span>.state[p]<br>        g_slice = grad_slices[idx]<br>        <span class="hljs-comment"># State init</span><br>        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> state:<br>            state[<span class="hljs-string">&#x27;step&#x27;</span>] = torch.tensor(<span class="hljs-number">0</span>, dtype=torch.int64, device=p.device)<br>            state[<span class="hljs-string">&#x27;exp_avg&#x27;</span>] = torch.zeros_like(p_slice)<br>            state[<span class="hljs-string">&#x27;exp_avg_sq&#x27;</span>] = torch.zeros_like(p_slice)<br>        exp_avg = state[<span class="hljs-string">&#x27;exp_avg&#x27;</span>]<br>        exp_avg_sq = state[<span class="hljs-string">&#x27;exp_avg_sq&#x27;</span>]<br>        state[<span class="hljs-string">&#x27;step&#x27;</span>] += <span class="hljs-number">1</span><br>        t = state[<span class="hljs-string">&#x27;step&#x27;</span>]<br>        <span class="hljs-comment"># weight decay</span><br>        <span class="hljs-keyword">if</span> wd != <span class="hljs-number">0</span>:<br>            eff_weight_decay = lr * wd * <span class="hljs-built_in">getattr</span>(p, <span class="hljs-string">&quot;wd_mul&quot;</span>, <span class="hljs-number">1.0</span>)<br>            p_slice.mul_(<span class="hljs-number">1</span> - eff_weight_decay)<br>        <span class="hljs-comment"># update running averages</span><br>        exp_avg.mul_(beta1).add_(g_slice, alpha=<span class="hljs-number">1</span> - beta1)<br>        exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=<span class="hljs-number">1</span> - beta2)<br>        <span class="hljs-comment"># bias corrections</span><br>        bias1 = <span class="hljs-number">1</span> - beta1 ** t<br>        bias2 = <span class="hljs-number">1</span> - beta2 ** t<br>        <span class="hljs-comment"># compute step</span><br>        denom = (exp_avg_sq / bias2).sqrt().add_(eps)<br>        step_size = lr / bias1<br>        update = exp_avg.div(denom).mul_(step_size)<br>        p_slice.add_(other=update, alpha=-<span class="hljs-number">1.0</span>)<br>        idx += <span class="hljs-number">1</span><br>        all_reduce_futures.append(dist.all_gather_into_tensor(p, p_slice, async_op=<span class="hljs-literal">True</span>).get_future())<br>torch.futures.collect_all(all_reduce_futures).wait()<br><span class="hljs-comment"># 阻塞直到所有 all_gather 操作完成，确保所有 GPU 上的参数都同步更新完毕。</span><br></code></pre></td></tr></table></figure>
<h4 id="muon-优化器">Muon 优化器</h4>
<p>咕咕咕，可先看 <a
target="_blank" rel="noopener" href="https://kexue.fm/archives/10592">link</a>。</p>
<h3 id="gpt-模型实现">GPT 模型实现</h3>
<p>架构流程： 1. 输入的Token进入输入嵌入层（Token
Embeddings），输入类型： <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">inputs: Tensor  <span class="hljs-comment"># (B, T) long tensor of token indices</span><br>outputs: Tensor <span class="hljs-comment"># (B, T, C) float tensor of logits over vocabulary</span><br></code></pre></td></tr></table></figure> 2. RoPE 位置嵌入层（Positional
Embeddings）为每个位置添加位置信息。 <figure class="highlight avrasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs avrasm"><span class="hljs-symbol">pos_emb:</span> Tensor <span class="hljs-meta"># (T, C) float tensor of positional embeddings</span><br></code></pre></td></tr></table></figure>
加法广播到输入嵌入上</p>
<ol start="3" type="1">
<li><p>多层 Transformer 块（Transformer
Blocks）处理嵌入，捕捉上下文关系。</p></li>
<li><p>输出层（Output Layer）将 Transformer 的输出映射到词汇表大小的
logits。</p></li>
<li><p>从 logits 计算损失（Loss
Computation），用于训练。或者按照multinomial分布采样生成下一个
token，用于推理。</p></li>
</ol>
<p>我们需要的块包括： - 输入嵌入层 - 预计算位置嵌入层 - Transformer 块
（多个堆叠+ 残差连接） - 层归一化 - 多头自注意力机制 （推理时缓存 K,V）
- 前馈神经网络 - 输出层</p>
<p>表格如下</p>
<table>

<thead>
<tr>
<th>模块名称</th>
<th>输入维度</th>
<th>输出维度</th>
<th>核心功能</th>
</tr>
</thead>
<tbody>
<tr>
<td>Token Embedding</td>
<td>(B,T)</td>
<td>(B,T,C)</td>
<td>将离散索引转为连续向量</td>
</tr>
<tr>
<td>Pos Embedding</td>
<td>(T,C)</td>
<td>(T,C)</td>
<td>提供序列位置的绝对/相对信息</td>
</tr>
<tr>
<td>Transformer Block</td>
<td>(B,T,C)</td>
<td>(B,T,C)</td>
<td>包含 LayerNorm -&gt; Self-Attention -&gt; Residual -&gt; LayerNorm
-&gt; FFN -&gt; Residual</td>
</tr>
<tr>
<td>Output Head</td>
<td>(B,T,C)</td>
<td>(B,T,V)</td>
<td>投影至词汇表空间</td>
</tr>
</tbody>
</table>
<p>优化方面：</p>
<p>nanochat 使用了 KV Cache 来加速推理过程中的自注意力计算，并且使用了
GQA （Grouped Query Attention）来优化多头注意力的计算效率。</p>
<p>对于 KV Cache 的推理，要记录当前 Token 的位置索引，并索引正确的 RoPE
位置嵌入（按绝对位置）。</p>
<p>对于 GQA 的实现，即保持 Q 的头数不变，但将 K 和 V
的头数减少分组，每组共享 K 和 V。</p>
<p>先来看 <code>gpt.py</code> 中注意力模块的实现</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">CausalSelfAttention</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, config, layer_idx</span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        <span class="hljs-comment"># 记录是第几层 Transformer，用于 KV Cache 索引</span><br>        <span class="hljs-variable language_">self</span>.layer_idx = layer_idx <br>        <span class="hljs-comment"># 有多少个 Q 头，多少个 KV 头</span><br>        <span class="hljs-variable language_">self</span>.n_head = config.n_head <br>        <span class="hljs-variable language_">self</span>.n_kv_head = config.n_kv_head<br>        <span class="hljs-comment"># 嵌入维度和每个头的维度</span><br>        <span class="hljs-variable language_">self</span>.n_embd = config.n_embd<br>        <span class="hljs-variable language_">self</span>.head_dim = <span class="hljs-variable language_">self</span>.n_embd // <span class="hljs-variable language_">self</span>.n_head<br>        <span class="hljs-keyword">assert</span> <span class="hljs-variable language_">self</span>.n_embd % <span class="hljs-variable language_">self</span>.n_head == <span class="hljs-number">0</span><br>        <span class="hljs-keyword">assert</span> <span class="hljs-variable language_">self</span>.n_kv_head &lt;= <span class="hljs-variable language_">self</span>.n_head <span class="hljs-keyword">and</span> <span class="hljs-variable language_">self</span>.n_head % <span class="hljs-variable language_">self</span>.n_kv_head == <span class="hljs-number">0</span><br>        <span class="hljs-comment"># 定义线性层用于生成 Q、K、V 和输出投影</span><br>        <span class="hljs-variable language_">self</span>.c_q = nn.Linear(<span class="hljs-variable language_">self</span>.n_embd, <span class="hljs-variable language_">self</span>.n_head * <span class="hljs-variable language_">self</span>.head_dim, bias=<span class="hljs-literal">False</span>)<br>        <span class="hljs-variable language_">self</span>.c_k = nn.Linear(<span class="hljs-variable language_">self</span>.n_embd, <span class="hljs-variable language_">self</span>.n_kv_head * <span class="hljs-variable language_">self</span>.head_dim, bias=<span class="hljs-literal">False</span>)<br>        <span class="hljs-variable language_">self</span>.c_v = nn.Linear(<span class="hljs-variable language_">self</span>.n_embd, <span class="hljs-variable language_">self</span>.n_kv_head * <span class="hljs-variable language_">self</span>.head_dim, bias=<span class="hljs-literal">False</span>)<br>        <span class="hljs-variable language_">self</span>.c_proj = nn.Linear(<span class="hljs-variable language_">self</span>.n_embd, <span class="hljs-variable language_">self</span>.n_embd, bias=<span class="hljs-literal">False</span>)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x, cos_sin, kv_cache</span>):<br>        B, T, C = x.size()<br><br>        <span class="hljs-comment"># Project the input to get queries, keys, and values</span><br>        q = <span class="hljs-variable language_">self</span>.c_q(x).view(B, T, <span class="hljs-variable language_">self</span>.n_head, <span class="hljs-variable language_">self</span>.head_dim)<br>        k = <span class="hljs-variable language_">self</span>.c_k(x).view(B, T, <span class="hljs-variable language_">self</span>.n_kv_head, <span class="hljs-variable language_">self</span>.head_dim)<br>        v = <span class="hljs-variable language_">self</span>.c_v(x).view(B, T, <span class="hljs-variable language_">self</span>.n_kv_head, <span class="hljs-variable language_">self</span>.head_dim)<br><br>        <span class="hljs-comment"># Apply Rotary Embeddings to queries and keys to get relative positional encoding</span><br>        cos, sin = cos_sin<br>        q, k = apply_rotary_emb(q, cos, sin), apply_rotary_emb(k, cos, sin) <span class="hljs-comment"># QK rotary embedding</span><br>        q, k = norm(q), norm(k) <span class="hljs-comment"># QK norm</span><br>        q, k, v = q.transpose(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>), k.transpose(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>), v.transpose(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>) <span class="hljs-comment"># make head be batch dim, i.e. (B, T, H, D) -&gt; (B, H, T, D)</span><br><br>        <span class="hljs-comment"># Apply KV cache: insert current k,v into cache, get the full view so far</span><br>        <span class="hljs-keyword">if</span> kv_cache <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            k, v = kv_cache.insert_kv(<span class="hljs-variable language_">self</span>.layer_idx, k, v)<br>        <span class="hljs-comment"># 在预训练的时候，Tq == Tk，因为没有缓存</span><br>        <span class="hljs-comment"># 在推理的时候，Tq 可以小于 Tk，因为有缓存，每次只处理一个 token，然后询问前面的缓存 + 当前 token</span><br>        Tq = q.size(<span class="hljs-number">2</span>) <span class="hljs-comment"># number of queries in this forward pass</span><br>        Tk = k.size(<span class="hljs-number">2</span>) <span class="hljs-comment"># number of keys/values in total (in the cache + current forward pass)</span><br><br>        <span class="hljs-comment"># Attention: queries attend to keys/values autoregressively. A few cases to handle:</span><br>        enable_gqa = <span class="hljs-variable language_">self</span>.n_head != <span class="hljs-variable language_">self</span>.n_kv_head <span class="hljs-comment"># Group Query Attention (GQA): duplicate key/value heads to match query heads if desired</span><br>        <span class="hljs-keyword">if</span> kv_cache <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">or</span> Tq == Tk:<br>            <span class="hljs-comment"># During training (no KV cache), attend as usual with causal attention</span><br>            <span class="hljs-comment"># And even if there is KV cache, we can still use this simple version when Tq == Tk</span><br>            y = F.scaled_dot_product_attention(q, k, v, is_causal=<span class="hljs-literal">True</span>, enable_gqa=enable_gqa)<br>            <span class="hljs-comment"># 训练时没有缓存，或者推理时当前查询数等于缓存的键值数，都可以直接使用因果注意力。</span><br>        <span class="hljs-keyword">elif</span> Tq == <span class="hljs-number">1</span>:<br>            <span class="hljs-comment"># During inference but with a single query in this forward pass:</span><br>            <span class="hljs-comment"># The query has to attend to all the keys/values in the cache</span><br>            y = F.scaled_dot_product_attention(q, k, v, is_causal=<span class="hljs-literal">False</span>, enable_gqa=enable_gqa)<br>            <span class="hljs-comment"># 推理时如果只有一个查询，可以直接让这个查询关注缓存中的所有键值对。</span><br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-comment"># During inference AND we have a chunk of queries in this forward pass:</span><br>            <span class="hljs-comment"># First, each query attends to all the cached keys/values (i.e. full prefix)</span><br>            attn_mask = torch.zeros((Tq, Tk), dtype=torch.<span class="hljs-built_in">bool</span>, device=q.device) <span class="hljs-comment"># True = keep, False = mask</span><br>            prefix_len = Tk - Tq<br>            attn_mask[:, :prefix_len] = <span class="hljs-literal">True</span><br>            <span class="hljs-comment"># Then, causal attention within this chunk</span><br>            attn_mask[:, prefix_len:] = torch.tril(torch.ones((Tq, Tq), dtype=torch.<span class="hljs-built_in">bool</span>, device=q.device))<br>            y = F.scaled_dot_product_attention(q, k, v, attn_mask=attn_mask, enable_gqa=enable_gqa)<br>            <span class="hljs-comment"># 推理时如果有多个查询，需要先让每个查询关注缓存中的所有键值对，然后在当前查询块内进行因果注意力。（下三角掩码）</span><br><br>        <span class="hljs-comment"># Re-assemble the heads side by side and project back to residual stream</span><br>        y = y.transpose(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>).contiguous().view(B, T, -<span class="hljs-number">1</span>)<br>        y = <span class="hljs-variable language_">self</span>.c_proj(y)<br>        <span class="hljs-keyword">return</span> y<br></code></pre></td></tr></table></figure>
<p>简单的 MLP 和 Transformer Block 堆叠块</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">MLP</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, config</span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        <span class="hljs-variable language_">self</span>.c_fc = nn.Linear(config.n_embd, <span class="hljs-number">4</span> * config.n_embd, bias=<span class="hljs-literal">False</span>)<br>        <span class="hljs-variable language_">self</span>.c_proj = nn.Linear(<span class="hljs-number">4</span> * config.n_embd, config.n_embd, bias=<span class="hljs-literal">False</span>)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        x = <span class="hljs-variable language_">self</span>.c_fc(x)<br>        x = F.relu(x).square()<br>        x = <span class="hljs-variable language_">self</span>.c_proj(x)<br>        <span class="hljs-keyword">return</span> x<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Block</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, config, layer_idx</span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        <span class="hljs-variable language_">self</span>.attn = CausalSelfAttention(config, layer_idx)<br>        <span class="hljs-variable language_">self</span>.mlp = MLP(config)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x, cos_sin, kv_cache</span>):<br>        x = x + <span class="hljs-variable language_">self</span>.attn(norm(x), cos_sin, kv_cache)<br>        x = x + <span class="hljs-variable language_">self</span>.mlp(norm(x))<br>        <span class="hljs-keyword">return</span> x<br></code></pre></td></tr></table></figure>
<p>最后是 GPT 模型的整体实现（去除部分辅助函数）：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">GPT</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, config, pad_vocab_size_to=<span class="hljs-number">64</span></span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        <span class="hljs-variable language_">self</span>.config = config<br>        <span class="hljs-comment"># For DDP, we want vocab_size divisible by world_size. Also, there are potential performance benefits, see:</span><br>        <span class="hljs-comment"># https://huggingface.co/docs/transformers/main_classes/model#transformers.PreTrainedModel.resize_token_embeddings</span><br>        padded_vocab_size = ((config.vocab_size + pad_vocab_size_to - <span class="hljs-number">1</span>) // pad_vocab_size_to) * pad_vocab_size_to<br>        <span class="hljs-keyword">if</span> padded_vocab_size != config.vocab_size:<br>            print0(<span class="hljs-string">f&quot;Padding vocab_size from <span class="hljs-subst">&#123;config.vocab_size&#125;</span> to <span class="hljs-subst">&#123;padded_vocab_size&#125;</span> to be divisible by <span class="hljs-subst">&#123;pad_vocab_size_to&#125;</span>&quot;</span>)<br>        <span class="hljs-variable language_">self</span>.transformer = nn.ModuleDict(&#123;<br>            <span class="hljs-string">&quot;wte&quot;</span>: nn.Embedding(padded_vocab_size, config.n_embd),<br>            <span class="hljs-string">&quot;h&quot;</span>: nn.ModuleList([Block(config, layer_idx) <span class="hljs-keyword">for</span> layer_idx <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(config.n_layer)]),<br>        &#125;)<br>        <span class="hljs-comment"># wte 是 word to embedding 的缩写，表示词嵌入层</span><br>        <span class="hljs-comment"># h 是 transformer blocks </span><br>        <span class="hljs-variable language_">self</span>.lm_head = nn.Linear(config.n_embd, padded_vocab_size, bias=<span class="hljs-literal">False</span>)<br>        <span class="hljs-comment"># To support meta device initialization, we init the rotary embeddings here, but it&#x27;s just &quot;fake&quot; meta tensors only.</span><br>        <span class="hljs-comment"># As for rotary_seq_len, these rotary embeddings are pretty small/cheap in memory,</span><br>        <span class="hljs-comment"># so let&#x27;s just over-compute them by 10X, but assert fail if we ever reach that amount.</span><br>        <span class="hljs-comment"># In the future we can dynamically grow the cache, for now it&#x27;s fine.</span><br>        <span class="hljs-variable language_">self</span>.rotary_seq_len = config.sequence_len * <span class="hljs-number">10</span> <span class="hljs-comment"># 10X over-compute should be enough, TODO make nicer?</span><br>        <span class="hljs-comment"># 这里的意思是预计算 RoPE 位置嵌入，长度是序列长度的 10 倍，以防止在推理时超出范围，比如模型上下文长度是 1024，则预计算长度是 10240。理论上 sin, cos 可以滚动计算，但为了简单起见，直接预计算一个较大的长度。</span><br>        head_dim = config.n_embd // config.n_head<br>        cos, sin = <span class="hljs-variable language_">self</span>._precompute_rotary_embeddings(<span class="hljs-variable language_">self</span>.rotary_seq_len, head_dim)<br>        <span class="hljs-variable language_">self</span>.register_buffer(<span class="hljs-string">&quot;cos&quot;</span>, cos, persistent=<span class="hljs-literal">False</span>) <span class="hljs-comment"># persistent=False means it&#x27;s not saved to the checkpoint</span><br>        <span class="hljs-variable language_">self</span>.register_buffer(<span class="hljs-string">&quot;sin&quot;</span>, sin, persistent=<span class="hljs-literal">False</span>)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">init_weights</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        Initialize the full model in this one function for maximum clarity.</span><br><span class="hljs-string"></span><br><span class="hljs-string">        wte (embedding):     normal, std=1.0</span><br><span class="hljs-string">        lm_head:             normal, std=0.001</span><br><span class="hljs-string">        for each block:</span><br><span class="hljs-string">            attn.c_q:        uniform, std=1/sqrt(n_embd)</span><br><span class="hljs-string">            attn.c_k:        uniform, std=1/sqrt(n_embd)</span><br><span class="hljs-string">            attn.c_v:        uniform, std=1/sqrt(n_embd)</span><br><span class="hljs-string">            attn.c_proj:     zeros</span><br><span class="hljs-string">            mlp.c_fc:        uniform, std=1/sqrt(n_embd)</span><br><span class="hljs-string">            mlp.c_proj:      zeros</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br><br>        <span class="hljs-comment"># Embedding and unembedding</span><br>        torch.nn.init.normal_(<span class="hljs-variable language_">self</span>.transformer.wte.weight, mean=<span class="hljs-number">0.0</span>, std=<span class="hljs-number">1.0</span>)<br>        torch.nn.init.normal_(<span class="hljs-variable language_">self</span>.lm_head.weight, mean=<span class="hljs-number">0.0</span>, std=<span class="hljs-number">0.001</span>)<br><br>        <span class="hljs-comment"># Transformer blocks: uniform init with bound = sqrt(3) * std (same standard deviation as normal)</span><br>        n_embd = <span class="hljs-variable language_">self</span>.config.n_embd<br>        s = <span class="hljs-number">3</span>**<span class="hljs-number">0.5</span> * n_embd**-<span class="hljs-number">0.5</span> <span class="hljs-comment"># sqrt(3) multiplier makes sure Uniform achieves the same std as Normal</span><br>        <span class="hljs-keyword">for</span> block <span class="hljs-keyword">in</span> <span class="hljs-variable language_">self</span>.transformer.h:<br>            torch.nn.init.uniform_(block.attn.c_q.weight, -s, s) <span class="hljs-comment"># weights use Uniform to avoid outliers</span><br>            torch.nn.init.uniform_(block.attn.c_k.weight, -s, s)<br>            torch.nn.init.uniform_(block.attn.c_v.weight, -s, s)<br>            torch.nn.init.zeros_(block.attn.c_proj.weight) <span class="hljs-comment"># projections are zero</span><br>            torch.nn.init.uniform_(block.mlp.c_fc.weight, -s, s)<br>            torch.nn.init.zeros_(block.mlp.c_proj.weight)<br><br>        <span class="hljs-comment"># Rotary embeddings</span><br>        head_dim = <span class="hljs-variable language_">self</span>.config.n_embd // <span class="hljs-variable language_">self</span>.config.n_head<br>        cos, sin = <span class="hljs-variable language_">self</span>._precompute_rotary_embeddings(<span class="hljs-variable language_">self</span>.rotary_seq_len, head_dim)<br>        <span class="hljs-variable language_">self</span>.cos, <span class="hljs-variable language_">self</span>.sin = cos, sin<br><br>        <span class="hljs-comment"># Cast token embeddings to bf16: optimizer can tolerate it and it saves memory</span><br>        <span class="hljs-keyword">if</span> <span class="hljs-variable language_">self</span>.transformer.wte.weight.device.<span class="hljs-built_in">type</span> == <span class="hljs-string">&quot;cuda&quot;</span>:<br>            <span class="hljs-variable language_">self</span>.transformer.wte.to(dtype=torch.bfloat16)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_precompute_rotary_embeddings</span>(<span class="hljs-params">self, seq_len, head_dim, base=<span class="hljs-number">10000</span>, device=<span class="hljs-literal">None</span></span>):<br>        <span class="hljs-comment"># <span class="hljs-doctag">TODO:</span> bump base theta more? e.g. 100K is more common more recently</span><br>        <span class="hljs-comment"># autodetect the device from model embeddings</span><br>        <span class="hljs-keyword">if</span> device <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>            device = <span class="hljs-variable language_">self</span>.transformer.wte.weight.device<br>        <span class="hljs-comment"># stride the channels</span><br>        channel_range = torch.arange(<span class="hljs-number">0</span>, head_dim, <span class="hljs-number">2</span>, dtype=torch.float32, device=device)<br>        inv_freq = <span class="hljs-number">1.0</span> / (base ** (channel_range / head_dim))<br>        <span class="hljs-comment"># stride the time steps</span><br>        t = torch.arange(seq_len, dtype=torch.float32, device=device)<br>        <span class="hljs-comment"># calculate the rotation frequencies at each (time, channel) pair</span><br>        freqs = torch.outer(t, inv_freq)<br>        cos, sin = freqs.cos(), freqs.sin()<br>        cos, sin = cos.bfloat16(), sin.bfloat16() <span class="hljs-comment"># keep them in bfloat16</span><br>        cos, sin = cos[<span class="hljs-literal">None</span>, :, <span class="hljs-literal">None</span>, :], sin[<span class="hljs-literal">None</span>, :, <span class="hljs-literal">None</span>, :] <span class="hljs-comment"># add batch and head dims for later broadcasting</span><br>        <span class="hljs-keyword">return</span> cos, sin<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">get_device</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">return</span> <span class="hljs-variable language_">self</span>.transformer.wte.weight.device<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">setup_optimizers</span>(<span class="hljs-params">self, unembedding_lr=<span class="hljs-number">0.004</span>, embedding_lr=<span class="hljs-number">0.2</span>, matrix_lr=<span class="hljs-number">0.02</span>, weight_decay=<span class="hljs-number">0.0</span>, adam_betas=(<span class="hljs-params"><span class="hljs-number">0.8</span>, <span class="hljs-number">0.95</span></span>)</span>):<br>        model_dim = <span class="hljs-variable language_">self</span>.config.n_embd<br>        ddp, rank, local_rank, world_size = get_dist_info()<br>        <span class="hljs-comment"># Separate out all parameters into 3 groups (matrix, embedding, lm_head)</span><br>        matrix_params = <span class="hljs-built_in">list</span>(<span class="hljs-variable language_">self</span>.transformer.h.parameters())<br>        embedding_params = <span class="hljs-built_in">list</span>(<span class="hljs-variable language_">self</span>.transformer.wte.parameters())<br>        lm_head_params = <span class="hljs-built_in">list</span>(<span class="hljs-variable language_">self</span>.lm_head.parameters())<br>        <span class="hljs-keyword">assert</span> <span class="hljs-built_in">len</span>(<span class="hljs-built_in">list</span>(<span class="hljs-variable language_">self</span>.parameters())) == <span class="hljs-built_in">len</span>(matrix_params) + <span class="hljs-built_in">len</span>(embedding_params) + <span class="hljs-built_in">len</span>(lm_head_params)<br>        <span class="hljs-comment"># Create the AdamW optimizer for the embedding and lm_head</span><br>        <span class="hljs-comment"># Scale the LR for the AdamW parameters by ∝1/√dmodel (having tuned the LRs for 768 dim model)</span><br>        dmodel_lr_scale = (model_dim / <span class="hljs-number">768</span>) ** -<span class="hljs-number">0.5</span><br>        print0(<span class="hljs-string">f&quot;Scaling the LR for the AdamW parameters ∝1/√(<span class="hljs-subst">&#123;model_dim&#125;</span>/768) = <span class="hljs-subst">&#123;dmodel_lr_scale:<span class="hljs-number">.6</span>f&#125;</span>&quot;</span>)<br>        adam_groups = [<br>            <span class="hljs-built_in">dict</span>(params=lm_head_params, lr=unembedding_lr * dmodel_lr_scale),<br>            <span class="hljs-built_in">dict</span>(params=embedding_params, lr=embedding_lr * dmodel_lr_scale),<br>        ]<br>        adamw_kwargs = <span class="hljs-built_in">dict</span>(betas=adam_betas, eps=<span class="hljs-number">1e-10</span>, weight_decay=weight_decay)<br>        AdamWFactory = DistAdamW <span class="hljs-keyword">if</span> ddp <span class="hljs-keyword">else</span> partial(torch.optim.AdamW, fused=<span class="hljs-literal">True</span>)<br>        adamw_optimizer = AdamWFactory(adam_groups, **adamw_kwargs)<br>        <span class="hljs-comment"># Create the Muon optimizer for the linear layers</span><br>        muon_kwargs = <span class="hljs-built_in">dict</span>(lr=matrix_lr, momentum=<span class="hljs-number">0.95</span>)<br>        MuonFactory = DistMuon <span class="hljs-keyword">if</span> ddp <span class="hljs-keyword">else</span> Muon<br>        muon_optimizer = MuonFactory(matrix_params, **muon_kwargs)<br><br>        <span class="hljs-comment"># 对2d矩阵参数使用 Muon 优化器，对1d的嵌入和输出层参数使用 AdamW 优化器。</span><br>        <br>        <span class="hljs-comment"># Combine them the two optimizers into one list</span><br>        optimizers = [adamw_optimizer, muon_optimizer]<br>        <span class="hljs-keyword">for</span> opt <span class="hljs-keyword">in</span> optimizers:<br>            <span class="hljs-keyword">for</span> group <span class="hljs-keyword">in</span> opt.param_groups:<br>                group[<span class="hljs-string">&quot;initial_lr&quot;</span>] = group[<span class="hljs-string">&quot;lr&quot;</span>]<br>        <span class="hljs-keyword">return</span> optimizers<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, idx, targets=<span class="hljs-literal">None</span>, kv_cache=<span class="hljs-literal">None</span>, loss_reduction=<span class="hljs-string">&#x27;mean&#x27;</span></span>):<br>        B, T = idx.size()<br><br>        <span class="hljs-comment"># Grab the rotary embeddings for the current sequence length (they are of shape (1, seq_len, 1, head_dim/2))</span><br>        <span class="hljs-keyword">assert</span> T &lt;= <span class="hljs-variable language_">self</span>.cos.size(<span class="hljs-number">1</span>), <span class="hljs-string">f&quot;Sequence length grew beyond the rotary embeddings cache: <span class="hljs-subst">&#123;T&#125;</span> &gt; <span class="hljs-subst">&#123;self.cos.size(<span class="hljs-number">1</span>)&#125;</span>&quot;</span><br>        <span class="hljs-keyword">assert</span> idx.device == <span class="hljs-variable language_">self</span>.cos.device, <span class="hljs-string">f&quot;Rotary embeddings and idx are on different devices: <span class="hljs-subst">&#123;idx.device&#125;</span> != <span class="hljs-subst">&#123;self.cos.device&#125;</span>&quot;</span><br>        <span class="hljs-keyword">assert</span> <span class="hljs-variable language_">self</span>.cos.dtype == torch.bfloat16, <span class="hljs-string">&quot;Rotary embeddings must be in bfloat16&quot;</span><br>        <span class="hljs-comment"># if kv cache exists, we need to offset the rotary embeddings to the current position in the cache</span><br>        T0 = <span class="hljs-number">0</span> <span class="hljs-keyword">if</span> kv_cache <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">else</span> kv_cache.get_pos()<br>        cos_sin = <span class="hljs-variable language_">self</span>.cos[:, T0:T0+T], <span class="hljs-variable language_">self</span>.sin[:, T0:T0+T] <span class="hljs-comment"># truncate cache to current sequence length</span><br><br>        <span class="hljs-comment"># Forward the trunk of the Transformer</span><br>        x = <span class="hljs-variable language_">self</span>.transformer.wte(idx)<br>        x = norm(x)<br>        <span class="hljs-keyword">for</span> block <span class="hljs-keyword">in</span> <span class="hljs-variable language_">self</span>.transformer.h:<br>            x = block(x, cos_sin, kv_cache)<br>        x = norm(x)<br><br>        <span class="hljs-comment"># Forward the lm_head (compute logits)</span><br>        softcap = <span class="hljs-number">15</span> <span class="hljs-comment"># smoothly cap the logits to the range [-softcap, softcap]</span><br>        logits = <span class="hljs-variable language_">self</span>.lm_head(x) <span class="hljs-comment"># (B, T, padded_vocab_size) &lt;- very big tensor, large amount of memory</span><br>        logits = logits[..., :<span class="hljs-variable language_">self</span>.config.vocab_size] <span class="hljs-comment"># slice to remove padding</span><br>        logits = logits.<span class="hljs-built_in">float</span>() <span class="hljs-comment"># switch to fp32 for logit softcap and loss computation</span><br>        logits = softcap * torch.tanh(logits / softcap) <span class="hljs-comment"># squash the logits</span><br><br>        <span class="hljs-comment"># 这里利用 tanh 函数对 logits 进行平滑限制在 [-softcap, softcap]，防止极端值导致训练不稳定。</span><br><br>        <span class="hljs-keyword">if</span> targets <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            <span class="hljs-comment"># training: given the targets, compute and return the loss</span><br>            <span class="hljs-comment"># TODO experiment with chunked cross-entropy?</span><br>            loss = F.cross_entropy(logits.view(-<span class="hljs-number">1</span>, logits.size(-<span class="hljs-number">1</span>)), targets.view(-<span class="hljs-number">1</span>), ignore_index=-<span class="hljs-number">1</span>, reduction=loss_reduction)<br>            <span class="hljs-keyword">return</span> loss<br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-comment"># inference: just return the logits directly</span><br>            <span class="hljs-keyword">return</span> logits<br><br><span class="hljs-meta">    @torch.inference_mode()</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">generate</span>(<span class="hljs-params">self, tokens, max_tokens, temperature=<span class="hljs-number">1.0</span>, top_k=<span class="hljs-literal">None</span>, seed=<span class="hljs-number">42</span></span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        Naive autoregressive streaming inference.</span><br><span class="hljs-string">        To make it super simple, let&#x27;s assume:</span><br><span class="hljs-string">        - batch size is 1</span><br><span class="hljs-string">        - ids and the yielded tokens are simple Python lists and ints</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        <span class="hljs-keyword">assert</span> <span class="hljs-built_in">isinstance</span>(tokens, <span class="hljs-built_in">list</span>)<br>        device = <span class="hljs-variable language_">self</span>.get_device()<br>        rng = <span class="hljs-literal">None</span><br>        <span class="hljs-keyword">if</span> temperature &gt; <span class="hljs-number">0</span>:<br>            rng = torch.Generator(device=device)<br>            rng.manual_seed(seed)<br>        ids = torch.tensor([tokens], dtype=torch.long, device=device) <span class="hljs-comment"># add batch dim</span><br>        <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(max_tokens):<br>            logits = <span class="hljs-variable language_">self</span>.forward(ids) <span class="hljs-comment"># (B, T, vocab_size)</span><br>            logits = logits[:, -<span class="hljs-number">1</span>, :] <span class="hljs-comment"># (B, vocab_size)</span><br>            <span class="hljs-keyword">if</span> top_k <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>                v, _ = torch.topk(logits, <span class="hljs-built_in">min</span>(top_k, logits.size(-<span class="hljs-number">1</span>)))<br>                logits[logits &lt; v[:, [-<span class="hljs-number">1</span>]]] = -<span class="hljs-built_in">float</span>(<span class="hljs-string">&#x27;Inf&#x27;</span>)<br>            <span class="hljs-keyword">if</span> temperature &gt; <span class="hljs-number">0</span>:<br>                logits = logits / temperature<br>                probs = F.softmax(logits, dim=-<span class="hljs-number">1</span>)<br>                next_ids = torch.multinomial(probs, num_samples=<span class="hljs-number">1</span>, generator=rng)<br>            <span class="hljs-keyword">else</span>:<br>                next_ids = torch.argmax(logits, dim=-<span class="hljs-number">1</span>, keepdim=<span class="hljs-literal">True</span>)<br>            ids = torch.cat((ids, next_ids), dim=<span class="hljs-number">1</span>)<br>            token = next_ids.item()<br>            <span class="hljs-keyword">yield</span> token<br></code></pre></td></tr></table></figure>
<p>以下是 Gemini 的总结</p>
<p>权重初始化：</p>
<ol type="1">
<li><p>嵌入层与输出层的极端标准差差异 <code>wte</code> (Token
Embedding): <code>std=1.0</code>: Token 嵌入被初始化为标准正态分布。由于
<span class="math inline">\(wte\)</span>
的权重通常会随后被层归一化（LayerNorm）处理，较大的初始标准差可以为模型提供丰富的初始特征表示。<code>lm_head</code>:
<code>std=0.001</code> 输出层使用了非常小的标准差。目的：
在训练开始时，使模型对所有词汇的预测概率趋于均匀分布。如果初始权重过大，模型会产生强烈的随机偏见，导致初始损失值（Loss）极高，增加收敛难度。</p></li>
<li><p>均匀分布初始化与 <span class="math inline">\(\sqrt{3}\)</span>
的数学推导代码中使用了均匀分布 <code>uniform_(-s, s)</code>
而非正态分布，并定义了 <span class="math inline">\(s = \sqrt{3} \times
\frac{1}{\sqrt{n\_embd}}\)</span>。为什么用 Uniform：
注释提到是为了“避免离群值（outliers）”。正态分布理论上可能产生极大或极小的权重，而均匀分布的范围是严格受限的。<span
class="math inline">\(\sqrt{3}\)</span> 的来源： 对于均匀分布 <span
class="math inline">\(U(-s, s)\)</span>，其方差为 <span
class="math inline">\(Var = \frac{(s - (-s))^2}{12} = \frac{4s^2}{12} =
\frac{s^2}{3}\)</span>。为了让均匀分布的方差等于期望的方差 <span
class="math inline">\(\sigma^2\)</span>（即 <span
class="math inline">\(1/n\_embd\)</span>），则需要： <span
class="math display">\[
\frac{s^2}{3} = \sigma^2 \implies s = \sigma \sqrt{3}
\]</span> 这确保了无论使用哪种分布，权重的统计特性是一致的。</p></li>
<li><p>投影层初始化为零 (<code>c_proj</code>:
zeros)这是一个非常关键的技巧，常见于高性能模型实现（如 GPT-2
或部分版本的 Llama）：残差流的恒等映射： 在训练初始时刻，如果注意力投影
<code>c_proj</code> 为零，那么 Transformer
块的输出就等于输入（因为残差连接 <span class="math inline">\(x + 0 =
x\)</span>）。逻辑：
这种做法类似于“恒等函数”初始化，让模型先学习直通的数据流动，再逐渐通过训练学习如何修改残差流中的特征。这极大地提高了极深网络的训练稳定性。</p></li>
<li><p>旋转位置嵌入（RoPE）的预计算操作： 调用内部函数生成
<code>cos</code> 和 <code>sin</code> 表。逻辑：
如前所述，这部分内容是确定性的数学值（频率分布），不需要学习，因此在初始化阶段一次性生成并缓存，以供推理和训练时快速索引。</p></li>
<li><p>混合精度转换 (<code>bfloat16</code>)操作： 显式将
<code>wte.weight</code> 转为 <code>bfloat16</code>。优势： 显存节省：
词表往往很大，将其从 float32 转为 bf16
可以直接节省一半的词表显存。精度特性： <code>bf16</code> 具有与
<code>fp32</code>
相同的指数位范围，能够有效防止训练中的溢出风险。</p></li>
</ol>
<p>RoPE 位置嵌入预计算公式：</p>
<p>设 <span class="math inline">\(d\)</span> 为每个注意力头的维度，<span
class="math inline">\(i\)</span> 为通道索引，<span
class="math inline">\(t\)</span> 为时间步索引，<span
class="math inline">\(\theta\)</span> 为基频（通常取 10000）。则每个位置
<span class="math inline">\(t\)</span> 和通道 <span
class="math inline">\(i\)</span> 的旋转频率和角度为： <span
class="math display">\[
\text{freq}(t, i) = \frac{1}{\theta^{\frac{2i}{d}}}
\]</span> <span class="math display">\[
\text{angle}(t, i) = t \times \text{freq}(t, i) =
\frac{t}{\theta^{\frac{2i}{d}}}
\]</span></p>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
    <div class="post-meta mr-3 d-flex align-items-center">
      <i class="iconfont icon-category"></i>
      

<span class="category-chains">
  
  
    
      <span class="category-chain">
        
  <a href="/categories/Programming/" class="category-chain-item">Programming</a>
  
  

      </span>
    
  
</span>

    </div>
  
  
    <div class="post-meta">
      <i class="iconfont icon-tags"></i>
      
        <a href="/tags/LLM/" class="print-no-link">#LLM</a>
      
        <a href="/tags/Open-Source/" class="print-no-link">#Open Source</a>
      
    </div>
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>nanochat 学习笔记</div>
      <div>https://notdesigned.github.io/2026/01/10/nanochat-学习笔记/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>Author</div>
          <div>Luocheng Liang</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>Posted on</div>
          <div>January 10, 2026</div>
        </div>
      
      
      
        <div class="license-meta-item">
          <div>Licensed under</div>
          <div>
            
              
              
                <a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by-sa/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - Attribution">
                    <i class="iconfont icon-cc-by"></i>
                  </span>
                </a>
              
                <a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by-sa/4.0/">
                  <span class="hint--top hint--rounded" aria-label="SA - Share-alike">
                    <i class="iconfont icon-cc-sa"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2026/02/26/Unified-Latents-%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B%E4%B8%8B%E7%9A%84%E7%94%9F%E6%88%90%E4%B8%8E%E7%BC%96%E7%A0%81%E9%85%8D%E5%90%88/" title="Unified Latents: 扩散模型的框架下，令生成与编码相互配合">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">Unified Latents: 扩散模型的框架下，令生成与编码相互配合</span>
                        <span class="visible-mobile">Previous</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2026/01/01/%E9%9A%8F%E6%9C%BA%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B%EF%BC%88SDE%EF%BC%89/" title="随机微分方程（SDE）">
                        <span class="hidden-mobile">随机微分方程（SDE）</span>
                        <span class="visible-mobile">Next</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
  
  
    <article id="comments" lazyload>
      
  <div id="gitalk-container"></div>
  <script type="text/javascript">
    Fluid.utils.loadComments('#gitalk-container', function() {
      Fluid.utils.createCssLink('/css/gitalk.css')
      Fluid.utils.createScript('https://lib.baomitu.com/gitalk/1.8.0/gitalk.min.js', function() {
        var options = Object.assign(
          {"clientID":"Ov23lisUTr0Y3YpBngo1","clientSecret":"4068f2efc1d2d6015f3c8089a4e0ad4c1aeb8ed2","repo":"gitalkcomment","owner":"NotDesigned","admin":["NotDesigned"],"language":"en","labels":["Gitalk"],"perPage":10,"pagerDirection":"last","distractionFreeMode":false,"createIssueManually":false,"proxy":"https://autumn-recipe-17e3.theadscn.workers.dev/https://github.com/login/oauth/access_token"},
          {
            id: 'b216c107261721d8af5797a5494aa507'
          }
        )
        var gitalk = new Gitalk(options);
        gitalk.render('gitalk-container');
      });
    });
  </script>
  <noscript>Please enable JavaScript to view the comments</noscript>


    </article>
  


          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header">
    <i class="iconfont icon-list"></i>
    <span>Table of Contents</span>
  </p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  


  
  









    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">Search</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">Keyword</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.4/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.20.1/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/5.0.0/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  
      <script>
        if (!window.MathJax) {
          window.MathJax = {
            tex    : {
              inlineMath: { '[+]': [['$', '$']] }
            },
            loader : {
              load: ['ui/lazy']
            },
            options: {
              renderActions: {
                insertedScript: [200, () => {
                  document.querySelectorAll('mjx-container').forEach(node => {
                    let target = node.parentNode;
                    if (target.nodeName.toLowerCase() === 'li') {
                      target.parentNode.classList.add('has-jax');
                    }
                  });
                }, '', false]
              }
            }
          };
        } else {
          MathJax.startup.document.state(0);
          MathJax.texReset();
          MathJax.typeset();
          MathJax.typesetPromise();
        }

        Fluid.events.registerRefreshCallback(function() {
          if ('MathJax' in window && MathJax.startup.document && typeof MathJax.startup.document.state === 'function') {
            MathJax.startup.document.state(0);
            MathJax.texReset();
            MathJax.typeset();
            MathJax.typesetPromise();
          }
        });
      </script>
    

  <script  src="https://lib.baomitu.com/mathjax/3.2.2/es5/tex-mml-chtml.js" ></script>

  <script  src="/js/local-search.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">Blog works best with JavaScript enabled</div>
  </noscript>
</body>
</html>
